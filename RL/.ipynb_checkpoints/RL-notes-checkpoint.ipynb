{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Deep Reinforcement Learning?\n",
    "Link = https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reinforcement Learning Process\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*aKYFRoEmmKkybqJOvLt2JQ.png?raw=true)\n",
    "\n",
    "This RL loops output a sequnce of **state**, **action** and **reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The central idea of the Reward Hypothesis\n",
    "\n",
    "Cumulative reward at each time step $t$:\n",
    "\n",
    "$$G_t=\\sum_{k=0}^{T}R_{t+k+1}\\tag{1}$$\n",
    "\n",
    "The rewards that come sooner are more probable to happen, since they are more predictable than the long term future reward.\n",
    "\n",
    "$$G_t=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\text{, where } 0\\leq\\gamma<1\\tag{2}$$\n",
    "\n",
    "- the larger $\\gamma$ the smaller the discount\n",
    "- the smaller $\\gamma$ the bigger the discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic or Continuing tasks\n",
    "\n",
    "**Episodic task** has an starting and an ending points<br>\n",
    "**continuous task** does not have ending point. The agent keeps running until we decide to stop him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo vs TD Learning methods\n",
    "\n",
    "In **Monte Carlo Approach** we collect the rewards **at the end of the episode** and then *calculate* the **maximum expected future reward**.\n",
    "\n",
    "$$V(S_t)\\leftarrow V(S_t) + \\alpha[G_t-V(S_t)]\\tag{3}$$\n",
    "\n",
    "In **Temporal difference learning** we estimate the **reward at *each step***\n",
    "\n",
    "$$\\displaystyle V(S_t) \\leftarrow \\underbrace{V(S_t)}_{\\text{previous estimate}} + \\alpha\\left[\\overbrace{\\underbrace{R_{t+1}}_{\\text{reward at t+1}}+\\underbrace{\\gamma V(S_{t+1})}_{\\text{discount value on next step}}}^{\\text{TD target}} \\right]\\tag{4}$$\n",
    "\n",
    "By running more and more episodes, **the agent will learn to play better and better**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration/Exploitation trade-off\n",
    "\n",
    "- **Exploration** is finding more information about the environment\n",
    "- **Exploitation** is exploiting known information to maximize the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three approaches to Reinforcement Learning\n",
    "#### Value-based\n",
    "Value-based RL optimizes the value function\n",
    "<br>***Value function** is a function that tells us the maximum expected future reward the agent will get at each step*.\n",
    "$$v_\\pi(s) =\\mathbb{E}\\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\mid S_t=s \\right]\n",
    "\\tag{5}$$\n",
    "\n",
    "#### Policy based\n",
    "Policy based RL directly optimizes the policy function $\\pi(s)$ without using a value function.\n",
    "\n",
    "$$\\underbrace{a\\quad=\\quad\\pi(s)}_{\\text{action = policy(function)}}\n",
    "\\tag{6}$$\n",
    "\n",
    "- *Deterministic* policy will always return the same action at a given state.\n",
    "- *Stochastic* policy outputs a distribution probability over actions\n",
    "\n",
    "$$\\pi(a|s) =\\mathbb{P}[A_t=a|S_t=s]\n",
    "\\tag{7}$$\n",
    "\n",
    "#### Model Based\n",
    "Model based RL models the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Deep Reinforcement Learning\n",
    "Deep reinforcement learning introduces deep neural networks to solve Reinforcement Learning problems--hence the name \"deep\".\n",
    "![](https://cdn-images-1.medium.com/max/1395/1*w5GuxedZ9ivRYqM_MLUxOQ.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving deeper into Reinforcement Learning with Q-Learning\n",
    "Link = https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe\n",
    "\n",
    "## Introducing the Q-table\n",
    "In **Q-table**, the columns will be actions $a$, the rows will be states $s$, the value of each cell will bethe maximum expected future reward for that given state and action $Q^*(x,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning algorithm: laerning the Action Value Function\n",
    "$$Q^\\pi(s_t,a_t)=\\mathbb{E}\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\mid s_t,a_t\\right]\\tag{8}$$\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*yklmxNRdXleiDbv6aSZUIg.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q-learning algorithm Process\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*QeoQEqWYYPs1P8yUwyaJVQ.png?raw=true)\n",
    "\n",
    "1. $\\ $ Initialize Q-values $Q(s,a)$ arbitrarily for all state-action pairs\n",
    "2. $\\ $For life or until learning is stopped \n",
    "3. $\\quad$ Choose an action $(a)$ in the current world state $(s)$ based on current Q-value estimates $Q(s,\\cdot)$\n",
    "4. $\\quad$ Take the action $(a)$ and observe the outcome state $(s')$ and reward $(r)$\n",
    "5. $\\quad$ Update $Q(s,a):=Q(s,a)+\\alpha[r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose an action at step 3?\n",
    "The idea is that in the beginning, we'll use the **epsilon greedy strategy**:\n",
    "- We specify an exploration ratie \"epsilon\", which we set to 1 in the beginning. This is the rate of steps that we'll do randomly. In the beginning, this rate must be at its highest value, because we don't know anything about the values in Q-table. This means we need to do alot of exploration, by randomly chossing our actions.\n",
    "- We generate a random number. If this number is larger \"epsilon\", the we will do \"exploitation\" (this means we use waht we already know to select the best action at each step). Else, we'll do exploration.\n",
    "- The idea is that we must have a big epsilon at the beginnign of the training of the Q-function. Then, reduce it progressively as the agent becomes more confident at estimating Q-values.\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*9StLEbor62FUDSoRwxyJrg.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap ...\n",
    "- Q-learning is a value-based Reinforcement Learning algorithm that is used to find the optimal action-selection policy using a q function.\n",
    "- It evaluates which action to take based on an action-value function that determines the value of being in a certain state and taking a certain action at that state.\n",
    "- Goal: maximize the value function Q (expected future reward given a state and action).\n",
    "- Q table helps us to find the best action for each state.\n",
    "- To maximize the expected reward by selecting the best of all possible actions.\n",
    "- The Q come from quality of a certain action in a certain state.\n",
    "- Function Q(state, action) → returns expected future reward of that action at that state.\n",
    "- This function can be estimated using Q-learning, which iteratively updates Q(s,a) using the Bellman Equation\n",
    "- Before we explore the environment: Q table gives the same arbitrary fixed value → but as we explore the environment → Q gives us a better and better approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
