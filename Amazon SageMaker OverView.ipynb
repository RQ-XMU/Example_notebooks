{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. What Is Amazon SageMaker?\n",
    "- Amazon SageMaker is a **fully managed machine learning service**.\n",
    "- helps to **quickly and easily build and train machine learning models**, and then **directly deploy** them into a production-ready hosted environment.\n",
    "- provides an **integrated Jupyter** authoring **notebook** instance for **easy access** to your **data** sources\n",
    "- provides **common machine learning algorithms** that are **optimized** to run efficiently against **extremely large data** in a **distributed environment**.\n",
    "- Deploy **with a single click console**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. How It Works\n",
    "This section provides an **overview of machine learning** and explains how Amazon SageMaker works.\n",
    "## 2.1. Machine Learning with Amazon SageMaker\n",
    "Typical workflow for creating a machine learning model:\n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/ml-concepts-10.png)\n",
    "1. **Generate example data**<br>\n",
    "    Data type depends on business problem<br>\n",
    "    a. **Fetch the data** — \n",
    "    pull the dataset or datasets into a single repository<br>\n",
    "    b. **Clean the data** — \n",
    "    To improve model training<br>\n",
    "    c. **Prepare or transform the data** — \n",
    "    To improve performance\n",
    "2. **Train a model**:<br>\n",
    "    a. **Training** the model — To train a model, you need<br>\n",
    "    - an algorithm (can be provided by Amazon Sagemaker or self-implementation)\n",
    "    - compute resources<br>\n",
    "    b. **Evaluating** the model — \n",
    "    to determine whether the accuracy of the inferences is acceptable.<br>\n",
    "    -.use either the AWS SDK for Python (Boto) or the high-level Python library in SageMaker<br>\n",
    "    -.use a Jupyter notebook in SageMaker notebook instance to train and evaluate model.\n",
    "3. **Deploy the model** — independently with Amazon SageMaker hosting services, decoupling from application code. \n",
    "\n",
    "Machine learning is a **continuous cycle**: \n",
    "- => **deploy** model\n",
    "- => **monitor** inferences\n",
    "- => **collect** \"ground truth\"\n",
    "- => **evalutate** model\n",
    "- => **retrain** model\n",
    "- => **deploy new** model\n",
    "- => ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Explore and Preprocess Data\n",
    "\n",
    "- Use a Jupyter notebook on an Amazon SageMaker \n",
    "- use a model to transform data by using Amazon SageMaker batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training a Model with Amazon SageMaker\n",
    "\n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture.png)\n",
    "\n",
    "Create a training job by SageMaker Conslole or API, including:\n",
    "- URL to data on S3\n",
    "- Compute resources, managed by Amazon SageMaker\n",
    "- URL for output data on S3\n",
    "- Amazon Elastic Container Registry path where the training code is stored\n",
    "\n",
    "**Training** options:\n",
    "- Use an algorithm provided by Amazon SageMaker\n",
    "- Use Apache Spark with Amazon SageMaker, similarly to use Spark MLLib\n",
    "- Submit custom code to train with deep learning frameworks: [TensorFlow](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html), [Apache MXNet](https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet.html)\n",
    "- Use custom algorithms in  a Docker image\n",
    "\n",
    "**WorkFlow**:\n",
    "- User creates the training job\n",
    "- => Amazon SageMaker launches the ML compute instances\n",
    "- => SageMaker uses the training code and the training dataset to train the model\n",
    "- =>SageMaker saves the resulting model artifacts and other output in the S3 bucket \n",
    "\n",
    "**Important**\n",
    "- Prevent out-of-memory error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Model Deployment in Amazon SageMaker\n",
    "- use Amazon **[SageMaker hosting services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)** to set up a persistent endpoint to get **one prediction** at a time, .\n",
    "- use Amazon **[SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)** to get **predictions** for an **entire dataset**.\n",
    "\n",
    "### 2.4.1. Deploying a Model on Amazon SageMaker Hosting Services\n",
    "A three-step process:\n",
    "1. Create a model in Amazon SageMaker for finding model components\n",
    "2. Create an endpoint configuration for an HTTPS endpoint, configure the endpoint to elastically scale the deployed ML compute instances in production\n",
    "3. Create an HTTPS endpoint: endpoint configuration to Amazon SageMaker\n",
    "\n",
    "**Considerations**\n",
    "- client application sends requests to the Amazon SageMaker HTTPS endpoint to obtain inferences from a deployed model, requests can be sent to this endpoint from Jupyter notebook during testing.\n",
    "- model trained with Amazon SageMaker can be deployed to specific deployment target.\n",
    "- multiple variants of a model can be deployed to the same Amazon SageMaker HTTPS endpoint.\n",
    "- a *ProductionVariant* can be configured to use Application Auto Scaling\n",
    "- an endpoint can be modified without taking models that are already deployed into production out of service.\n",
    "- **Changing or deleting** model artifacts or changing inference code **after deploying** a model produces **unpredictable results**.\n",
    "\n",
    "### 2.4.2. Getting Inferences by Using Amazon SageMaker Batch Transform \n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/batch-transform.png)\n",
    "\n",
    "Batch transform manages all compute resources necessary to get inferences. This includes launching instances and deleting them after the transform job completes. \n",
    "\n",
    "To perform a batch transform, create a transform job including:\n",
    "- path to data on S3 bucket\n",
    "- compute resources\n",
    "- path to S3 for output data\n",
    "- name of model in the transform job\n",
    "\n",
    "Batch transform is ideal for situations where:\n",
    "- You want to get inferences for an entire dataset and store them online.\n",
    "- You don't need a persistent endpoint that applications can call to get inferences.\n",
    "- You don't need the sub-second latency that Amazon SageMaker hosted endpoints provide.\n",
    "- You want to preprocess your data before using the data to train a new model or generate inferences.\n",
    "\n",
    "**Considerations**:\n",
    "- transform job can be created by SageMaker Console or API\n",
    "- Amazon SageMaker follows the transform job to read input, launches ML and save output\n",
    "- Amazon SageMaker uses Multipart Upload API to upload output data results from a transform job to S3. \n",
    "- For testing model variants, create separate transform jobs for each variant using a validation data set.\n",
    "- For large datasets or data of indeterminate size, create an infinite stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Validating Machine Learning Models \n",
    "\n",
    "After training, a model needs to be evaluated to determine whether its performance and accuracy allow to achieve business goals.\n",
    "\n",
    "- **Offline testing**: Deploy trained model to an alpha endpoint, and use historical data to send inference requests to it.\n",
    "- **Online testing** with live data: choose to send a portion of the traffic to a model variant for evaluation.\n",
    "\n",
    "**Options** for **offline** evaluation:\n",
    "- **using a \"holdout set\"**: use 20-30% of the training data for validation\n",
    "- **k-fold validation**: split data into k+1 folds, user 1 folds for validation & k folds for training; run k+1 times => k+1 models => aggregate to obtain final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 The Amazon SageMaker Programming Model \n",
    "\n",
    "- use SageMaker APIs to create and manage notebook instances and train and deploy models\n",
    "- alternatives:\n",
    "    - Use the Amazon SageMaker console\n",
    "    - Modify the example Jupyter notebooks\n",
    "    - Write model training and inference code from scratch\n",
    "        - high-level Python library\n",
    "        - AWS SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Getting Started\n",
    "\n",
    "## [3.1: Setting Up](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)\n",
    "### [3.1.1: Create an AWS Account and an Administrator User](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html)\n",
    "- [Create an AWS Account](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html#gs-account-create)\n",
    "- [Create an IAM Administrator User and Sign In](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html#gs-account-user)\n",
    "### [3.1.2: Create an S3 Bucket](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html)\n",
    "\n",
    "## [3.2: Create an Amazon SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "\n",
    "## [3.3: Train a Model with a Built-in Algorithm and Deploy It](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1.html)\n",
    "\n",
    "### [3.3.1: Create a Jupyter Notebook and Initialize Variables](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html)\n",
    "### [3.3.2: Download, Explore, and Transform the Training Data](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data.html)\n",
    "- 1: Download the MNIST Dataset\n",
    "- 2: Explore the Training Dataset\n",
    "- 3: Transform the Training Dataset and Upload It to S3\n",
    "\n",
    "### [3.3.3: Train a Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html)\n",
    "- 1: Choose the Training Algorithm\n",
    "- 2: Create a Training Job\n",
    "\n",
    "### [3.3.4: Deploy the Model to Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-model-deployment.html)\n",
    "- 1: Deploy the Model to Amazon **SageMaker Hosting** Services\n",
    "- 2: Deploy the Model to Amazon **SageMaker Batch Transform**\n",
    "\n",
    "### [3.3.5: Validate the Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html)\n",
    "\n",
    "## [3.4: Clean up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)\n",
    "## [3.5: Additional Considerations](https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Automatic Model Tuning\n",
    "- Automatic model tuning = **hyperparameter tuning**, finds the **best** version of a **model** by running **many training jobs** on dataset using the **algorithm** and **ranges of hyperparameters** that were specified.\n",
    "- use Amazon SageMaker automatic model tuning with **built-in** algorithms, **custom algorithms**, and Amazon SageMaker **pre-built containers** for machine learning frameworks\n",
    "- Before start hyperparameter tuning, it requires a well-defined machine learning problem, including:\n",
    "    - A **dataset**\n",
    "    - An understanding of the **type of algorithm** needed to train\n",
    "    - A clear understanding of **how to measure success**\n",
    "    \n",
    "## 4.1 How Hyperparameter Tuning Works\n",
    "- **Workflow**:\n",
    "    - hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results, \n",
    "    - runs training jobs to test these guesses. \n",
    "    - after testing uses regression to choose the next set of hyperparameter values\n",
    "- Hyperparameter tuning uses an Amazon SageMaker implementation of **Bayesian optimization**\n",
    "- Use explore/exploit trade-off strategy\n",
    "- **Note**:\n",
    "    - might not improve model\n",
    "    - exploring all of the possible combinations is impractical with complex model\n",
    "    - need to choose the right ranges to explore\n",
    "- **Refernces**:\n",
    "    - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning, [link](https://arxiv.org/abs/1012.2599)\n",
    "    - Practical Bayesian Optimization of Machine Learning Algorithms, [link](https://arxiv.org/abs/1206.2944)\n",
    "    - Taking the Human Out of the Loop: A Review of Bayesian Optimization, [link](http://ieeexplore.ieee.org/document/7352306/?reload=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Defining Objective Metrics\n",
    "- Not required for SageMaker built-in algorithms, just select and use\n",
    "- Required with regular expressions (**regex**) for custom algorithms. Algorthms needs to emit at least one metric by writing evaluation data to *stderr* or *stdout*\n",
    "- The hyperparameter tuning job returns the training job that returned the best value for the objective metric as the best training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Defining Hyperparameter Ranges\n",
    "Choosing hyperparameters and ranges significantly affects the performance of your tuning job.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"tree_method\",\n",
    "          \"Values\": [\"auto\", \"exact\", \"approx\", \"hist\"]\n",
    "        }          \n",
    "      ],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"eta\",\n",
    "          \"MaxValue\" : \"0.5\",\n",
    "          \"MinValue\": \"0\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"max_depth\",\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\", \n",
    "        }\n",
    "      ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Example: Hyperparameter Tuning Job\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Design Considerations\n",
    "- Choosing the Number of Hyperparameters\n",
    "    - It's possible to use up to 20 variables simultaneously in a hyperparameter tuning job\n",
    "- Choosing Hyperparameter Ranges\n",
    "    - better results can obtain by searching only in a small range where all possible values in the range are reasonable.\n",
    "- Use Logarithmic Scales for Hyperparameters\n",
    "    - could improve hyperparameter optimization\n",
    "- Choosing the Best Degree of Parallelism\n",
    "    - running in parallel gets more work done quickly\n",
    "    - running one training job at a time achieves the best results with the least amount of compute time. \n",
    "- Running Training Jobs on Multiple Instances\n",
    "    - hyperparameter tuning uses the last-reported objective metric from all instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Using Notebook Instances\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)\n",
    "- Creating a Notebook Instance\n",
    "- Accessing Notebook Instances\n",
    "- Using Example Notebooks\n",
    "- Set the Notebook Kernel\n",
    "- Installing External Libraries and Kernels in Notebook Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Using Built-in Algorithms\n",
    "Because a model is created to **address a business question**, the **first** step is to **understand the problem** needed to solve. Specifically, the format of the answer influences the algorithm.\n",
    "\n",
    "**Examples:**\n",
    "- Answers that fit into discrete categories >>> use *Linear Learner* and *XGBoost*\n",
    "- Answers that are quantitative >>> also use *Linear Learner* and *XGBoost*\n",
    "- Answers in the form of discrete recommendations >>> use *Factorization Machines*\n",
    "- Classify customer >>> K-Means Algorithm\n",
    "- understand customer attributes >>> PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Common Information\n",
    "### 6.1.1 Common Parameters \n",
    "\n",
    "**Computer resources**\n",
    "![](https://imgur.com/f3gNKOm.png)\n",
    "\n",
    "\n",
    "**AWS region**<br>\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n",
    "\n",
    "### 6.1.2 Common Data Formats \n",
    "#### Training\n",
    "- Training Data Formats \n",
    "    - CSV\n",
    "    - protobuf [recordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html#data-format) format\n",
    "- Trained Model Deserialization\n",
    "    - Amazon SageMaker models are stored as **model.tar.gz** in the S3 bucket specified in *OutputDataConfig S3OutputPath* parameter of the *create_training_job* call.\n",
    "\n",
    "#### Inference\n",
    "- Inference Request Serialization\n",
    "    - text/csv, \n",
    "    - application/json, \n",
    "    - application/x-recordio-protobuf.\n",
    "    - text/x-libsvm\n",
    "- Inference Response Deserialization \n",
    "    - Amazon SageMaker algorithms return JSON in several layouts.\n",
    "- Common Request Formats for All Algorithms \n",
    "    - JSON\n",
    "    - JSONLINES\n",
    "    - CSV\n",
    "    - RECORDIO\n",
    "- Using Batch Transform with Build-in Algorithms \n",
    "    - JSONLINES\n",
    "\n",
    "### 6.1.3 Suggested Instance Types \n",
    "For training and hosting Amazon SageMaker algorithms, we recommend using the following EC2 instance types:\n",
    "- ml.m4.xlarge, ml.m4.4xlarge, and ml.m4.10xlarge\n",
    "- ml.c4.xlarge, ml.c4.2xlarge, and ml.c4.8xlarge\n",
    "- ml.p2.xlarge, ml.p2.8xlarge, and ml.p2.16xlarge\n",
    "\n",
    "### 6.1.4 Logs \n",
    "**Note**\n",
    "If a job fails and logs do not appear in CloudWatch, it's likely that an error occurred before the start of training. Reasons include specifying the wrong training image or S3 location.\n",
    "\n",
    "The contents of logs vary by algorithms. However, you can typically find the following information:\n",
    "- Confirmation of arguments provided at the beginning of the log\n",
    "- Errors that occurred during training\n",
    "- Measurement of an algorithms accuracy or numerical performance\n",
    "- Timings for the algorithm, and any major stages within the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 BlazingText\n",
    "[link](https://dl.acm.org/citation.cfm?doid=3146347.3146354)\n",
    "- A highly optimized implementations of the **Word2vec** (sentiment analysis, named entity recognition, machine translation) and **text classification** (web search, information retrieval, ranking and document classification) algorithms.\n",
    "- Similar to Word2vec, it provides the Skip-gram and continuous bag-of-words (CBOW) training architectures.\n",
    "\n",
    "**BlazingText provides the following features**:\n",
    "- Accelerated training of fastText text classifier on multi-core CPUs or a GPU and Word2Vec on GPUs using highly optimized CUDA kernels.\n",
    "- Enriched Word Vectors with Subword Information by learning vector representations for character n-grams.\n",
    "- A batch_skipgram mode for the Word2Vec algorithm that allows faster training and distributed computation across multiple CPU nodes. \n",
    "\n",
    "### 6.2.1 Input/Output Interface\n",
    "### 6.2.2 Training and Validation Data Format\n",
    "- Word2Vec algorithm: a training sentence per line\n",
    "- Text Classification algorithm: a training sentence per line along with the labels\n",
    "### 6.2.3 Model artifacts and Inference \n",
    "- Word2Vec algorithm: *vectors.txt* which contains words to vectors mapping (compatible with other tools like Gensim and Spacy) and *vectors.bin*.\n",
    "- Text Classification algorithm: model.bin \n",
    "### 6.2.4 EC2 Instance [Recommendation]()\n",
    "### 6.2.5 BlazingText Sample [Notebooks](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_text_classification_dbpedia/blazingtext_text_classification_dbpedia.ipynb)\n",
    "\n",
    "### 6.2.6 [Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html)\n",
    "#### Word2Vec Hyperparameters\n",
    "- mode, batch_size, buckets, epochs, evaluation, learning_rate\n",
    "- min_char, min_count, max_char, negative_samples\n",
    "- sampling_threshold, subwords, vector_dim, window_size\n",
    "#### Text Classification Hyperparameters\n",
    "- mode, buckets, early_stopping, epochs, learning_rate\n",
    "- min_count, min_epochs, patience, vector_dim, word_ngrams\n",
    "\n",
    "### 6.2.7 [Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html) a BlazingText Model\n",
    "#### Metrics Computed by the BlazingText Algorithm \n",
    " <div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b7b7\">\n",
    "          <tr>\n",
    "             <th>Metric Name</th>\n",
    "             <th>Description</th>\n",
    "             <th>Optimization Direction</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">train:mean_rho</code></td>\n",
    "             <td>\n",
    "                <p>Mean rho (Spearman's rank correlation coefficient) on <a href=\"https://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\" target=\"_blank\">WS-353 word similarity datasets</a>.\n",
    "                </p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>Maximize</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">validation:accuracy</code></td>\n",
    "             <td>\n",
    "                <p>Classification accuracy on user specified validation dataset.\n",
    "                </p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>Maximize</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    " </div>\n",
    "\n",
    "#### Tunable Hyperparameters for Word2Vec\n",
    "<div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b9b3b5\">\n",
    "          <tr>\n",
    "             <th>Parameter Name</th>\n",
    "             <th>Parameter Type</th>\n",
    "             <th>Recommended Ranges</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">batch_size</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[8-32]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">epochs</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-15]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">learning_rate</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>MinValue: 0.005, MaxValue: 0.01</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">min_count</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[0-100]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">mode</code></td>\n",
    "             <td>\n",
    "                <p>CategoricalParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>['batch_skipgram', 'skipgram', 'cbow']</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">negative_samples</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-25]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">sampling_threshold</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "               <p>MinValue: 0.0001, MaxValue: 0.001</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">vector_dim</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[32-300]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">window_size</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1-10]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    " </div>\n",
    " \n",
    "#### Tunable Hyperparameters for Text Classification\n",
    " <div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b9b5b5\">\n",
    "          <tr>\n",
    "             <th>Parameter Name</th>\n",
    "             <th>Parameter Type</th>\n",
    "             <th>Recommended Ranges</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">buckets</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1000000-10000000]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">epochs</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-15]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">learning_rate</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>MinValue: 0.005, MaxValue: 0.01</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">min_count</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[0-100]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">mode</code></td>\n",
    "             <td>\n",
    "                <p>CategoricalParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>['supervised']</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">vector_dim</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[32-300]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">word_ngrams</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1-3]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 DeepAR Forecasting\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)\n",
    "\n",
    "Amazon SageMaker DeepAR is a supervised learning algorithm for forecasting scalar (that is, one-dimensional) **time series** using recurrent neural networks (**RNN**) - **ARIMA, ETS**\n",
    "\n",
    "Examples of such time series groupings are **demand** for different **products**, **server loads**, and **requests** for web pages. In this case, it can be beneficial to train a single model jointly over all of these time series. \n",
    "\n",
    "**Topics**:\n",
    "### 6.3.1    Input/Output Interface\n",
    "- DeepAR supports two data channels **train** and **test** with *JSONLINES* file format (either ***.json*** or ***.json.gz*** or ***.parquet***)\n",
    "- Use RMSE or weighted quantile loss for evaluation\n",
    "     \n",
    "     $$\\text{RMSE} = \\sqrt{\\frac{1}{nT}\\sum_{i,t}{\\left(\\hat{y}_{i,t}-y_{i.t}\\right)^2}}$$\n",
    "     $$\\text{wQuantileLoss}[\\tau] = 2\\frac{\\sum_{i,t}{Q_{i,t}^{(\\tau)}}}{\\sum_{i,t}{|y_{i,t}|}}\\quad\\text{with}\\quad Q_{i,t}^{(\\tau)}=\\begin{cases}(1-\\tau)|q_{i,t}^{(\\tau)}-y_{i,t}| & \\text{ if } q_{i,t}^{(\\tau)}>y_{i,t}\\\\ \\tau|q_{i,t}^{(\\tau)}-y_{i,t}| & \\text{ otherwise } \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2    Recommended Best Practices\n",
    "- always provide entire time series for training, testing, and when calling the model for prediction\n",
    "- dataset can be split into training and test datasets for tuning a DeepAR Model at different end points\n",
    "- do not use very large values (>400) for *prediction length*\n",
    "- use the same values for *prediction* and *context* lengths.\n",
    "- train DeepAR model on as **many time series** as available\n",
    "### 6.3.3 EC2 Instance Recommendations\n",
    "- can use GPU and CPU\n",
    "- use large machine for large model size\n",
    "    \n",
    "### 6.3.4 DeepAR Sample Notebooks: \n",
    "[Time series forecasting with DeepAR - Synthetic data](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb)\n",
    "\n",
    "### 6.3.5 How DeepAR Works\n",
    "- Under the Hood:<br>\n",
    "    DeepAR automatically creates feature time series<br>\n",
    "    DeepAR model is trained by randomly sampling several training examples from each of the time series in the training dataset.<br>\n",
    "    To capture seasonality patterns, DeepAR also automatically feeds lagged values from the target time series. <br>\n",
    "    For inference, the trained model takes as input target time series, which might or might not have been used during training, and forecasts a probability distribution for the next prediction_length values.\n",
    "    \n",
    "### 6.3.6 [DeepAR Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)\n",
    "- context_length, prediction_length\n",
    "- epochs (+ early_stopping_patience)\n",
    "- time_freq (every minutes, hourly, daily, weekly, monthly)\n",
    "- cardinality (for categorical)\n",
    "- dropout_rate, embedding_dimension, learning_rate\n",
    "- likelihood (gaussian, beta, negative-binomial, student-T, deterministic-L1)\n",
    "- mini_batch_size, num_cells, num_dynamic_feat, num_eval_samples, num_layers, test_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.7 Tuning a DeepAR Model\n",
    "- **Metrics Computed by the DeepAR Algorithm**:\n",
    "<table id=\"w1649aac23c28c21b7b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:RMSE</code></td>\n",
    "     <td>\n",
    "        <p>Root mean square error between forecast and actual target computed on the test set.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:mean_wQuantileLoss</code></td>\n",
    "     <td>\n",
    "        <p>Average overall quantile losses computed on the test set. Setting the <code class=\"code\">test_quantiles</code> hyperparameter controls which quantiles are used. \n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">train:final_loss</code></td>\n",
    "     <td>\n",
    "        <p>Training negative log-likelihood loss averaged over the last training epoch for the model.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "</table>    \n",
    "- **Tunable Hyperparameters**:\n",
    "<table id=\"w1649aac23c28c21b9b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 32, MaxValue: 1028</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">epochs</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 1000</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">context_length</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 200</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_cells</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 30, MaxValue: 200</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_layers</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 8</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">dropout_rate</code></td>\n",
    "     <td>\n",
    "        <p>ContinuousParameterRange</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 0.00, MaxValue: 0.2</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">embedding_dimension</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 50</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">learning_rate</code></td>\n",
    "     <td>\n",
    "        <p>ContinuousParameterRange</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1e-5, MaxValue: 1e-1</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.8 DeepAR Inference Formats\n",
    "[**JSON**](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Factorization Machines\n",
    "- A factorization machine is a **general-purpose supervised learning** algorithm that can be used for both **classification** and **regression** tasks. \n",
    "- It is an **extension of a linear model** that is designed to capture interactions between features within high dimensional sparse datasets economically.\n",
    "\n",
    "### 6.4.1 Input/Output Interface\n",
    "- RMSE, Log Loss, Accuracy, F1-score\n",
    "- application/json, x-recordio-protobuf\n",
    "\n",
    "### 6.4.2 EC2 Instance Recommendation\n",
    "- CPUs instances\n",
    "\n",
    "### 6.4.3 Factorization Machines Sample Notebooks\n",
    "- [ An Introduction to Factorization Machines with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/factorization_machines_mnist/factorization_machines_mnist.ipynb)\n",
    "\n",
    "### 6.4.4 How Factorization Machines Work\n",
    "- for prediction task, [FM](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) estimates a function $\\hat{y}$ from a feature set $x_i$ to a target domain:\n",
    "$$\\hat{y}=w_0+\\sum_i w_ix_i +\\sum_i\\sum_{j>i}<v_i,v_j>x_ix_j$$\n",
    "- for regression task, FM minimizes:\n",
    "$$L=\\frac{1}{N}\\sum_n(y_n-\\hat{y}_n)^2$$\n",
    "- for classification task, FM minimizes:\n",
    "$$L=\\frac{1}{N}\\sum_n\\left[y_n\\log\\hat{p}_n+(1-y_n)\\log(1-\\hat{p}_n)\\right]\\text{ where } \\hat{p}_n=\\frac{1}{1+e^{-\\hat{y}_n}}$$\n",
    "\n",
    "### 6.4.5 Factorization Machines Hyperparameters\n",
    "- feature_dim, num_factors,\n",
    "- predictor_type (binary_classifier, regressor)\n",
    "- bias_init_method, bias_init_scale, bias_init_sigma\n",
    "- bias_init_value, bias_lr, bias_wd, \n",
    "- clip_gradient, epochs, eps\n",
    "- factors_init_method, factors_init_scale, factors_init_sigma, factors_init_value, factors_lr, factors_wd, \n",
    "- linear_lr, linear_init_method, linear_init_scale, linear_init_sigma, linear_init_value, linear_wd, \n",
    "- mini_batch_size, rescale_grad\n",
    "\n",
    "### 6.4.6 Tuning a Factorization Machines Model\n",
    "- **Metrics Computed by the Factorization Machines Algorithm**\n",
    "<table>\n",
    "<tr>\n",
    "    <th>a</th>\n",
    "    <th>a</th>\n",
    "    <th>a</th>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### 6.4.7 Factorization Machine Response Formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 6.5 Image Classification Algorithm\n",
    "## 6.6 K-Means Algorithm\n",
    "## 6.7 K-Nearest Neighbors\n",
    "## 6.8 Latent Dirichlet Allocation (LDA)\n",
    "## 6.9 Linear Learner\n",
    "## 6.10 Neural Topic Model (NTM)\n",
    "## 6.11 Object Detection Algorithm\n",
    "## 6.12 Principal Component Analysis (PCA)\n",
    "## 6.13 Random Cut Forest\n",
    "## 6.14 Sequence to Sequence (seq2seq)\n",
    "## 6.15 XGBoost Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
