{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building 3-layer neural network from numpy\n",
    "Letâ€™s start by importing some libraires required for creating our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np ## Fr numerical python\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every layer will have a forward pass and backpass implementation. let's create a mian class layer which can do a forward pass .*forward()* and backward pass .*backward()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # A building block. Each layer is capable of performing two things:\n",
    "    # - process inpput to get output \n",
    "    # output = layer.forward(input)\n",
    "    \n",
    "    # - propagate gradients through itself:\n",
    "    # grad_input = layer.backward(input, gradoutput)\n",
    "    \n",
    "    # Some layers also have learnable parameters which they update \n",
    "    # during layer.backward.\n",
    "    \n",
    "    def __init__(self):\n",
    "        # here we can initialize layer parameters (if any) \n",
    "        # and auxiliary stuff\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    def forward(self, input_):\n",
    "        # Takes input data of shape [batch, input_units], \n",
    "        #return ouput data [batch, output_units]\n",
    "        \n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input_\n",
    "    def backward(self, input_, grad_output):\n",
    "        # Performs a backpropagation step through the layer, \n",
    "        # with respect to the given input.\n",
    "        # To compute loss gradients w.r.t input, we need to apply \n",
    "        # chain rule (backprop):\n",
    "        # d_loss/d_x = (d_loss/d_layer) * (d_layer/d_x)\n",
    "        # luckily, we already receive d_loss/d_layer as input, \n",
    "        # so you only need to multiply it by d_layere/d_x.\n",
    "        # If our layer has parameters (e.g. dense layer), \n",
    "        # we also need to update them here using d_loss/d_layer\n",
    "        # Thegradient of dummy layer is precisely grad_output, \n",
    "        # but we'll write it more explicity\n",
    "        num_units = input_.shape[1]\n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearity ReLU layer\n",
    "This is the simplest layer you can get: it simply applies a nonlinarity to each element of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        # ReLu layer simply applies elementwise rectified linear unit \n",
    "        # to all inputs\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        # Apply elementwise ReLU to [batch, input_units] matrix\n",
    "        relu_forward = np.maximum(0, input_)\n",
    "        return relu_forward\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        # Compute gradient of loss w.r.t ReLU input\n",
    "        relu_grad = input_ > 0\n",
    "        return grad_output*relu_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layer\n",
    "Now let's build somethign more complicate. Unlike nonlinearity, a dense layer actully has something to learn. <br>\n",
    "A dense layer applies affine transformation. in a vectorized form, it can be described as:\n",
    "$$f(\\mathbf{X}) = \\mathbf{W} . \\mathbf{X} + \\mathbf{b}$$\n",
    "Where:<br>\n",
    "- $\\mathbf{X}$ is an object-feature matrix of shape [batch_size, num_features], <br>\n",
    "- $\\mathbf{W}$ is a weight matrix [num_features, num_outputs]<br>\n",
    "- and $\\mathbf{b}$ is a vector of num_outputs biases.\n",
    "\n",
    "\n",
    "Both $\\mathbf{W}$ and $\\mathbf{b}$ are initialized during layer creation and updated each time backward is called. Note that we are using **Xavier initialization** which is a trick to train our model to converge faster. Instead of initializing our weights with small numbers which are distributed randomly, we initialize our weights with mean zero and variance of 2/(number of inputs + number of outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        # A dense layer is a layer which performs a learned affine \n",
    "        # transformation:\n",
    "        # f(X) = W*X + b\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.normal(\n",
    "            loc=0.0, \n",
    "            scale=np.sqrt(2/(input_units+output_units)),\n",
    "            size=(input_units, output_units)\n",
    "        )\n",
    "        self.biases = np.zeros(output_units)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        # perform an affine transformation:\n",
    "        # f(X) = W*X + b\n",
    "        \n",
    "        # input shape: [batch, input_units]\n",
    "        # output shape: [batch, output_units]\n",
    "        \n",
    "        return np.dot(input_, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, input_, grad_output):\n",
    "        # compute d_f/d_x = d_f/d_dense * d_dense/d_x\n",
    "        # where d_dense/d_x = weights transposed\n",
    "        # and d_f/d_dense = grad_output\n",
    "        \n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        \n",
    "        # compute gradient w.r.t. weights and bias (things to update)\n",
    "        grad_weights = np.dot(input_.T, grad_output)\n",
    "        grad_biases = grad_output.mean(axis=0)*input_.shape[0]\n",
    "        \n",
    "        assert grad_weights.shape == self.weights.shape\\\n",
    "               and grad_biases == self.biases.shape\n",
    "        \n",
    "        # Here we perform a schochastic gradient descent step.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loss function\n",
    "Since we want to predict probabilities, it would be logical for us to define softmax nonlinearity on top of our network and compute loss given predicted probabilities. However, there is a better way to do so.\n",
    "\n",
    "If we write down the expression for crossentropy as a function of softmax logits(a), you'll see:\n",
    "$$loss = -\\log\\frac{e^{\\alpha_{\\text{correct}}}}{\\sum_i{e^{\\alpha_i}}}$$\n",
    "If we take a closer look, we'll see that it can be written as:\n",
    "$$loss = -\\alpha_{\\text{correct}}+\\log\\sum_i{e^{\\alpha_i}}$$\n",
    "It's called Log-softmax and it's better than naive log(softmax(a)) in all aspects:\n",
    "\n",
    "- Better numerical stability\n",
    "- Easier to get derivative rigt\n",
    "- Marginally faster to compute\n",
    "\n",
    "So why not just use log-softmax throughout our computation and never actually bother to estimate probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
