{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1 What is Deep Reinforcement Learning?\n",
    "Link = https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reinforcement Learning Process\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*aKYFRoEmmKkybqJOvLt2JQ.png?raw=true)\n",
    "\n",
    "This RL loops output a sequnce of **state**, **action** and **reward**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The central idea of the Reward Hypothesis\n",
    "\n",
    "Cumulative reward at each time step $t$:\n",
    "\n",
    "$$G_t=\\sum_{k=0}^{T}R_{t+k+1}\\tag{1.1}$$\n",
    "\n",
    "The rewards that come sooner are more probable to happen, since they are more predictable than the long term future reward.\n",
    "\n",
    "$$G_t=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}\\text{, where } 0\\leq\\gamma<1\\tag{1.2}$$\n",
    "\n",
    "- the larger $\\gamma$ the smaller the discount\n",
    "- the smaller $\\gamma$ the bigger the discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episodic or Continuing tasks\n",
    "\n",
    "**Episodic task** has an starting and an ending points<br>\n",
    "**continuous task** does not have ending point. The agent keeps running until we decide to stop him."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo vs TD Learning methods\n",
    "\n",
    "In **Monte Carlo Approach** we collect the rewards **at the end of the episode** and then *calculate* the **maximum expected future reward**.\n",
    "\n",
    "$$V(S_t)\\leftarrow V(S_t) + \\alpha[G_t-V(S_t)]\\tag{1.3}$$\n",
    "\n",
    "In **Temporal difference learning** we estimate the **reward at *each step***\n",
    "\n",
    "$$\\displaystyle V(S_t) \\leftarrow \\underbrace{V(S_t)}_{\\text{previous estimate}} + \\alpha\\left[\\overbrace{\\underbrace{R_{t+1}}_{\\text{reward at t+1}}+\\underbrace{\\gamma V(S_{t+1})}_{\\text{discount value on next step}}}^{\\text{TD target}} \\right]\\tag{1.4}$$\n",
    "\n",
    "By running more and more episodes, **the agent will learn to play better and better**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration/Exploitation trade-off\n",
    "\n",
    "- **Exploration** is finding more information about the environment\n",
    "- **Exploitation** is exploiting known information to maximize the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three approaches to Reinforcement Learning\n",
    "#### Value-based\n",
    "Value-based RL optimizes the value function\n",
    "<br>***Value function** is a function that tells us the maximum expected future reward the agent will get at each step*.\n",
    "$$v_\\pi(s) =\\mathbb{E}\\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\mid S_t=s \\right]\n",
    "\\tag{1.5}$$\n",
    "\n",
    "#### Policy based\n",
    "Policy based RL directly optimizes the policy function $\\pi(s)$ without using a value function.\n",
    "\n",
    "$$\\underbrace{a\\quad=\\quad\\pi(s)}_{\\text{action = policy(function)}}\n",
    "\\tag{1.6}$$\n",
    "\n",
    "- *Deterministic* policy will always return the same action at a given state.\n",
    "- *Stochastic* policy outputs a distribution probability over actions\n",
    "\n",
    "$$\\pi(a|s) =\\mathbb{P}[A_t=a|S_t=s]\n",
    "\\tag{1.7}$$\n",
    "\n",
    "#### Model Based\n",
    "Model based RL models the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Deep Reinforcement Learning\n",
    "Deep reinforcement learning introduces deep neural networks to solve Reinforcement Learning problems--hence the name \"deep\".\n",
    "![](https://cdn-images-1.medium.com/max/1395/1*w5GuxedZ9ivRYqM_MLUxOQ.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-2 Diving deeper into Reinforcement Learning with Q-Learning\n",
    "Link = https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe\n",
    "\n",
    "## Introducing the Q-table\n",
    "In **Q-table**, the columns will be actions $a$, the rows will be states $s$, the value of each cell will bethe maximum expected future reward for that given state and action $Q^*(x,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning algorithm: laerning the Action Value Function\n",
    "$$Q^\\pi(s_t,a_t)=\\mathbb{E}\\left[\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\\mid s_t,a_t\\right]\\tag{2.1}$$\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*yklmxNRdXleiDbv6aSZUIg.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q-learning algorithm Process\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*QeoQEqWYYPs1P8yUwyaJVQ.png?raw=true)\n",
    "\n",
    "1. $\\ $ Initialize Q-values $Q(s,a)$ arbitrarily for all state-action pairs\n",
    "2. $\\ $For life or until learning is stopped \n",
    "3. $\\quad$ Choose an action $(a)$ in the current world state $(s)$ based on current Q-value estimates $Q(s,\\cdot)$\n",
    "4. $\\quad$ Take the action $(a)$ and observe the outcome state $(s')$ and reward $(r)$\n",
    "5. $\\quad$ Update $Q(s,a):=Q(s,a)+\\alpha[r+\\gamma\\max_{a'}Q(s',a')-Q(s,a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose an action at step 3?\n",
    "The idea is that in the beginning, we'll use the **epsilon greedy strategy**:\n",
    "- We specify an exploration ratie \"epsilon\", which we set to 1 in the beginning. This is the rate of steps that we'll do randomly. In the beginning, this rate must be at its highest value, because we don't know anything about the values in Q-table. This means we need to do alot of exploration, by randomly chossing our actions.\n",
    "- We generate a random number. If this number is larger \"epsilon\", the we will do \"exploitation\" (this means we use waht we already know to select the best action at each step). Else, we'll do exploration.\n",
    "- The idea is that we must have a big epsilon at the beginnign of the training of the Q-function. Then, reduce it progressively as the agent becomes more confident at estimating Q-values.\n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*9StLEbor62FUDSoRwxyJrg.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap ...\n",
    "- Q-learning is a value-based Reinforcement Learning algorithm that is used to find the optimal action-selection policy using a q function.\n",
    "- It evaluates which action to take based on an action-value function that determines the value of being in a certain state and taking a certain action at that state.\n",
    "- Goal: maximize the value function Q (expected future reward given a state and action).\n",
    "- Q table helps us to find the best action for each state.\n",
    "- To maximize the expected reward by selecting the best of all possible actions.\n",
    "- The Q come from quality of a certain action in a certain state.\n",
    "- Function Q(state, action) → returns expected future reward of that action at that state.\n",
    "- This function can be estimated using Q-learning, which iteratively updates Q(s,a) using the Bellman Equation\n",
    "- Before we explore the environment: Q table gives the same arbitrary fixed value → but as we explore the environment → Q gives us a better and better approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-3 Deep Q-Learning\n",
    "# An introduction to Deep Q-Learning: let’s play Doom\n",
    "Link = https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ‘Deep’ to Q-Learning\n",
    "Q-learning updates Q-table, this is a good strategy but not scalable (when size of state and action state are **giant**).\n",
    "\n",
    "Create a neural network that will approximate, given a state, the different Q-values for each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Deep Q-Learning work?\n",
    "![](https://cdn-images-1.medium.com/max/1395/1*LglEewHrVsuEGpBun8_KTg.png?raw=true)\n",
    "\n",
    "Our Deep Q Neural Network takes a stack or four frams as an input. These pass through its network, and output a vector of Q-values for each action possible in the given state. We need to take the biggest Q-value of this vector to find our best action (move left/right or shoot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing part\n",
    "![](https://cdn-images-1.medium.com/max/1116/1*QgGnC_0BkQEtPqMUftRC6A.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem of temporal limitation\n",
    "Link = https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\n",
    "\n",
    "The first question that you can ask is why we stack frames together?\n",
    "\n",
    "We stack frames together because it helps us to handle the problem of temporal limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using convolution networks\n",
    "#### ELU function\n",
    "Link = https://arxiv.org/pdf/1511.07289.pdf\n",
    "$$\n",
    "f(x)=\n",
    "\\left\\{\\begin{matrix}\n",
    " x & \\text{if } x> 0 \\\\  \n",
    " \\alpha(\\exp(x)-1)& \\text{otherwise} \n",
    "\\end{matrix}\\right.\n",
    "\\tag{3.1}$$\n",
    "\n",
    "$$\n",
    "f'(x)=\n",
    "\\left\\{\\begin{matrix}\n",
    " 1 & \\text{if } x> 0 \\\\  \n",
    " f(x)+\\alpha& \\text{otherwise} \n",
    "\\end{matrix}\\right.\n",
    "\\tag{3.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay: making more efficient use of observed experience\n",
    "### Avoid forgetting previous experiences\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*RFt8MBBkUSPZdolp_WfZFA.png?raw=true)\n",
    "Think of the **replay buffer** as a folder where every sheet is an experiment tuple (state, action, reward). We feed it by interacting with the environment. And then we take some random sheet to feed the newral network.\n",
    "### Reducing correlation between experiences\n",
    "First, we must stop learning while interacting with the environment. We shouldtry different things and play a little randomly to explore the state space. We can save these experiences in the replay buffer.\n",
    "\n",
    "Then, we can recall these experiences and learn from them. After that, go back to play with updated value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning algorithm\n",
    "The error is calculated by taking the difference between our Q_target (maximum possible value from the next state) and Q_value (our current prediction of the Q-value)\n",
    "\n",
    "$$\\underbrace{\\Delta w}_{\\text{change in weights}}=\\alpha\\left[\\overbrace{\\underbrace{R+\\gamma\\max_a\\hat{Q}(s',a,w)}_{\\text{Q-target}}-\\underbrace{\\hat{Q}(s,a,w)}_{\\text{Q-value}}}^{\\text{Temporal difference error}}\\right]\\underbrace{\\nabla_w\\hat{Q}(s,a,w)}_{\\text{gradient of current predicted Q-value}} \\tag{3.3}$$\n",
    "\n",
    "There are two processes that are happening in this algorithm:\n",
    "- We sample the environment where we performan actions and store the observed experiences tuples in a replay memory.\n",
    "- Select the small batch of tuple random and learn from it using a gradient descent update step."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initialize Doom Environment E\n",
    "Initialize replay Memory M with capacity N (=finite capacity)\n",
    "Initialize the DQN weights w\n",
    "\n",
    "for episode in max_epidose:\n",
    "    s = Environment state\n",
    "    for steps in max_step:\n",
    "        Choose action a from state s uing epsilon greedy.\n",
    "        Taken action a, get r (reward) and s' (next state)\n",
    "        Store experience tuple<s,a,r,s'> in M\n",
    "        s = s' (state = new_state\n",
    "        \n",
    "        Get random minibatch of exp tuples from M\n",
    "        Set Q-target = reward(s,a) +  γmaxQ(s')\n",
    "        Update w =  α(Q_target - Q_value) *  ∇w Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
