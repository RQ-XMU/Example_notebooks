{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. What Is Amazon SageMaker?\n",
    "- Amazon SageMaker is a **fully managed machine learning service**.\n",
    "- helps to **quickly and easily build and train machine learning models**, and then **directly deploy** them into a production-ready hosted environment.\n",
    "- provides an **integrated Jupyter** authoring **notebook** instance for **easy access** to your **data** sources\n",
    "- provides **common machine learning algorithms** that are **optimized** to run efficiently against **extremely large data** in a **distributed environment**.\n",
    "- Deploy **with a single click console**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. How It Works\n",
    "This section provides an **overview of machine learning** and explains how Amazon SageMaker works.\n",
    "## 2.1. Machine Learning with Amazon SageMaker\n",
    "Typical workflow for creating a machine learning model:\n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/ml-concepts-10.png)\n",
    "1. **Generate example data**<br>\n",
    "    Data type depends on business problem<br>\n",
    "    a. **Fetch the data** — \n",
    "    pull the dataset or datasets into a single repository<br>\n",
    "    b. **Clean the data** — \n",
    "    To improve model training<br>\n",
    "    c. **Prepare or transform the data** — \n",
    "    To improve performance\n",
    "2. **Train a model**:<br>\n",
    "    a. **Training** the model — To train a model, you need<br>\n",
    "    - an algorithm (can be provided by Amazon Sagemaker or self-implementation)\n",
    "    - compute resources<br>\n",
    "    b. **Evaluating** the model — \n",
    "    to determine whether the accuracy of the inferences is acceptable.<br>\n",
    "    -.use either the AWS SDK for Python (Boto) or the high-level Python library in SageMaker<br>\n",
    "    -.use a Jupyter notebook in SageMaker notebook instance to train and evaluate model.\n",
    "3. **Deploy the model** — independently with Amazon SageMaker hosting services, decoupling from application code. \n",
    "\n",
    "Machine learning is a **continuous cycle**: \n",
    "- => **deploy** model\n",
    "- => **monitor** inferences\n",
    "- => **collect** \"ground truth\"\n",
    "- => **evalutate** model\n",
    "- => **retrain** model\n",
    "- => **deploy new** model\n",
    "- => ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Explore and Preprocess Data\n",
    "\n",
    "- Use a Jupyter notebook on an Amazon SageMaker \n",
    "- use a model to transform data by using Amazon SageMaker batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training a Model with Amazon SageMaker\n",
    "\n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture.png)\n",
    "\n",
    "Create a training job by SageMaker Conslole or API, including:\n",
    "- URL to data on S3\n",
    "- Compute resources, managed by Amazon SageMaker\n",
    "- URL for output data on S3\n",
    "- Amazon Elastic Container Registry path where the training code is stored\n",
    "\n",
    "**Training** options:\n",
    "- Use an algorithm provided by Amazon SageMaker\n",
    "- Use Apache Spark with Amazon SageMaker, similarly to use Spark MLLib\n",
    "- Submit custom code to train with deep learning frameworks: [TensorFlow](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html), [Apache MXNet](https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet.html)\n",
    "- Use custom algorithms in  a Docker image\n",
    "\n",
    "**WorkFlow**:\n",
    "- User creates the training job\n",
    "- => Amazon SageMaker launches the ML compute instances\n",
    "- => SageMaker uses the training code and the training dataset to train the model\n",
    "- =>SageMaker saves the resulting model artifacts and other output in the S3 bucket \n",
    "\n",
    "**Important**\n",
    "- Prevent out-of-memory error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Model Deployment in Amazon SageMaker\n",
    "- use Amazon **[SageMaker hosting services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)** to set up a persistent endpoint to get **one prediction** at a time, .\n",
    "- use Amazon **[SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)** to get **predictions** for an **entire dataset**.\n",
    "\n",
    "### 2.4.1. Deploying a Model on Amazon SageMaker Hosting Services\n",
    "A three-step process:\n",
    "1. Create a model in Amazon SageMaker for finding model components\n",
    "2. Create an endpoint configuration for an HTTPS endpoint, configure the endpoint to elastically scale the deployed ML compute instances in production\n",
    "3. Create an HTTPS endpoint: endpoint configuration to Amazon SageMaker\n",
    "\n",
    "**Considerations**\n",
    "- client application sends requests to the Amazon SageMaker HTTPS endpoint to obtain inferences from a deployed model, requests can be sent to this endpoint from Jupyter notebook during testing.\n",
    "- model trained with Amazon SageMaker can be deployed to specific deployment target.\n",
    "- multiple variants of a model can be deployed to the same Amazon SageMaker HTTPS endpoint.\n",
    "- a *ProductionVariant* can be configured to use Application Auto Scaling\n",
    "- an endpoint can be modified without taking models that are already deployed into production out of service.\n",
    "- **Changing or deleting** model artifacts or changing inference code **after deploying** a model produces **unpredictable results**.\n",
    "\n",
    "### 2.4.2. Getting Inferences by Using Amazon SageMaker Batch Transform \n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/batch-transform.png)\n",
    "\n",
    "Batch transform manages all compute resources necessary to get inferences. This includes launching instances and deleting them after the transform job completes. \n",
    "\n",
    "To perform a batch transform, create a transform job including:\n",
    "- path to data on S3 bucket\n",
    "- compute resources\n",
    "- path to S3 for output data\n",
    "- name of model in the transform job\n",
    "\n",
    "Batch transform is ideal for situations where:\n",
    "- You want to get inferences for an entire dataset and store them online.\n",
    "- You don't need a persistent endpoint that applications can call to get inferences.\n",
    "- You don't need the sub-second latency that Amazon SageMaker hosted endpoints provide.\n",
    "- You want to preprocess your data before using the data to train a new model or generate inferences.\n",
    "\n",
    "**Considerations**:\n",
    "- transform job can be created by SageMaker Console or API\n",
    "- Amazon SageMaker follows the transform job to read input, launches ML and save output\n",
    "- Amazon SageMaker uses Multipart Upload API to upload output data results from a transform job to S3. \n",
    "- For testing model variants, create separate transform jobs for each variant using a validation data set.\n",
    "- For large datasets or data of indeterminate size, create an infinite stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Validating Machine Learning Models \n",
    "\n",
    "After training, a model needs to be evaluated to determine whether its performance and accuracy allow to achieve business goals.\n",
    "\n",
    "- **Offline testing**: Deploy trained model to an alpha endpoint, and use historical data to send inference requests to it.\n",
    "- **Online testing** with live data: choose to send a portion of the traffic to a model variant for evaluation.\n",
    "\n",
    "**Options** for **offline** evaluation:\n",
    "- **using a \"holdout set\"**: use 20-30% of the training data for validation\n",
    "- **k-fold validation**: split data into k+1 folds, user 1 folds for validation & k folds for training; run k+1 times => k+1 models => aggregate to obtain final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 The Amazon SageMaker Programming Model \n",
    "\n",
    "- use SageMaker APIs to create and manage notebook instances and train and deploy models\n",
    "- alternatives:\n",
    "    - Use the Amazon SageMaker console\n",
    "    - Modify the example Jupyter notebooks\n",
    "    - Write model training and inference code from scratch\n",
    "        - high-level Python library\n",
    "        - AWS SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Getting Started\n",
    "\n",
    "## [3.1: Setting Up](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)\n",
    "### [3.1.1: Create an AWS Account and an Administrator User](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html)\n",
    "- [Create an AWS Account](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html#gs-account-create)\n",
    "- [Create an IAM Administrator User and Sign In](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html#gs-account-user)\n",
    "### [3.1.2: Create an S3 Bucket](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html)\n",
    "\n",
    "## [3.2: Create an Amazon SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "\n",
    "## [3.3: Train a Model with a Built-in Algorithm and Deploy It](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1.html)\n",
    "\n",
    "### [3.3.1: Create a Jupyter Notebook and Initialize Variables](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html)\n",
    "### [3.3.2: Download, Explore, and Transform the Training Data](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data.html)\n",
    "- 1: Download the MNIST Dataset\n",
    "- 2: Explore the Training Dataset\n",
    "- 3: Transform the Training Dataset and Upload It to S3\n",
    "\n",
    "### [3.3.3: Train a Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html)\n",
    "- 1: Choose the Training Algorithm\n",
    "- 2: Create a Training Job\n",
    "\n",
    "### [3.3.4: Deploy the Model to Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-model-deployment.html)\n",
    "- 1: Deploy the Model to Amazon **SageMaker Hosting** Services\n",
    "- 2: Deploy the Model to Amazon **SageMaker Batch Transform**\n",
    "\n",
    "### [3.3.5: Validate the Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html)\n",
    "\n",
    "## [3.4: Clean up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)\n",
    "## [3.5: Additional Considerations](https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Automatic Model Tuning\n",
    "- Automatic model tuning = **hyperparameter tuning**, finds the **best** version of a **model** by running **many training jobs** on dataset using the **algorithm** and **ranges of hyperparameters** that were specified.\n",
    "- use Amazon SageMaker automatic model tuning with **built-in** algorithms, **custom algorithms**, and Amazon SageMaker **pre-built containers** for machine learning frameworks\n",
    "- Before start hyperparameter tuning, it requires a well-defined machine learning problem, including:\n",
    "    - A **dataset**\n",
    "    - An understanding of the **type of algorithm** needed to train\n",
    "    - A clear understanding of **how to measure success**\n",
    "    \n",
    "## 4.1 How Hyperparameter Tuning Works\n",
    "- **Workflow**:\n",
    "    - hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results, \n",
    "    - runs training jobs to test these guesses. \n",
    "    - after testing uses regression to choose the next set of hyperparameter values\n",
    "- Hyperparameter tuning uses an Amazon SageMaker implementation of **Bayesian optimization**\n",
    "- Use explore/exploit trade-off strategy\n",
    "- **Note**:\n",
    "    - might not improve model\n",
    "    - exploring all of the possible combinations is impractical with complex model\n",
    "    - need to choose the right ranges to explore\n",
    "- **Refernces**:\n",
    "    - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning, [link](https://arxiv.org/abs/1012.2599)\n",
    "    - Practical Bayesian Optimization of Machine Learning Algorithms, [link](https://arxiv.org/abs/1206.2944)\n",
    "    - Taking the Human Out of the Loop: A Review of Bayesian Optimization, [link](http://ieeexplore.ieee.org/document/7352306/?reload=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Defining Objective Metrics\n",
    "- Not required for SageMaker built-in algorithms, just select and use\n",
    "- Required with regular expressions (**regex**) for custom algorithms. Algorthms needs to emit at least one metric by writing evaluation data to *stderr* or *stdout*\n",
    "- The hyperparameter tuning job returns the training job that returned the best value for the objective metric as the best training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Defining Hyperparameter Ranges\n",
    "Choosing hyperparameters and ranges significantly affects the performance of your tuning job.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"tree_method\",\n",
    "          \"Values\": [\"auto\", \"exact\", \"approx\", \"hist\"]\n",
    "        }          \n",
    "      ],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"eta\",\n",
    "          \"MaxValue\" : \"0.5\",\n",
    "          \"MinValue\": \"0\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"max_depth\",\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\", \n",
    "        }\n",
    "      ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Example: Hyperparameter Tuning Job\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Design Considerations\n",
    "- Choosing the Number of Hyperparameters\n",
    "    - It's possible to use up to 20 variables simultaneously in a hyperparameter tuning job\n",
    "- Choosing Hyperparameter Ranges\n",
    "    - better results can obtain by searching only in a small range where all possible values in the range are reasonable.\n",
    "- Use Logarithmic Scales for Hyperparameters\n",
    "    - could improve hyperparameter optimization\n",
    "- Choosing the Best Degree of Parallelism\n",
    "    - running in parallel gets more work done quickly\n",
    "    - running one training job at a time achieves the best results with the least amount of compute time. \n",
    "- Running Training Jobs on Multiple Instances\n",
    "    - hyperparameter tuning uses the last-reported objective metric from all instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Using Notebook Instances\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)\n",
    "- Creating a Notebook Instance\n",
    "- Accessing Notebook Instances\n",
    "- Using Example Notebooks\n",
    "- Set the Notebook Kernel\n",
    "- Installing External Libraries and Kernels in Notebook Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Using Built-in Algorithms\n",
    "Because a model is created to **address a business question**, the **first** step is to **understand the problem** needed to solve. Specifically, the format of the answer influences the algorithm.\n",
    "\n",
    "**Examples:**\n",
    "- Answers that fit into discrete categories >>> use *Linear Learner* and *XGBoost*\n",
    "- Answers that are quantitative >>> also use *Linear Learner* and *XGBoost*\n",
    "- Answers in the form of discrete recommendations >>> use *Factorization Machines*\n",
    "- Classify customer >>> K-Means Algorithm\n",
    "- understand customer attributes >>> PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Common Information\n",
    "### 6.1.1 Common Parameters \n",
    "\n",
    "**Computer resources**\n",
    "![](https://imgur.com/f3gNKOm.png)\n",
    "\n",
    "\n",
    "**AWS region**<br>\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n",
    "\n",
    "### 6.1.2 Common Data Formats \n",
    "#### Training\n",
    "- Training Data Formats \n",
    "    - CSV\n",
    "    - protobuf [recordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html#data-format) format\n",
    "- Trained Model Deserialization\n",
    "    - Amazon SageMaker models are stored as **model.tar.gz** in the S3 bucket specified in *OutputDataConfig S3OutputPath* parameter of the *create_training_job* call.\n",
    "\n",
    "#### Inference\n",
    "- Inference Request Serialization\n",
    "    - text/csv, \n",
    "    - application/json, \n",
    "    - application/x-recordio-protobuf.\n",
    "    - text/x-libsvm\n",
    "- Inference Response Deserialization \n",
    "    - Amazon SageMaker algorithms return JSON in several layouts.\n",
    "- Common Request Formats for All Algorithms \n",
    "    - JSON\n",
    "    - JSONLINES\n",
    "    - CSV\n",
    "    - RECORDIO\n",
    "- Using Batch Transform with Build-in Algorithms \n",
    "    - JSONLINES\n",
    "\n",
    "### 6.1.3 Suggested Instance Types \n",
    "For training and hosting Amazon SageMaker algorithms, we recommend using the following EC2 instance types:\n",
    "- ml.m4.xlarge, ml.m4.4xlarge, and ml.m4.10xlarge\n",
    "- ml.c4.xlarge, ml.c4.2xlarge, and ml.c4.8xlarge\n",
    "- ml.p2.xlarge, ml.p2.8xlarge, and ml.p2.16xlarge\n",
    "\n",
    "### 6.1.4 Logs \n",
    "**Note**\n",
    "If a job fails and logs do not appear in CloudWatch, it's likely that an error occurred before the start of training. Reasons include specifying the wrong training image or S3 location.\n",
    "\n",
    "The contents of logs vary by algorithms. However, you can typically find the following information:\n",
    "- Confirmation of arguments provided at the beginning of the log\n",
    "- Errors that occurred during training\n",
    "- Measurement of an algorithms accuracy or numerical performance\n",
    "- Timings for the algorithm, and any major stages within the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 BlazingText\n",
    "[link](https://dl.acm.org/citation.cfm?doid=3146347.3146354)\n",
    "- A highly optimized implementations of the **Word2vec** (sentiment analysis, named entity recognition, machine translation) and **text classification** (web search, information retrieval, ranking and document classification) algorithms.\n",
    "- Similar to Word2vec, it provides the Skip-gram and continuous bag-of-words (CBOW) training architectures.\n",
    "\n",
    "**BlazingText provides the following features**:\n",
    "- Accelerated training of fastText text classifier on multi-core CPUs or a GPU and Word2Vec on GPUs using highly optimized CUDA kernels.\n",
    "- Enriched Word Vectors with Subword Information by learning vector representations for character n-grams.\n",
    "- A batch_skipgram mode for the Word2Vec algorithm that allows faster training and distributed computation across multiple CPU nodes. \n",
    "\n",
    "### 6.2.1 Input/Output Interface\n",
    "### 6.2.2 Training and Validation Data Format\n",
    "- Word2Vec algorithm: a training sentence per line\n",
    "- Text Classification algorithm: a training sentence per line along with the labels\n",
    "### 6.2.3 Model artifacts and Inference \n",
    "- Word2Vec algorithm: *vectors.txt* which contains words to vectors mapping (compatible with other tools like Gensim and Spacy) and *vectors.bin*.\n",
    "- Text Classification algorithm: model.bin \n",
    "### 6.2.4 EC2 Instance [Recommendation]()\n",
    "### 6.2.5 BlazingText Sample [Notebooks](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_text_classification_dbpedia/blazingtext_text_classification_dbpedia.ipynb)\n",
    "\n",
    "### 6.2.6 [Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html)\n",
    "#### Word2Vec Hyperparameters\n",
    "- mode, batch_size, buckets, epochs, evaluation, learning_rate\n",
    "- min_char, min_count, max_char, negative_samples\n",
    "- sampling_threshold, subwords, vector_dim, window_size\n",
    "#### Text Classification Hyperparameters\n",
    "- mode, buckets, early_stopping, epochs, learning_rate\n",
    "- min_count, min_epochs, patience, vector_dim, word_ngrams\n",
    "\n",
    "### 6.2.7 [Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html) a BlazingText Model\n",
    "#### Metrics Computed by the BlazingText Algorithm \n",
    " <div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b7b7\">\n",
    "          <tr>\n",
    "             <th>Metric Name</th>\n",
    "             <th>Description</th>\n",
    "             <th>Optimization Direction</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">train:mean_rho</code></td>\n",
    "             <td>\n",
    "                <p>Mean rho (Spearman's rank correlation coefficient) on <a href=\"https://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\" target=\"_blank\">WS-353 word similarity datasets</a>.\n",
    "                </p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>Maximize</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">validation:accuracy</code></td>\n",
    "             <td>\n",
    "                <p>Classification accuracy on user specified validation dataset.\n",
    "                </p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>Maximize</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    " </div>\n",
    "\n",
    "#### Tunable Hyperparameters for Word2Vec\n",
    "<div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b9b3b5\">\n",
    "          <tr>\n",
    "             <th>Parameter Name</th>\n",
    "             <th>Parameter Type</th>\n",
    "             <th>Recommended Ranges</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">batch_size</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[8-32]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">epochs</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-15]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">learning_rate</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>MinValue: 0.005, MaxValue: 0.01</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">min_count</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[0-100]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">mode</code></td>\n",
    "             <td>\n",
    "                <p>CategoricalParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>['batch_skipgram', 'skipgram', 'cbow']</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">negative_samples</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-25]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">sampling_threshold</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "               <p>MinValue: 0.0001, MaxValue: 0.001</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">vector_dim</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[32-300]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">window_size</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1-10]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    " </div>\n",
    " \n",
    "#### Tunable Hyperparameters for Text Classification\n",
    " <div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b9b5b5\">\n",
    "          <tr>\n",
    "             <th>Parameter Name</th>\n",
    "             <th>Parameter Type</th>\n",
    "             <th>Recommended Ranges</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">buckets</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1000000-10000000]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">epochs</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-15]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">learning_rate</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>MinValue: 0.005, MaxValue: 0.01</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">min_count</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[0-100]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">mode</code></td>\n",
    "             <td>\n",
    "                <p>CategoricalParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>['supervised']</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">vector_dim</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[32-300]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">word_ngrams</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1-3]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 DeepAR Forecasting\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)\n",
    "\n",
    "Amazon SageMaker DeepAR is a supervised learning algorithm for forecasting scalar (that is, one-dimensional) **time series** using recurrent neural networks (**RNN**) - **ARIMA, ETS**\n",
    "\n",
    "Examples of such time series groupings are **demand** for different **products**, **server loads**, and **requests** for web pages. In this case, it can be beneficial to train a single model jointly over all of these time series. \n",
    "\n",
    "**Topics**:\n",
    "### 6.3.1    Input/Output Interface\n",
    "- DeepAR supports two data channels **train** and **test** with *JSONLINES* file format (either ***.json*** or ***.json.gz*** or ***.parquet***)\n",
    "- Use RMSE or weighted quantile loss for evaluation\n",
    "     \n",
    "     $$\\text{RMSE} = \\sqrt{\\frac{1}{nT}\\sum_{i,t}{\\left(\\hat{y}_{i,t}-y_{i.t}\\right)^2}}$$\n",
    "     $$\\text{wQuantileLoss}[\\tau] = 2\\frac{\\sum_{i,t}{Q_{i,t}^{(\\tau)}}}{\\sum_{i,t}{|y_{i,t}|}}\\quad\\text{with}\\quad Q_{i,t}^{(\\tau)}=\\begin{cases}(1-\\tau)|q_{i,t}^{(\\tau)}-y_{i,t}| & \\text{ if } q_{i,t}^{(\\tau)}>y_{i,t}\\\\ \\tau|q_{i,t}^{(\\tau)}-y_{i,t}| & \\text{ otherwise } \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2    Recommended Best Practices\n",
    "- always provide entire time series for training, testing, and when calling the model for prediction\n",
    "- dataset can be split into training and test datasets for tuning a DeepAR Model at different end points\n",
    "- do not use very large values (>400) for *prediction length*\n",
    "- use the same values for *prediction* and *context* lengths.\n",
    "- train DeepAR model on as **many time series** as available\n",
    "### 6.3.3 EC2 Instance Recommendations\n",
    "- can use GPU and CPU\n",
    "- use large machine for large model size\n",
    "    \n",
    "### 6.3.4 DeepAR Sample Notebooks: \n",
    "[Time series forecasting with DeepAR - Synthetic data](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb)\n",
    "\n",
    "### 6.3.5 How DeepAR Works\n",
    "- Under the Hood:<br>\n",
    "    DeepAR automatically creates feature time series<br>\n",
    "    DeepAR model is trained by randomly sampling several training examples from each of the time series in the training dataset.<br>\n",
    "    To capture seasonality patterns, DeepAR also automatically feeds lagged values from the target time series. <br>\n",
    "    For inference, the trained model takes as input target time series, which might or might not have been used during training, and forecasts a probability distribution for the next prediction_length values.\n",
    "    \n",
    "### 6.3.6 [DeepAR Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)\n",
    "- context_length, prediction_length\n",
    "- epochs (+ early_stopping_patience)\n",
    "- time_freq (every minutes, hourly, daily, weekly, monthly)\n",
    "- cardinality (for categorical)\n",
    "- dropout_rate, embedding_dimension, learning_rate\n",
    "- likelihood (gaussian, beta, negative-binomial, student-T, deterministic-L1)\n",
    "- mini_batch_size, num_cells, num_dynamic_feat, num_eval_samples, num_layers, test_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.7 Tuning a DeepAR Model\n",
    "- **Metrics Computed by the DeepAR Algorithm**:\n",
    "<table id=\"w1649aac23c28c21b7b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:RMSE</code></td>\n",
    "     <td>\n",
    "        <p>Root mean square error between forecast and actual target computed on the test set.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:mean_wQuantileLoss</code></td>\n",
    "     <td>\n",
    "        <p>Average overall quantile losses computed on the test set. Setting the <code class=\"code\">test_quantiles</code> hyperparameter controls which quantiles are used. \n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">train:final_loss</code></td>\n",
    "     <td>\n",
    "        <p>Training negative log-likelihood loss averaged over the last training epoch for the model.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "</table>    \n",
    "- **Tunable Hyperparameters**:\n",
    "<table id=\"w1649aac23c28c21b9b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 32, MaxValue: 1028</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">epochs</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 1000</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">context_length</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 200</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_cells</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 30, MaxValue: 200</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_layers</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 8</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">dropout_rate</code></td>\n",
    "     <td>\n",
    "        <p>ContinuousParameterRange</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 0.00, MaxValue: 0.2</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">embedding_dimension</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 50</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">learning_rate</code></td>\n",
    "     <td>\n",
    "        <p>ContinuousParameterRange</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1e-5, MaxValue: 1e-1</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.8 DeepAR Inference Formats\n",
    "[**JSON**](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Factorization Machines\n",
    "- A factorization machine is a **general-purpose supervised learning** algorithm that can be used for both **classification** and **regression** tasks. \n",
    "- It is an **extension of a linear model** that is designed to capture interactions between features within high dimensional sparse datasets economically.\n",
    "\n",
    "### 6.4.1 Input/Output Interface\n",
    "- RMSE, Log Loss, Accuracy, F1-score\n",
    "- application/json, x-recordio-protobuf\n",
    "\n",
    "### 6.4.2 EC2 Instance Recommendation\n",
    "- CPUs instances\n",
    "\n",
    "### 6.4.3 Factorization Machines Sample Notebooks\n",
    "- [ An Introduction to Factorization Machines with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/factorization_machines_mnist/factorization_machines_mnist.ipynb)\n",
    "\n",
    "### 6.4.4 How Factorization Machines Work\n",
    "- for prediction task, [FM](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) estimates a function $\\hat{y}$ from a feature set $x_i$ to a target domain:\n",
    "$$\\hat{y}=w_0+\\sum_i w_ix_i +\\sum_i\\sum_{j>i}<v_i,v_j>x_ix_j$$\n",
    "- for regression task, FM minimizes:\n",
    "$$L=\\frac{1}{N}\\sum_n(y_n-\\hat{y}_n)^2$$\n",
    "- for classification task, FM minimizes:\n",
    "$$L=\\frac{1}{N}\\sum_n\\left[y_n\\log\\hat{p}_n+(1-y_n)\\log(1-\\hat{p}_n)\\right]\\text{ where } \\hat{p}_n=\\frac{1}{1+e^{-\\hat{y}_n}}$$\n",
    "\n",
    "### 6.4.5 Factorization Machines Hyperparameters\n",
    "- feature_dim, num_factors,\n",
    "- predictor_type (binary_classifier, regressor)\n",
    "- bias_init_method, bias_init_scale, bias_init_sigma\n",
    "- bias_init_value, bias_lr, bias_wd, \n",
    "- clip_gradient, epochs, eps\n",
    "- factors_init_method, factors_init_scale, factors_init_sigma, factors_init_value, factors_lr, factors_wd, \n",
    "- linear_lr, linear_init_method, linear_init_scale, linear_init_sigma, linear_init_value, linear_wd, \n",
    "- mini_batch_size, rescale_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.6 Tuning a Factorization Machines Model\n",
    "- **Metrics Computed by the Factorization Machines Algorithm**\n",
    "\n",
    "**Regression**\n",
    "<table id=\"w1649aac23c31c21b7b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:rmse</code></td>\n",
    "     <td><p>Root Mean Square Error</p></td>\n",
    "     <td><p>Minimize</p></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "**Classification**\n",
    "<table id=\"w1649aac23c31c21b7b9\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:binary_classification_accuracy</code></td>\n",
    "     <td><p>Accuracy</p></td>\n",
    "     <td><p>Maximize</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:binary_classification_cross_entropy</code></td>\n",
    "     <td><p>Cross Entropy</p></td>\n",
    "     <td><p>Minimize</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:binary_f_beta</code></td>\n",
    "     <td><p>Beta</p></td>\n",
    "     <td><p>Maximize</p></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "- **Tunable Hyperparameters**\n",
    "<table id=\"w1649aac23c31c21b9b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "     <th>Dependency</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_init_scale</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==uniform</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_init_sigma</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==normal</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_init_value</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==constant</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_lr</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_wd</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">epoch</code></td>\n",
    "     <td><p>IntegerParameterRange</p></td>\n",
    "     <td><p>MinValue: 1, MaxValue: 1000</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_init_scale</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==uniform</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_init_sigma</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==normal</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_init_value</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==constant</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_lr</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_wd</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512]</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_init_scale</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==uniform</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_init_sigma</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==normal</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_init_value</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==constant</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_lr</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_wd</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td><p>IntegerParameterRange</p></td>\n",
    "     <td><p>MinValue: 100, MaxValue: 10000</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.7 Factorization Machine Response Formats\n",
    "[JSON](https://docs.aws.amazon.com/sagemaker/latest/dg/fm-in-formats.html#fm-json), [JSONLINES](https://docs.aws.amazon.com/sagemaker/latest/dg/fm-in-formats.html#fm-jsonlines), [RECORDIO](https://docs.aws.amazon.com/sagemaker/latest/dg/fm-in-formats.html#fm-recordio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Image Classification Algorithm\n",
    "- A **supervised learning** algorithm that **takes an image as input** and **classifies** it into one of **multiple** output **categories**. \n",
    "- Uses a convolutional neural network (**ResNet**) that can be **trained from scratch**, or trained using **transfer learning** when a large number of training images are not available.\n",
    "- input format: **MXNet [RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html)** (differs from the *protobuf* data), .jpg, .png\n",
    "- References:\n",
    "    - [Deep residual learning for image recognition ](https://arxiv.org/abs/1512.03385)\n",
    "    - [ImageNet image database](http://www.image-net.org/)\n",
    "    - [Image classification in MXNet](https://github.com/apache/incubator-mxnet/tree/master/example/image-classification)\n",
    "    \n",
    "### 6.5.1 Input/Output Interface\n",
    "- application/x-recordio\n",
    "- application/x-image\n",
    "- training with both, inference only with image format\n",
    "\n",
    "### 6.5.2 EC2 Instance Recommendation\n",
    "- prefer GPU\n",
    "- can train with CPU\n",
    "\n",
    "### 6.5.3 Image Classification Sample Notebooks\n",
    "- [ End-to-End Multiclass Image Classification Example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining.ipynb)\n",
    "\n",
    "### 6.5.4 How Image Classification Works\n",
    "- takes an image as input and classifies it into one of the output categories\n",
    "- full and transfer learnings\n",
    "\n",
    "### 6.5.5 Hyperparameters\n",
    "- [link](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)\n",
    "- num_classes, num_training_samples, augmentation_type (crop, crop_color, crop_color_transform)\n",
    "- beta_1, beta_2, checkpoint_frequency, epochs, eps, gamma\n",
    "- image_shape, kv_store, \n",
    "- learning_rate, lr_scheduler_factor, mlr_scheduler_step, mini_batch_size\n",
    "- momentum, multi_label, num_layers, optimizer\n",
    "- precision_dtype, resize, top_k\n",
    "- use_pretrained_model, use_weighted_loss, weight_decay\n",
    "\n",
    "### 6.5.6 Tuning an Image Classification Model\n",
    "- **Metrics Computed by the Image Classification Algorithm**\n",
    "<table id=\"w1649aac23c34c25b7b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "      <th>Description</th>\n",
    "      <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:accuracy</code></td>\n",
    "     <td>        <p>The ratio of the number of correct predictions to the total           number of predictions made.        </p>     </td>\n",
    "     <td>        <p>Maximize</p>     </td>\n",
    "  </tr>\n",
    "</table>\n",
    "- **Tunable Hyperparameters**\n",
    "<table id=\"w1649aac23c34c25b9b7\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">beta_1</code></td>\n",
    "     <td>       <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-6, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">beta_2</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-6, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">eps</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-8, MaxValue: 1.0</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code class=\"code\">gamma</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "      <td>        <p>MinValue: 1e-8, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code class=\"code\">learning_rate</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-6, MaxValue: 0.5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 8, MaxValue: 512</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">momentum</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">optimizer</code></td>\n",
    "     <td>        <p>CategoricalParameterRanges</p>     </td>\n",
    "     <td>        <p>['sgd', ‘adam’, ‘rmsprop’, 'nag']</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">weight_decay</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "    \n",
    "</table>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 K-Means Algorithm\n",
    "- **find discrete groupings within data**, where members of a group are **as similar as possible** to one another and as **different** as possible from members of other groups.\n",
    "- the version used by Amazon SageMaker is more accurate than [web-scale](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)\n",
    "- [k-means algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/algo-kmeans-tech-notes.html) expects tabular data, \n",
    "- Euclidean distance represents the similarity\n",
    "\n",
    "### 6.6.1 Input/Output Interface\n",
    "[k-means Response Formats](https://docs.aws.amazon.com/sagemaker/latest/dg/km-in-formats.html)\n",
    "\n",
    "### 6.6.2 EC2 Instance Recommendation\n",
    "- recommeded CPU instances\n",
    "- can train with GPUs\n",
    "\n",
    "### 6.6.3 K-Means Sample Notebooks\n",
    "- [Analyze US census data for population segmentation using Amazon SageMaker](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans/sagemaker-countycensusclustering.ipynb)\n",
    "\n",
    "\n",
    "### 6.6.4 How K-Means Clustering Works\n",
    "- Step 1: Determine the Initial Cluster Centers\n",
    "- Step 2: Iterate over the Training Dataset and Calculate Cluster Centers \n",
    "- Step 3: Reduce the Clusters from K to k\n",
    "\n",
    "### 6.6.5 K-Means Hyperparameters\n",
    "- feature_dim, mini_batch_size\n",
    "- k, init_method, \n",
    "- epochs, eval_metrics, extra_center_factor\n",
    "- half_life_time_size\n",
    "- local_lloyd_max_iter, local_lloyd_init_method, local_lloyd_num_trials, local_lloyd_tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.6 Tuning a K-Means Model\n",
    "- **Metrics Computed by the K-Means Algorithm**\n",
    "<table id=\"w1649aac23c37c21b9b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:msd</code></td>\n",
    "     <td>\n",
    "        <p>Mean squared distances between each record in the test set and the closest center of the model.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>        <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:ssd</code></td>\n",
    "     <td>\n",
    "        <p>Sum of the squared distances between each record in the\n",
    "           test set and the closest center of the model.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>        <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "- **Tunable Hyperparameters**:\n",
    "<table id=\"w1649aac23c37c21c11b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">epochs</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1, MaxValue:10</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">extra_center_factor</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 4, MaxValue:10</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">init_method</code></td>\n",
    "     <td>        <p>CategoricalParameterRanges</p>     </td>\n",
    "     <td>        <p>['kmeans++', 'random']</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 3000, MaxValue:15000</p>     </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.7 k-means Response Formats\n",
    "- JSON\n",
    "- JSONLINES\n",
    "- RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 K-Nearest Neighbors\n",
    "- an index-based algorithm\n",
    "- uses a non-parametric method for **classification** or **regression**\n",
    "- For **classification** problems, the algorithm queries the **k points** that are **closest** to the sample point and **returns the most frequently used label** of their class **as the predicted** label. \n",
    "- For **regression problems**, the algorithm queries the **k closest points** to the sample point and **returns the average** of their feature values **as the predicted** value.\n",
    "- **three steps**: sampling, dimension reduction, and index building\n",
    "\n",
    "### 6.7.1 Input/Output Interface\n",
    "- train in: text/csv, application/x-recordio-protobuf; \n",
    "- train out: text/csv\n",
    "- inference in: application/json, application/x-recordio-protobuf, text/csv, \n",
    "- inference out: application/json, application/x-recordio-protobuf\n",
    "- batch transform: application/jsonlines\n",
    "\n",
    "### 6.7.2 kNN Sample Notebooks\n",
    "[K-Nearest Neighbor Covertype](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/k_nearest_neighbors_covtype/k_nearest_neighbors_covtype.ipynb)\n",
    "\n",
    "### 6.7.4 EC2 Instance Recommendation\n",
    "- Training: CPU\n",
    "- Inference: CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.3 How It Works\n",
    "- Step 1: Sampling\n",
    "- Step 2: Dimension Reduction: ***sign*** for specifies a random projection and ***fjlt*** for fast Johnson-Lindenstrauss transform\n",
    "- Step 3: Building an Index\n",
    "- Model Serialization: preparation for inference\n",
    "\n",
    "### 6.7.5 K-Nearest Neighbors Hyperparameters\n",
    "- [feature_dim](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN_hyperparameters.html), k, predictor_type (classifier, regressor)\n",
    "- sample_size, dimension_reduction_target, dimension_reduction_type (sign, fjlt) \n",
    "- faiss_index_ivf_nlists, faiss_index_pq_m, \n",
    "- index_metric, index_type, mini_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.6 Tuning a K-Nearest Neighbors Model\n",
    "- **Metrics Computed by the K-Nearest Neighbors Algorithm**\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Metric Name</th>\n",
    "        <th>Optimization Direction</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>test:accuracy</td>\n",
    "        <td>Maximize</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>test:mse</td>\n",
    "        <td>Minimize</td>\n",
    "    </tr>\n",
    "</table>\n",
    "- **Tunable Hyperparameters**\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Parameter Name</th>\n",
    "        <th>Parameter Type</th>\n",
    "        <th>Recommended Ranges</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>k</td>\n",
    "        <td>IntegerParameterRanges</td>\n",
    "        <td>MinValue: 1, MaxValue: 1024</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>sample_size</td>\n",
    "        <td>IntegerParameterRanges</td>\n",
    "        <td>MinValue: 256, MaxValue: 20000000</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### 6.7.7 Data Formats for K-Nearest Neighbors Training Input\n",
    "- [CSV] and [RECORDIO](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-in-formats.html)\n",
    "\n",
    "### 6.7.8 K-NN Request and Response Formats\n",
    "- [INPUT](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-inference-formats.html): CSV, JSON, JSONLINES, RECORDIO\n",
    "- [OUPUT](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-inference-formats.html#kNN-output-json): JSON, JSONLINES, VERBOSE JSON, RECORDIO-PROTOBUF, VERBOSE RECORDIO-PROTOBUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Latent Dirichlet Allocation (LDA)\n",
    "- attempts to describe a set of observations as a mixture of distinct categories\n",
    "- used to discover a user-specified number of topics shared by documents within a text corpus\n",
    "\n",
    "### 6.8.1 Input/Output Interface\n",
    "Supports recordIO-wrapped-protobuf (dense and sparse) and CSV file formats\n",
    "For inference, text/csv, application/json, and application/x-recordio-protobuf content types are supported.\n",
    "\n",
    "### 6.8.2 EC2 Instance Recommendation\n",
    "- currently only supports single-instance CPU training\n",
    "\n",
    "### 6.8.3 LDA Sample Notebooks\n",
    "[An Introduction to SageMaker LDA](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/lda_topic_modeling/LDA-Introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8.4 How LDA Works\n",
    "- an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of different categories\n",
    "- LDA is a generative probability model\n",
    "- LDA can be used for a variety of tasks, from **clustering customers** based on product purchases to **automatic harmonic** analysis in music. \n",
    "- is most commonly associated with **topic modeling** in text corpuses.\n",
    "- LDA is a \"bag-of-words\" model\n",
    "- For each word: \n",
    "    - Choose a **topic** z ∼ Multinomial(θ) \n",
    "    - Choose the corresponding **topic-word distribution** β_z\n",
    "    - Draw a **word** w ∼ Multinomial(β_z)\n",
    "- The goal is to find parameters α and β:\n",
    "    - α — A prior estimate on topic probability\n",
    "    - β — \"topic-word distribution.\"\n",
    "- use Gibbs sampling or Expectation Maximization (EM) techniques to estimate\n",
    "- **Tensor decomposition algorithm**:\n",
    "    - The goal is to calculate the spectral decomposition of a V x V x V tensor\n",
    "    - uses a V x V moment matrix to find a whitening matrix of dimension V x k\n",
    "    - This same whitening matrix can then be used to find a smaller k x k x k tensor\n",
    "    - Alternating Least Squares is used to decompose the smaller k x k x k tensor. \n",
    "- [More info](https://docs.aws.amazon.com/sagemaker/latest/dg/lda-how-it-works.html)\n",
    "        \n",
    "### 6.8.5 LDA Hyperparameters\n",
    "- num_topics, feature_dim, mini_batch_size\n",
    "- alpha0, max_restarts, max_iterations, tol\n",
    "\n",
    "### 6.8.6 Tuning an LDA Model\n",
    "- **Metrics Computed by the LDA Algorithm**\n",
    "    - Maximize per-word log-likelihood\n",
    "- **Tunable Hyperparameters**\n",
    "    - alpha0: 0.1-10\n",
    "    - number of topic: 1-150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 Linear Learner\n",
    "- algorithms used for solving either **classification** or **regression** problems\n",
    "- learns a linear function (**Linear regression**), or linear threshold function (**Logistic regression**) for classification, mapping a vector x to an approximation of the label y\n",
    "\n",
    "### 6.9.1 Input/Output Interface\n",
    "- recordIO wrapped protobuf and CSV\n",
    "- application/x-recordio-protobuf, text/csv,\n",
    "- For inference: application/json, application/x-recordio-protobuf, and text/csv\n",
    "\n",
    "### 6.9.2 EC2 Instance Recommendation\n",
    "- single- or multi-machine CPU and GPU instances\n",
    "\n",
    "### 6.9.3 Linear Learner Sample Notebooks\n",
    "- [ An Introduction to Linear Learner with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/linear_learner_mnist/linear_learner_mnist.ipynb)\n",
    "\n",
    "### 6.9.4 How It Works\n",
    "- Step 1: Preprocessing: normalization and standalization\n",
    "- Step 2: Training \n",
    "- Step 3: Validation and Setting the Threshold \n",
    "\n",
    "### 6.9.5 Linear Learner Hyperparameters\n",
    "- feature_dim, num_classes, predictor_type, \n",
    "- accuracy_top_k, balance_multiclass_weights\n",
    "- beta_1, beta_2, bias_lr_mult, bias_wd_mult\n",
    "- binary_classifier_model_selection_criteria\n",
    "- early_stopping_tolerance, early_stopping_patience, \n",
    "- epochs [...](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html)\n",
    "\n",
    "\n",
    "### 6.9.6 Tuning a Linear Learner Model\n",
    "- **Metrics Computed by the Linear Learner Algorithm**, [source](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html#linear-learner-metrics)\n",
    "- **Tuning Hyperparameters** [source](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html#linear-learner-tunable-hyperparameters)\n",
    "\n",
    "### 6.9.7 Linear Learner Response Formats\n",
    "- JSON, JSONLINES, RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10 Neural Topic Model (NTM)\n",
    "- an unsupervised learning algorithm that is used to **organize a corpus of documents into topics** that contain word groupings based on their statistical distribution\n",
    "- [NTM](https://arxiv.org/pdf/1511.06038.pdf) and LDA are distinct algorithms for topic modeling\n",
    "\n",
    "### 6.10.1 Input/Output Interface\n",
    "- for train, validation, test, and [auxiliary](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb): recordIO-wrapped-protobuf, csv\n",
    "- for inference: text/csv, application/json, application/jsonlines, application/x-recordio-protobuf\n",
    "- [WETC score](https://arxiv.org/pdf/1809.02687.pdf)\n",
    "\n",
    "\n",
    "### 6.10.2 EC2 Instance Recommendation\n",
    "- GPU and CPU instance types\n",
    "\n",
    "### 6.10.3 NTM Sample Notebooks\n",
    "- [Introduction to Basic Functionality of NTM](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/ntm_synthetic/ntm_synthetic.ipynb)\n",
    "\n",
    "### 6.10.4 NTM Hyperparameters\n",
    "[link](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm_hyperparameters.html)\n",
    "\n",
    "### 6.10.5 Tuning an NTM Model\n",
    "- Minimize ***total_loss***\n",
    "- Tunable Hyperparameters: encoder_layers_activation, learning_rate, mini_batch_size, optimizer, rescale_gradient, weight_decay\n",
    "\n",
    "### 6.10.6 NTM Response Formats\n",
    "- JSON, JSONLINES, RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11 Object Detection Algorithm\n",
    "- **detects and classifies** objects in images using a single deep neural network\n",
    "- It uses the **Single Shot multibox Detector** (SSD) framework and supports two base networks: **VGG** and **ResNet**. The network can be **trained from scratch**, or trained with models that have been **pre-trained** on the **ImageNet** dataset.\n",
    "\n",
    "### 6.11.1 Input/Output Interface\n",
    "- Training with RecordIO Format \n",
    "- Training with Image Format\n",
    "\n",
    "### 6.11.2 EC2 Instance Recommendation\n",
    "- GPU instances \n",
    "\n",
    "### 6.11.3 Object Detection Sample Notebooks\n",
    "- [Object Detection using the Image and JSON format](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_image_json_format.ipynb)\n",
    "\n",
    "### 6.11.4 How Object Detection Works\n",
    "- takes an image as input and outputs the category that the object belongs to, along with a confidence score\n",
    "- Object detection in Amazon SageMaker supports both **VGG-16** and **ResNet-50** as a base network for SSD.\n",
    "- full/transfer learning\n",
    "\n",
    "### 6.11.5 Object Detection Hyperparameters\n",
    "- num_classes, num_training_samples, base_network\n",
    "- image_shape, epochs, freeze_layer_pattern\n",
    "- kv_store, label_width\n",
    "- learning_rate, lr_scheduler_factor, lr_scheduler_step, mini_batch_size, momentum, nms_threshold, \n",
    "- optimizer, overlap_threshold, \n",
    "- use_pretrained_model, weight_decay\n",
    "\n",
    "### 6.11.6 Tuning an Object Detection Model\n",
    "- Maximize **mAP**\n",
    "- Tunable Hyperparameters: learning_rate, mini_batch_size, momentum, optimizer, weight_decay\n",
    "\n",
    "### 6.11.7 Object Detection Request and Response Formats\n",
    "- in: image/jpeg and image/png\n",
    "- out: JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.12 Principal Component Analysis (PCA)\n",
    "-  attempts to **reduce the dimensionality**, retaining as **much infor**mation as possible\n",
    "- uses tabular data\n",
    "\n",
    "### 6.12.1 Input/Output Interface\n",
    "- training:  recordIO-wrapped-protobuf, csv\n",
    "- inference: text/csv, application/json, application/x-recordio-protobuf\n",
    "\n",
    "### 6.12.2 EC2 Instance Recommendation\n",
    "- GPU and CPU computation\n",
    "\n",
    "### 6.12.3 Principal Component Analysis Sample Notebooks\n",
    "- [Principal Component Analysis Sample Notebooks](https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html#PCA-sample-notebooks)\n",
    "- [An Introduction to PCA with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/pca_mnist/pca_mnist.ipynb)\n",
    "\n",
    "### 6.12.4 How PCA [Works](https://docs.aws.amazon.com/sagemaker/latest/dg/how-pca-works.html)\n",
    "- finding a new set of features called **components**, which are composites of the original features, but are **uncorrelated with one another**. \n",
    "- The **first** component accounts for the **largest possible variability** in the data, the **second** component the **second most variability**, and so on.\n",
    "- Mode 1: **Regular**: for datasets with sparse data and a moderate number of observations and features\n",
    "- Mode 2: **[Randomized](https://docs.aws.amazon.com/sagemaker/latest/dg/how-pca-works.html#mode-2)**: for datasets with both a large number of observations and features, [FJLT transform](https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf)\n",
    "\n",
    "### 6.12.5 PCA Hyperparameters\n",
    "- feature_dim, mini_batch_size, num_components, algorithm_mode\n",
    "- extra_components, subtract_mean\n",
    "\n",
    "### 6.12.6 PCA Response Formats\n",
    "- JSON, JSONLINES, RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.13 Random Cut Forest\n",
    "- algorithm for **detecting anomalous** data points within a data set\n",
    "\n",
    "### 6.13. Input/Output Interface\n",
    "Instance Recommendations\n",
    "Randon Cut Forest Sample Notebooks\n",
    "How RCF Works\n",
    "RCF Hyperparameters\n",
    "Tuning a RCF Model\n",
    "RCF Response Formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## 6.14 Sequence to Sequence (seq2seq)\n",
    "## 6.15 XGBoost Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
