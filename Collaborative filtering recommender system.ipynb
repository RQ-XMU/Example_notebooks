{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will use Apache Spark to build simple movie recommendation system on big MovieLens 20M dataset (https://grouplens.org/datasets/movielens/). The dataset contains 20 million ratings for 27000 movies given by 138000 users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset format: comma-separated values (https://en.wikipedia.org/wiki/CSV)\n",
    "\n",
    "The first line is a header:\n",
    "userId,movieId,rating,timestamp\n",
    "\n",
    "_userId_, _movieId_ are integers representing user and movies identifiers correspondingly\n",
    "\n",
    "_rating_ is a floating point number from 1.0 to 5.0\n",
    "\n",
    "_timestamp_ is an integer but it wonâ€™t be used in the assignment :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .enableHiveSupport() \\\n",
    "    .appName(\"recSys\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now use spark session or spark context to read csv.\n",
    "raw_data = sc.textFile('ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header userId,movieId,rating,timestamp\n",
      "[(1, 2, 3.5), (1, 29, 3.5), (1, 32, 3.5)]\n"
     ]
    }
   ],
   "source": [
    "# remove header & timeID column\n",
    "raw_data_header = raw_data.take(1)[0]\n",
    "print(\"header\", raw_data_header)\n",
    "movies_data = raw_data.filter(lambda line: line!=raw_data_header)\\\n",
    "                      .map(lambda line: line.split(\",\"))\\\n",
    "                      .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))).cache()\n",
    "print(movies_data.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will use collaborative filtering approach for making predictions. To find latent vector representation for users and items we suggest you to use explicit ALS method. Refer to the Spark MLLib documentation for Python API details: https://spark.apache.org/docs/2.1.0/api/python/pyspark.mllib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into three folds: on each iteration one fold will be used for testing and the other two folds will be used for training. Wrap necessary values with _Rating_ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating\n",
    "import math\n",
    "fold_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = movies_data.randomSplit([1.0/fold_count for i in range(fold_count)], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations: to make your solution more efficient prepare RDDs for all the folds beforehand running the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ALS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to choose training parameters. It is recommended to set the rank equal to 20,\n",
    "regularization term lambda equal to 0.01 and the number of iterations equal to 5.\n",
    "Feel free to experiment and choose your own parameters to see how they influence the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 20\n",
    "iterations = 10\n",
    "lambda_ = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the explicit ALS model on two of three folds and evaluate its performance on remaining test fold. Performance evaluation should be done using the classic RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 5, iterations: 5, lambda_: 0.010000, RMSE: 0.825655\n",
      "rank: 5, iterations: 5, lambda_: 0.030000, RMSE: 0.824144\n",
      "rank: 5, iterations: 5, lambda_: 0.100000, RMSE: 0.819327\n",
      "rank: 5, iterations: 5, lambda_: 0.300000, RMSE: 0.887157\n",
      "rank: 5, iterations: 10, lambda_: 0.010000, RMSE: 0.818208\n",
      "rank: 5, iterations: 10, lambda_: 0.030000, RMSE: 0.816474\n",
      "rank: 5, iterations: 10, lambda_: 0.100000, RMSE: 0.816076\n",
      "rank: 5, iterations: 10, lambda_: 0.300000, RMSE: 0.906710\n",
      "rank: 5, iterations: 15, lambda_: 0.010000, RMSE: 0.814075\n",
      "rank: 5, iterations: 15, lambda_: 0.030000, RMSE: 0.811254\n",
      "rank: 5, iterations: 15, lambda_: 0.100000, RMSE: 0.815826\n",
      "rank: 5, iterations: 15, lambda_: 0.300000, RMSE: 0.907917\n",
      "rank: 10, iterations: 5, lambda_: 0.010000, RMSE: 0.825239\n",
      "rank: 10, iterations: 5, lambda_: 0.030000, RMSE: 0.817640\n",
      "rank: 10, iterations: 5, lambda_: 0.100000, RMSE: 0.817744\n",
      "rank: 10, iterations: 5, lambda_: 0.300000, RMSE: 0.884754\n",
      "rank: 10, iterations: 10, lambda_: 0.010000, RMSE: 0.816148\n",
      "rank: 10, iterations: 10, lambda_: 0.030000, RMSE: 0.804048\n",
      "rank: 10, iterations: 10, lambda_: 0.100000, RMSE: 0.807857\n",
      "rank: 10, iterations: 10, lambda_: 0.300000, RMSE: 0.903750\n",
      "rank: 10, iterations: 15, lambda_: 0.010000, RMSE: 0.815114\n",
      "rank: 10, iterations: 15, lambda_: 0.030000, RMSE: 0.799920\n",
      "rank: 10, iterations: 15, lambda_: 0.100000, RMSE: 0.806933\n",
      "rank: 10, iterations: 15, lambda_: 0.300000, RMSE: 0.907788\n",
      "rank: 15, iterations: 5, lambda_: 0.010000, RMSE: 0.832690\n",
      "rank: 15, iterations: 5, lambda_: 0.030000, RMSE: 0.812600\n",
      "rank: 15, iterations: 5, lambda_: 0.100000, RMSE: 0.819208\n",
      "rank: 15, iterations: 5, lambda_: 0.300000, RMSE: 0.883498\n",
      "rank: 15, iterations: 10, lambda_: 0.010000, RMSE: 0.828237\n",
      "rank: 15, iterations: 10, lambda_: 0.030000, RMSE: 0.802052\n",
      "rank: 15, iterations: 10, lambda_: 0.100000, RMSE: 0.810754\n",
      "rank: 15, iterations: 10, lambda_: 0.300000, RMSE: 0.903418\n",
      "rank: 15, iterations: 15, lambda_: 0.010000, RMSE: 0.827868\n",
      "rank: 15, iterations: 15, lambda_: 0.030000, RMSE: 0.797310\n",
      "rank: 15, iterations: 15, lambda_: 0.100000, RMSE: 0.804969\n",
      "rank: 15, iterations: 15, lambda_: 0.300000, RMSE: 0.907619\n",
      "rank: 20, iterations: 5, lambda_: 0.010000, RMSE: 0.841434\n",
      "rank: 20, iterations: 5, lambda_: 0.030000, RMSE: 0.818243\n",
      "rank: 20, iterations: 5, lambda_: 0.100000, RMSE: 0.823530\n",
      "rank: 20, iterations: 5, lambda_: 0.300000, RMSE: 0.883481\n",
      "rank: 20, iterations: 10, lambda_: 0.010000, RMSE: 0.841402\n",
      "rank: 20, iterations: 10, lambda_: 0.030000, RMSE: 0.803210\n",
      "rank: 20, iterations: 10, lambda_: 0.100000, RMSE: 0.810122\n",
      "rank: 20, iterations: 10, lambda_: 0.300000, RMSE: 0.903465\n",
      "rank: 20, iterations: 15, lambda_: 0.010000, RMSE: 0.840161\n",
      "rank: 20, iterations: 15, lambda_: 0.030000, RMSE: 0.797456\n",
      "rank: 20, iterations: 15, lambda_: 0.100000, RMSE: 0.805429\n",
      "rank: 20, iterations: 15, lambda_: 0.300000, RMSE: 0.907657\n",
      "rank: 25, iterations: 5, lambda_: 0.010000, RMSE: 0.850139\n",
      "rank: 25, iterations: 5, lambda_: 0.030000, RMSE: 0.818619\n",
      "rank: 25, iterations: 5, lambda_: 0.100000, RMSE: 0.824765\n",
      "rank: 25, iterations: 5, lambda_: 0.300000, RMSE: 0.882337\n",
      "rank: 25, iterations: 10, lambda_: 0.010000, RMSE: 0.853794\n",
      "rank: 25, iterations: 10, lambda_: 0.030000, RMSE: 0.804650\n",
      "rank: 25, iterations: 10, lambda_: 0.100000, RMSE: 0.808625\n",
      "rank: 25, iterations: 10, lambda_: 0.300000, RMSE: 0.903344\n",
      "rank: 25, iterations: 15, lambda_: 0.010000, RMSE: 0.850663\n",
      "rank: 25, iterations: 15, lambda_: 0.030000, RMSE: 0.799299\n",
      "rank: 25, iterations: 15, lambda_: 0.100000, RMSE: 0.805063\n",
      "rank: 25, iterations: 15, lambda_: 0.300000, RMSE: 0.907619\n",
      "rank: 30, iterations: 5, lambda_: 0.010000, RMSE: 0.857249\n",
      "rank: 30, iterations: 5, lambda_: 0.030000, RMSE: 0.820223\n",
      "rank: 30, iterations: 5, lambda_: 0.100000, RMSE: 0.823732\n",
      "rank: 30, iterations: 5, lambda_: 0.300000, RMSE: 0.882266\n",
      "rank: 30, iterations: 10, lambda_: 0.010000, RMSE: 0.860737\n",
      "rank: 30, iterations: 10, lambda_: 0.030000, RMSE: 0.806207\n",
      "rank: 30, iterations: 10, lambda_: 0.100000, RMSE: 0.808814\n",
      "rank: 30, iterations: 10, lambda_: 0.300000, RMSE: 0.903472\n",
      "rank: 30, iterations: 15, lambda_: 0.010000, RMSE: 0.859161\n",
      "rank: 30, iterations: 15, lambda_: 0.030000, RMSE: 0.800571\n",
      "rank: 30, iterations: 15, lambda_: 0.100000, RMSE: 0.805370\n",
      "rank: 30, iterations: 15, lambda_: 0.300000, RMSE: 0.907633\n"
     ]
    }
   ],
   "source": [
    "for rank in [5, 10, 15, 20, 25, 30]:\n",
    "    for iterations in [5, 10, 15]:\n",
    "        for lambda_ in [0.01, 0.03, 0.1, 0.3]:\n",
    "            scores = []\n",
    "            \n",
    "            for test_fold_index in range(fold_count):\n",
    "\n",
    "                testRDD = folds[test_fold_index]\n",
    "                indices = list(range(fold_count))\n",
    "                indices.remove(test_fold_index)\n",
    "\n",
    "                # get userID + movieID\n",
    "                testSet = testRDD.map(lambda r: (r[0], r[1]))\n",
    "                trainRDD = sc.union([folds[i] for i in indices])\n",
    "\n",
    "                model = ALS.train(ratings=trainRDD, rank=rank, iterations=iterations, lambda_=lambda_)\n",
    "\n",
    "                # get prediction\n",
    "                predictions = model.predictAll(testSet).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "                # join with testSet\n",
    "                rates_and_preds = testRDD.map(lambda r: ((r[0], r[1]), r[2]))\\\n",
    "                                     .join(predictions)\n",
    "\n",
    "                # calculate RMSE\n",
    "                error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "                scores.append(error)\n",
    "                #     pass # Your code goes here\n",
    "                \n",
    "            print (\"rank: %d, iterations: %d, lambda_: %f, RMSE: %f\" % (rank, iterations, lambda_, sum(scores) / len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average RMSE scores on all the three folds and output the result to stdout.\n",
    "You could refer to publicly available benchmarks (e.g. http://mymedialite.net/examples/datasets.html)\n",
    "to find out what score to expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9409765230362396"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score\n",
    "# 0.9471727528537421\n",
    "# 0.945164679413495\n",
    "# 0.9435918668544166\n",
    "# 0.9427975749365988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
