{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Reinforcement Learning with Tensorflow: Deep Q-Networks andÂ Beyond\n",
    "\n",
    "This iPython notebook implements a Deep Q-Network using both Double DQN and Dueling DQN. The agent learn to solve a navigation task in a basic grid world. To learn more, read here: https://medium.com/p/8438a3e2b8df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the game environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to adjust the size of the gridworld. Making it smaller provides an easier task for our DQN agent, while making the world larger increases the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa4350bef28>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa4352bb4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "env = gameEnv(partial=False,size=5)\n",
    "env.reset()\n",
    "plt.imshow(env.renderEnv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of a starting environment in our simple game. The agent controls the blue square, and can move up, down, left, or right. The goal is to move to the green square (for +1 reward) and avoid the red square (for -1 reward). The position of the three blocks is randomized every episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the network itself\n",
    "\n",
    "Remember that:\n",
    "\n",
    "**Double DQNs**\n",
    "$$\n",
    "\\underbrace{\\mathcal{Q}(s,a)}_{\\text{TD target}}=r(s,a) +\\underbrace{\\gamma \\mathcal{Q}\\left(s',\\underbrace{arg\\max_a\\mathcal{Q}(s',a)}_{\\begin{matrix}\\text{DQN chooses action}\\\\\\text{for the next state}\\end{matrix}}\\right)}_{\\begin{matrix}\\text{target network calculates the Q-}\\\\ \\text{value of taking that action at state }s\\end{matrix}}\\tag{1}\n",
    "$$\n",
    "\n",
    "**Dueling DQNs**\n",
    "$$\n",
    "\\mathcal{Q}(s,a;\\theta,\\alpha,\\beta)=V(s;\\theta,\\beta)+\\left[A(s,a;\\theta,\\alpha)-\\underbrace{\\frac{1}{\\mathcal{A}}\\sum_{a'}A(s,a';\\theta,\\alpha)}_{\\text{average advantage}}\\right]\n",
    "\\tag{2}$$\n",
    "where:\n",
    "- $\\theta$ - common network parameters\n",
    "- $\\alpha$ - advantage stream parameters\n",
    "- $\\beta$ - value stream parameters\n",
    "\n",
    "**DQN architecture:**\n",
    "![](https://cdn-images-1.medium.com/max/873/1*N_t9I7MeejAoWlDuH1i7cw.png)\n",
    "Above: Regular DQN with a single stream for Q-values. Below: Dueling DQN where the value and advantage are calculated separately and then combined only at the final layer into a Q value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self, h_size):\n",
    "        \"\"\"\n",
    "        The network recieves a frame from the game, flattened into an array.\n",
    "        It then resizes it and processes it through four convolutional layers.\n",
    "        \"\"\"\n",
    "        # Scalar input, from images size 84x84x3\n",
    "        self.scalarInput =  tf.placeholder(shape=[None,21168], dtype=tf.float32)  # 21168 = 84x84x3\n",
    "        \n",
    "        # Conver scalar input to images\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,84,84,3])\n",
    "        \n",
    "        # First convNet\n",
    "        # In 84x84x3, out 20x20x32\n",
    "        self.conv1 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.imageIn,\n",
    "                num_outputs=32,\n",
    "                kernel_size=[8,8],\n",
    "                stride=[4,4],\n",
    "                padding='VALID',\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # Second convNet\n",
    "        # In 20x20x32, out 9x9x64\n",
    "        self.conv2 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.conv1,\n",
    "                num_outputs=64,\n",
    "                kernel_size=[4,4],\n",
    "                stride=[2,2],\n",
    "                padding='VALID',\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # Third convNet\n",
    "        # in 9x9x64, out 7x7x64\n",
    "        self.conv3 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.conv2,\n",
    "                num_outputs=64,\n",
    "                kernel_size=[3,3],\n",
    "                stride=[1,1],\n",
    "                padding='VALID', \n",
    "                biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # Fourth convnet\n",
    "        # in 7x7x64, out 1x1xh_size\n",
    "        self.conv4 = \\\n",
    "            slim.conv2d(\n",
    "                inputs=self.conv3,\n",
    "                num_outputs=h_size,\n",
    "                kernel_size=[7,7],\n",
    "                stride=[1,1],\n",
    "                padding='VALID', \n",
    "                biases_initializer=None\n",
    "        )\n",
    "        \n",
    "        # We take the output from the final convolutional layer \n",
    "        # and split it into separate advantage and value streams.\n",
    "        \n",
    "        self.streamAC, self.streamVC = tf.split(\n",
    "            value=self.conv4, \n",
    "            num_or_size_splits=2,\n",
    "            axis=3)\n",
    "        \n",
    "        # conver to flatten\n",
    "        self.streamA = slim.flatten(self.streamAC)\n",
    "        self.streamV = slim.flatten(self.streamVC)\n",
    "        \n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        # Then combine them together to get our final Q-values.\n",
    "        # See the formula (2)\n",
    "        self.Qout = self.Value + \\\n",
    "                    tf.subtract(\n",
    "                        self.Advantage,\n",
    "                        tf.reduce_mean(self.Advantage,axis=1,keep_dims=True)\n",
    "        )\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        # Below we obtain the loss by taking the sum of squares difference \n",
    "        # between the target and prediction Q values.\n",
    "        \n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(indices=self.actions, depth=env.actions, dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        # Set objective function for training\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ArgMax_5:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Qnetwork object at 0x7fa434ff6320>\n"
     ]
    }
   ],
   "source": [
    "network = Qnetwork(16)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class allows us to store experies and sample then randomly to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"\n",
    "    This class allows us to store experiences and sample then randomly to train the network.\n",
    "    experiment = (current state, action, reward, next state, over game status)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\"\n",
    "        Extend buffer by experience, keeping buffer size\n",
    "        \"\"\"\n",
    "        # check to remove fist elements of buffer\n",
    "        # then extend buffer by experience, keep buffer_size = constant \n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        \"\"\"\n",
    "        Randomly take size experiences from buffer\n",
    "        \"\"\"\n",
    "        return np.reshape(\n",
    "                    np.array(random.sample(self.buffer,size)),\n",
    "                    [size,5]  # 5 = len (experiment)\n",
    "               )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple function to resize our game frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    \"\"\"\n",
    "    This is a simple function to resize our game frames from 84x84x3 to 21168\n",
    "    \"\"\"\n",
    "    return np.reshape(states, [21168])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions allow us to update the parameters of our target network with those of the primary network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars, tau):\n",
    "    \"\"\"\n",
    "    Create computational graph for updating parameters\n",
    "    tfVars: all variables to update\n",
    "    tau: Rate to update target network (tfVars[total_vars//2:]) \n",
    "         toward primary network (var:= tfVars[0:total_vars//2])\n",
    "    \"\"\"\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []  # operation holder\n",
    "    \n",
    "    # Dueling process\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(\n",
    "            tfVars[total_vars//2 + idx].assign(\n",
    "                (var.value()*tau) + ((1-tau)*tfVars[total_vars//2 + idx].value())\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    \"\"\"\n",
    "    Run updating operations\n",
    "    \"\"\"\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting all the training parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32          # How many experiences to use for each training step.\n",
    "update_freq = 4          # How often to perform a training step.\n",
    "y = .99                  # Discount factor on the target Q-values (gamma)\n",
    "startE = 1               # Starting chance of random action (initial exploration rate)\n",
    "endE = 0.1               # Final chance of random action (final exploration rate)\n",
    "annealing_steps = 10000. # How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000     # How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000  # How many steps of random actions before training begins.\n",
    "max_epLength = 50        # The max allowed length of our episode.\n",
    "load_model = False       # Whether to load a saved model.\n",
    "path = \"./dqn\"           # The path to save our model to.\n",
    "h_size = 512             # The size of the final convolutional layer before splitting it into \n",
    "                         # Advantage and Value streams.\n",
    "tau = 0.001              # Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "# create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "# Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational graph**\n",
    "![](https://raw.githubusercontent.com/nguyenbh1507/Example_notebooks/master/RL/Double-Dueling-DQN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model at episode number: 0\n",
      "Total steps: 12616 Mean reward for last 10 episodes: 2.5 Epsilon: 0.7645599999999098\n",
      "Total steps: 13116 Mean reward for last 10 episodes: 1.7 Epsilon: 0.7195599999998925\n",
      "Total steps: 13616 Mean reward for last 10 episodes: 1.1 Epsilon: 0.6745599999998753\n",
      "Total steps: 14116 Mean reward for last 10 episodes: 1.7 Epsilon: 0.629559999999858\n",
      "Total steps: 14616 Mean reward for last 10 episodes: 0.7 Epsilon: 0.5845599999998408\n",
      "Total steps: 15116 Mean reward for last 10 episodes: 1.5 Epsilon: 0.5395599999998235\n",
      "Total steps: 15616 Mean reward for last 10 episodes: 2.7 Epsilon: 0.49455999999980965\n",
      "Total steps: 16116 Mean reward for last 10 episodes: 2.9 Epsilon: 0.44955999999982016\n",
      "Total steps: 16616 Mean reward for last 10 episodes: 2.2 Epsilon: 0.40455999999983067\n",
      "Total steps: 17116 Mean reward for last 10 episodes: 3.4 Epsilon: 0.3595599999998412\n",
      "Total steps: 17616 Mean reward for last 10 episodes: 2.5 Epsilon: 0.3145599999998517\n",
      "Total steps: 18116 Mean reward for last 10 episodes: 1.7 Epsilon: 0.2695599999998622\n",
      "Total steps: 18616 Mean reward for last 10 episodes: 1.7 Epsilon: 0.22455999999986484\n",
      "Total steps: 19116 Mean reward for last 10 episodes: 2.1 Epsilon: 0.17955999999986147\n",
      "Total steps: 19616 Mean reward for last 10 episodes: 2.8 Epsilon: 0.1345599999998581\n",
      "Total steps: 20116 Mean reward for last 10 episodes: 2.3 Epsilon: 0.09999999999985551\n",
      "Total steps: 20616 Mean reward for last 10 episodes: 1.6 Epsilon: 0.09999999999985551\n",
      "Total steps: 21116 Mean reward for last 10 episodes: 2.1 Epsilon: 0.09999999999985551\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # For tensor board only\n",
    "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # loading model\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    \n",
    "    # training\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        # Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)  # reshape map to [21168] \n",
    "        d = False\n",
    "        \n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        # The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            j += 1\n",
    "            \n",
    "            # Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            else:\n",
    "                # feed current state to mainQN.scalarInput and get action by predict \"mainQN.predict\"\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "                \n",
    "            s1,r,d = env.step(a)  # new state, reward, done or not\n",
    "            s1 = processState(s1) # reshape map to [21168]\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Save the experience to our episode buffer\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) \n",
    "            \n",
    "            # In case execute more than pre_train_steps\n",
    "            if total_steps > pre_train_steps:\n",
    "                \n",
    "                # update epsilon\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # Only replay before each \"update_freq\" steps\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    # Get a random batch of experiences.\n",
    "                    trainBatch = myBuffer.sample(batch_size) \n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    \n",
    "                    # feed experience batch to both networks\n",
    "                    # predict actions\n",
    "                    Q1 = sess.run(\n",
    "                        mainQN.predict, \n",
    "                        feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])}\n",
    "                    )\n",
    "                    # calculate Q-value Q(s', argmax_a[Q(s', a)]), equation (1)\n",
    "                    Q2 = sess.run(\n",
    "                        targetQN.Qout, \n",
    "                        feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])}\n",
    "                    )\n",
    "                    \n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)  # index 4 of batch ~ end game status \n",
    "                    doubleQ = Q2[range(batch_size),Q1]  \n",
    "                    \n",
    "                    # Q-target = R + gamma * Q-value, equation (1)\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)  # index 2 of batch ~ reward\n",
    "                    \n",
    "                    # Update the network with our target values.\n",
    "                    _ = sess.run(\n",
    "                        mainQN.updateModel, \n",
    "                        feed_dict={\n",
    "                            mainQN.scalarInput:np.vstack(trainBatch[:,0]),\n",
    "                            mainQN.targetQ:targetQ, \n",
    "                            mainQN.actions:trainBatch[:,1]\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    # Update the target network toward the primary network.\n",
    "                    updateTarget(targetOps,sess) \n",
    "            \n",
    "            # cummulate reward\n",
    "            rAll += r\n",
    "            \n",
    "            # update state\n",
    "            s = s1\n",
    "            \n",
    "            # if game over then break while loop\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        \n",
    "        jList.append(j)     # list of number steps in each episode\n",
    "        \n",
    "        rList.append(rAll)  # list of reward in each episode\n",
    "        \n",
    "        # eriodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved model at episode number: %d\" %i)\n",
    "            \n",
    "        \n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\n",
    "                \"Total steps:\", total_steps, \n",
    "                \"Mean reward for last 10 episodes:\", np.mean(rList[-10:]), \n",
    "                \"Epsilon:\", e\n",
    "            )\n",
    "    \n",
    "    # save model at last episode    \n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking network learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of succesful episodes: 0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean reward over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-bf9bad6aa48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrMat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrMean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrMat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrMean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mscl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mwgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
