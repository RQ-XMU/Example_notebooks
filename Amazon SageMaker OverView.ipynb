{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. What Is Amazon SageMaker?\n",
    "- Amazon SageMaker is a **fully managed machine learning service**.\n",
    "- helps to **quickly and easily build and train machine learning models**, and then **directly deploy** them into a production-ready hosted environment.\n",
    "- provides an **integrated Jupyter** authoring **notebook** instance for **easy access** to your **data** sources\n",
    "- provides **common machine learning algorithms** that are **optimized** to run efficiently against **extremely large data** in a **distributed environment**.\n",
    "- Deploy **with a single click console**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. How It Works\n",
    "This section provides an **overview of machine learning** and explains how Amazon SageMaker works.\n",
    "## 2.1. Machine Learning with Amazon SageMaker\n",
    "Typical workflow for creating a machine learning model:\n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/ml-concepts-10.png)\n",
    "1. **Generate example data**<br>\n",
    "    Data type depends on business problem<br>\n",
    "    a. **Fetch the data** — \n",
    "    pull the dataset or datasets into a single repository<br>\n",
    "    b. **Clean the data** — \n",
    "    To improve model training<br>\n",
    "    c. **Prepare or transform the data** — \n",
    "    To improve performance\n",
    "2. **Train a model**:<br>\n",
    "    a. **Training** the model — To train a model, you need<br>\n",
    "    - an algorithm (can be provided by Amazon Sagemaker or self-implementation)\n",
    "    - compute resources<br>\n",
    "    b. **Evaluating** the model — \n",
    "    to determine whether the accuracy of the inferences is acceptable.<br>\n",
    "    -.use either the AWS SDK for Python (Boto) or the high-level Python library in SageMaker<br>\n",
    "    -.use a Jupyter notebook in SageMaker notebook instance to train and evaluate model.\n",
    "3. **Deploy the model** — independently with Amazon SageMaker hosting services, decoupling from application code. \n",
    "\n",
    "Machine learning is a **continuous cycle**: \n",
    "- => **deploy** model\n",
    "- => **monitor** inferences\n",
    "- => **collect** \"ground truth\"\n",
    "- => **evalutate** model\n",
    "- => **retrain** model\n",
    "- => **deploy new** model\n",
    "- => ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Explore and Preprocess Data\n",
    "\n",
    "- Use a Jupyter notebook on an Amazon SageMaker \n",
    "- use a model to transform data by using Amazon SageMaker batch transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Training a Model with Amazon SageMaker\n",
    "\n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/sagemaker-architecture.png)\n",
    "\n",
    "Create a training job by SageMaker Conslole or API, including:\n",
    "- URL to data on S3\n",
    "- Compute resources, managed by Amazon SageMaker\n",
    "- URL for output data on S3\n",
    "- Amazon Elastic Container Registry path where the training code is stored\n",
    "\n",
    "**Training** options:\n",
    "- Use an algorithm provided by Amazon SageMaker\n",
    "- Use Apache Spark with Amazon SageMaker, similarly to use Spark MLLib\n",
    "- Submit custom code to train with deep learning frameworks: [TensorFlow](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html), [Apache MXNet](https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet.html)\n",
    "- Use custom algorithms in  a Docker image\n",
    "\n",
    "**WorkFlow**:\n",
    "- User creates the training job\n",
    "- => Amazon SageMaker launches the ML compute instances\n",
    "- => SageMaker uses the training code and the training dataset to train the model\n",
    "- =>SageMaker saves the resulting model artifacts and other output in the S3 bucket \n",
    "\n",
    "**Important**\n",
    "- Prevent out-of-memory error!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Model Deployment in Amazon SageMaker\n",
    "- use Amazon **[SageMaker hosting services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)** to set up a persistent endpoint to get **one prediction** at a time, .\n",
    "- use Amazon **[SageMaker batch transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)** to get **predictions** for an **entire dataset**.\n",
    "\n",
    "### 2.4.1. Deploying a Model on Amazon SageMaker Hosting Services\n",
    "A three-step process:\n",
    "1. Create a model in Amazon SageMaker for finding model components\n",
    "2. Create an endpoint configuration for an HTTPS endpoint, configure the endpoint to elastically scale the deployed ML compute instances in production\n",
    "3. Create an HTTPS endpoint: endpoint configuration to Amazon SageMaker\n",
    "\n",
    "**Considerations**\n",
    "- client application sends requests to the Amazon SageMaker HTTPS endpoint to obtain inferences from a deployed model, requests can be sent to this endpoint from Jupyter notebook during testing.\n",
    "- model trained with Amazon SageMaker can be deployed to specific deployment target.\n",
    "- multiple variants of a model can be deployed to the same Amazon SageMaker HTTPS endpoint.\n",
    "- a *ProductionVariant* can be configured to use Application Auto Scaling\n",
    "- an endpoint can be modified without taking models that are already deployed into production out of service.\n",
    "- **Changing or deleting** model artifacts or changing inference code **after deploying** a model produces **unpredictable results**.\n",
    "\n",
    "### 2.4.2. Getting Inferences by Using Amazon SageMaker Batch Transform \n",
    "![](https://docs.aws.amazon.com/sagemaker/latest/dg/images/batch-transform.png)\n",
    "\n",
    "Batch transform manages all compute resources necessary to get inferences. This includes launching instances and deleting them after the transform job completes. \n",
    "\n",
    "To perform a batch transform, create a transform job including:\n",
    "- path to data on S3 bucket\n",
    "- compute resources\n",
    "- path to S3 for output data\n",
    "- name of model in the transform job\n",
    "\n",
    "Batch transform is ideal for situations where:\n",
    "- You want to get inferences for an entire dataset and store them online.\n",
    "- You don't need a persistent endpoint that applications can call to get inferences.\n",
    "- You don't need the sub-second latency that Amazon SageMaker hosted endpoints provide.\n",
    "- You want to preprocess your data before using the data to train a new model or generate inferences.\n",
    "\n",
    "**Considerations**:\n",
    "- transform job can be created by SageMaker Console or API\n",
    "- Amazon SageMaker follows the transform job to read input, launches ML and save output\n",
    "- Amazon SageMaker uses Multipart Upload API to upload output data results from a transform job to S3. \n",
    "- For testing model variants, create separate transform jobs for each variant using a validation data set.\n",
    "- For large datasets or data of indeterminate size, create an infinite stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Validating Machine Learning Models \n",
    "\n",
    "After training, a model needs to be evaluated to determine whether its performance and accuracy allow to achieve business goals.\n",
    "\n",
    "- **Offline testing**: Deploy trained model to an alpha endpoint, and use historical data to send inference requests to it.\n",
    "- **Online testing** with live data: choose to send a portion of the traffic to a model variant for evaluation.\n",
    "\n",
    "**Options** for **offline** evaluation:\n",
    "- **using a \"holdout set\"**: use 20-30% of the training data for validation\n",
    "- **k-fold validation**: split data into k+1 folds, user 1 folds for validation & k folds for training; run k+1 times => k+1 models => aggregate to obtain final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 The Amazon SageMaker Programming Model \n",
    "\n",
    "- use SageMaker APIs to create and manage notebook instances and train and deploy models\n",
    "- alternatives:\n",
    "    - Use the Amazon SageMaker console\n",
    "    - Modify the example Jupyter notebooks\n",
    "    - Write model training and inference code from scratch\n",
    "        - high-level Python library\n",
    "        - AWS SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Getting Started\n",
    "\n",
    "## [3.1: Setting Up](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)\n",
    "### [3.1.1: Create an AWS Account and an Administrator User](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html)\n",
    "- [Create an AWS Account](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html#gs-account-create)\n",
    "- [Create an IAM Administrator User and Sign In](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-account.html#gs-account-user)\n",
    "### [3.1.2: Create an S3 Bucket](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-config-permissions.html)\n",
    "\n",
    "## [3.2: Create an Amazon SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "\n",
    "## [3.3: Train a Model with a Built-in Algorithm and Deploy It](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1.html)\n",
    "\n",
    "### [3.3.1: Create a Jupyter Notebook and Initialize Variables](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html)\n",
    "### [3.3.2: Download, Explore, and Transform the Training Data](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data.html)\n",
    "- 1: Download the MNIST Dataset\n",
    "- 2: Explore the Training Dataset\n",
    "- 3: Transform the Training Dataset and Upload It to S3\n",
    "\n",
    "### [3.3.3: Train a Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html)\n",
    "- 1: Choose the Training Algorithm\n",
    "- 2: Create a Training Job\n",
    "\n",
    "### [3.3.4: Deploy the Model to Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-model-deployment.html)\n",
    "- 1: Deploy the Model to Amazon **SageMaker Hosting** Services\n",
    "- 2: Deploy the Model to Amazon **SageMaker Batch Transform**\n",
    "\n",
    "### [3.3.5: Validate the Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html)\n",
    "\n",
    "## [3.4: Clean up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)\n",
    "## [3.5: Additional Considerations](https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Automatic Model Tuning\n",
    "- Automatic model tuning = **hyperparameter tuning**, finds the **best** version of a **model** by running **many training jobs** on dataset using the **algorithm** and **ranges of hyperparameters** that were specified.\n",
    "- use Amazon SageMaker automatic model tuning with **built-in** algorithms, **custom algorithms**, and Amazon SageMaker **pre-built containers** for machine learning frameworks\n",
    "- Before start hyperparameter tuning, it requires a well-defined machine learning problem, including:\n",
    "    - A **dataset**\n",
    "    - An understanding of the **type of algorithm** needed to train\n",
    "    - A clear understanding of **how to measure success**\n",
    "    \n",
    "## 4.1 How Hyperparameter Tuning Works\n",
    "- **Workflow**:\n",
    "    - hyperparameter tuning makes guesses about which hyperparameter combinations are likely to get the best results, \n",
    "    - runs training jobs to test these guesses. \n",
    "    - after testing uses regression to choose the next set of hyperparameter values\n",
    "- Hyperparameter tuning uses an Amazon SageMaker implementation of **Bayesian optimization**\n",
    "- Use explore/exploit trade-off strategy\n",
    "- **Note**:\n",
    "    - might not improve model\n",
    "    - exploring all of the possible combinations is impractical with complex model\n",
    "    - need to choose the right ranges to explore\n",
    "- **Refernces**:\n",
    "    - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning, [link](https://arxiv.org/abs/1012.2599)\n",
    "    - Practical Bayesian Optimization of Machine Learning Algorithms, [link](https://arxiv.org/abs/1206.2944)\n",
    "    - Taking the Human Out of the Loop: A Review of Bayesian Optimization, [link](http://ieeexplore.ieee.org/document/7352306/?reload=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Defining Objective Metrics\n",
    "- Not required for SageMaker built-in algorithms, just select and use\n",
    "- Required with regular expressions (**regex**) for custom algorithms. Algorthms needs to emit at least one metric by writing evaluation data to *stderr* or *stdout*\n",
    "- The hyperparameter tuning job returns the training job that returned the best value for the objective metric as the best training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Defining Hyperparameter Ranges\n",
    "Choosing hyperparameters and ranges significantly affects the performance of your tuning job.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"ParameterRanges\": {\n",
    "      \"CategoricalParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"tree_method\",\n",
    "          \"Values\": [\"auto\", \"exact\", \"approx\", \"hist\"]\n",
    "        }          \n",
    "      ],\n",
    "      \"ContinuousParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"eta\",\n",
    "          \"MaxValue\" : \"0.5\",\n",
    "          \"MinValue\": \"0\"\n",
    "        }\n",
    "      ],\n",
    "      \"IntegerParameterRanges\": [\n",
    "        {\n",
    "          \"Name\": \"max_depth\",\n",
    "          \"MaxValue\": \"10\",\n",
    "          \"MinValue\": \"1\", \n",
    "        }\n",
    "      ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Example: Hyperparameter Tuning Job\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Design Considerations\n",
    "- Choosing the Number of Hyperparameters\n",
    "    - It's possible to use up to 20 variables simultaneously in a hyperparameter tuning job\n",
    "- Choosing Hyperparameter Ranges\n",
    "    - better results can obtain by searching only in a small range where all possible values in the range are reasonable.\n",
    "- Use Logarithmic Scales for Hyperparameters\n",
    "    - could improve hyperparameter optimization\n",
    "- Choosing the Best Degree of Parallelism\n",
    "    - running in parallel gets more work done quickly\n",
    "    - running one training job at a time achieves the best results with the least amount of compute time. \n",
    "- Running Training Jobs on Multiple Instances\n",
    "    - hyperparameter tuning uses the last-reported objective metric from all instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Using Notebook Instances\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)\n",
    "- Creating a Notebook Instance\n",
    "- Accessing Notebook Instances\n",
    "- Using Example Notebooks\n",
    "- Set the Notebook Kernel\n",
    "- Installing External Libraries and Kernels in Notebook Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Using Built-in Algorithms\n",
    "Because a model is created to **address a business question**, the **first** step is to **understand the problem** needed to solve. Specifically, the format of the answer influences the algorithm.\n",
    "\n",
    "**Examples:**\n",
    "- Answers that fit into discrete categories >>> use *Linear Learner* and *XGBoost*\n",
    "- Answers that are quantitative >>> also use *Linear Learner* and *XGBoost*\n",
    "- Answers in the form of discrete recommendations >>> use *Factorization Machines*\n",
    "- Classify customer >>> K-Means Algorithm\n",
    "- understand customer attributes >>> PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Common Information\n",
    "### 6.1.1 Common Parameters \n",
    "\n",
    "**Computer resources**\n",
    "![](https://imgur.com/f3gNKOm.png)\n",
    "\n",
    "\n",
    "**AWS region**<br>\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n",
    "\n",
    "### 6.1.2 Common Data Formats \n",
    "#### Training\n",
    "- Training Data Formats \n",
    "    - CSV\n",
    "    - protobuf [recordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html#data-format) format\n",
    "- Trained Model Deserialization\n",
    "    - Amazon SageMaker models are stored as **model.tar.gz** in the S3 bucket specified in *OutputDataConfig S3OutputPath* parameter of the *create_training_job* call.\n",
    "\n",
    "#### Inference\n",
    "- Inference Request Serialization\n",
    "    - text/csv, \n",
    "    - application/json, \n",
    "    - application/x-recordio-protobuf.\n",
    "    - text/x-libsvm\n",
    "- Inference Response Deserialization \n",
    "    - Amazon SageMaker algorithms return JSON in several layouts.\n",
    "- Common Request Formats for All Algorithms \n",
    "    - JSON\n",
    "    - JSONLINES\n",
    "    - CSV\n",
    "    - RECORDIO\n",
    "- Using Batch Transform with Build-in Algorithms \n",
    "    - JSONLINES\n",
    "\n",
    "### 6.1.3 Suggested Instance Types \n",
    "For training and hosting Amazon SageMaker algorithms, we recommend using the following EC2 instance types:\n",
    "- ml.m4.xlarge, ml.m4.4xlarge, and ml.m4.10xlarge\n",
    "- ml.c4.xlarge, ml.c4.2xlarge, and ml.c4.8xlarge\n",
    "- ml.p2.xlarge, ml.p2.8xlarge, and ml.p2.16xlarge\n",
    "\n",
    "### 6.1.4 Logs \n",
    "**Note**\n",
    "If a job fails and logs do not appear in CloudWatch, it's likely that an error occurred before the start of training. Reasons include specifying the wrong training image or S3 location.\n",
    "\n",
    "The contents of logs vary by algorithms. However, you can typically find the following information:\n",
    "- Confirmation of arguments provided at the beginning of the log\n",
    "- Errors that occurred during training\n",
    "- Measurement of an algorithms accuracy or numerical performance\n",
    "- Timings for the algorithm, and any major stages within the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 BlazingText\n",
    "[link](https://dl.acm.org/citation.cfm?doid=3146347.3146354)\n",
    "- A highly optimized implementations of the **Word2vec** (sentiment analysis, named entity recognition, machine translation) and **text classification** (web search, information retrieval, ranking and document classification) algorithms.\n",
    "- Similar to Word2vec, it provides the Skip-gram and continuous bag-of-words (CBOW) training architectures.\n",
    "\n",
    "**BlazingText provides the following features**:\n",
    "- Accelerated training of fastText text classifier on multi-core CPUs or a GPU and Word2Vec on GPUs using highly optimized CUDA kernels.\n",
    "- Enriched Word Vectors with Subword Information by learning vector representations for character n-grams.\n",
    "- A batch_skipgram mode for the Word2Vec algorithm that allows faster training and distributed computation across multiple CPU nodes. \n",
    "\n",
    "### 6.2.1 Input/Output Interface\n",
    "### 6.2.2 Training and Validation Data Format\n",
    "- Word2Vec algorithm: a training sentence per line\n",
    "- Text Classification algorithm: a training sentence per line along with the labels\n",
    "### 6.2.3 Model artifacts and Inference \n",
    "- Word2Vec algorithm: *vectors.txt* which contains words to vectors mapping (compatible with other tools like Gensim and Spacy) and *vectors.bin*.\n",
    "- Text Classification algorithm: model.bin \n",
    "### 6.2.4 EC2 Instance [Recommendation]()\n",
    "### 6.2.5 BlazingText Sample [Notebooks](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_text_classification_dbpedia/blazingtext_text_classification_dbpedia.ipynb)\n",
    "\n",
    "### 6.2.6 [Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html)\n",
    "#### Word2Vec Hyperparameters\n",
    "- mode, batch_size, buckets, epochs, evaluation, learning_rate\n",
    "- min_char, min_count, max_char, negative_samples\n",
    "- sampling_threshold, subwords, vector_dim, window_size\n",
    "#### Text Classification Hyperparameters\n",
    "- mode, buckets, early_stopping, epochs, learning_rate\n",
    "- min_count, min_epochs, patience, vector_dim, word_ngrams\n",
    "\n",
    "### 6.2.7 [Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html) a BlazingText Model\n",
    "#### Metrics Computed by the BlazingText Algorithm \n",
    " <div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b7b7\">\n",
    "          <tr>\n",
    "             <th>Metric Name</th>\n",
    "             <th>Description</th>\n",
    "             <th>Optimization Direction</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">train:mean_rho</code></td>\n",
    "             <td>\n",
    "                <p>Mean rho (Spearman's rank correlation coefficient) on <a href=\"https://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\" target=\"_blank\">WS-353 word similarity datasets</a>.\n",
    "                </p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>Maximize</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">validation:accuracy</code></td>\n",
    "             <td>\n",
    "                <p>Classification accuracy on user specified validation dataset.\n",
    "                </p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>Maximize</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    " </div>\n",
    "\n",
    "#### Tunable Hyperparameters for Word2Vec\n",
    "<div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b9b3b5\">\n",
    "          <tr>\n",
    "             <th>Parameter Name</th>\n",
    "             <th>Parameter Type</th>\n",
    "             <th>Recommended Ranges</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">batch_size</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[8-32]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">epochs</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-15]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">learning_rate</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>MinValue: 0.005, MaxValue: 0.01</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">min_count</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[0-100]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">mode</code></td>\n",
    "             <td>\n",
    "                <p>CategoricalParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>['batch_skipgram', 'skipgram', 'cbow']</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">negative_samples</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-25]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">sampling_threshold</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "               <p>MinValue: 0.0001, MaxValue: 0.001</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">vector_dim</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[32-300]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">window_size</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1-10]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    " </div>\n",
    " \n",
    "#### Tunable Hyperparameters for Text Classification\n",
    " <div class=\"table\">\n",
    "    <div class=\"table-contents\">\n",
    "       <table id=\"w1584aac23c25c29b9b5b5\">\n",
    "          <tr>\n",
    "             <th>Parameter Name</th>\n",
    "             <th>Parameter Type</th>\n",
    "             <th>Recommended Ranges</th>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">buckets</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1000000-10000000]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">epochs</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[5-15]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">learning_rate</code></td>\n",
    "             <td>\n",
    "                <p>ContinuousParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>MinValue: 0.005, MaxValue: 0.01</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">min_count</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[0-100]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">mode</code></td>\n",
    "             <td>\n",
    "                <p>CategoricalParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>['supervised']</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">vector_dim</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[32-300]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "          <tr>\n",
    "             <td><code class=\"code\">word_ngrams</code></td>\n",
    "             <td>\n",
    "                <p>IntegerParameterRange</p>\n",
    "             </td>\n",
    "             <td>\n",
    "                <p>[1-3]</p>\n",
    "             </td>\n",
    "          </tr>\n",
    "       </table>\n",
    "   </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 DeepAR Forecasting\n",
    "[Link](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)\n",
    "\n",
    "Amazon SageMaker DeepAR is a supervised learning algorithm for forecasting scalar (that is, one-dimensional) **time series** using recurrent neural networks (**RNN**) - **ARIMA, ETS**\n",
    "\n",
    "Examples of such time series groupings are **demand** for different **products**, **server loads**, and **requests** for web pages. In this case, it can be beneficial to train a single model jointly over all of these time series. \n",
    "\n",
    "**Topics**:\n",
    "### 6.3.1    Input/Output Interface\n",
    "- DeepAR supports two data channels **train** and **test** with *JSONLINES* file format (either ***.json*** or ***.json.gz*** or ***.parquet***)\n",
    "- Use RMSE or weighted quantile loss for evaluation\n",
    "     \n",
    "     $$\\text{RMSE} = \\sqrt{\\frac{1}{nT}\\sum_{i,t}{\\left(\\hat{y}_{i,t}-y_{i.t}\\right)^2}}$$\n",
    "     $$\\text{wQuantileLoss}[\\tau] = 2\\frac{\\sum_{i,t}{Q_{i,t}^{(\\tau)}}}{\\sum_{i,t}{|y_{i,t}|}}\\quad\\text{with}\\quad Q_{i,t}^{(\\tau)}=\\begin{cases}(1-\\tau)|q_{i,t}^{(\\tau)}-y_{i,t}| & \\text{ if } q_{i,t}^{(\\tau)}>y_{i,t}\\\\ \\tau|q_{i,t}^{(\\tau)}-y_{i,t}| & \\text{ otherwise } \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2    Recommended Best Practices\n",
    "- always provide entire time series for training, testing, and when calling the model for prediction\n",
    "- dataset can be split into training and test datasets for tuning a DeepAR Model at different end points\n",
    "- do not use very large values (>400) for *prediction length*\n",
    "- use the same values for *prediction* and *context* lengths.\n",
    "- train DeepAR model on as **many time series** as available\n",
    "### 6.3.3 EC2 Instance Recommendations\n",
    "- can use GPU and CPU\n",
    "- use large machine for large model size\n",
    "    \n",
    "### 6.3.4 DeepAR Sample Notebooks: \n",
    "[Time series forecasting with DeepAR - Synthetic data](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb)\n",
    "\n",
    "### 6.3.5 How DeepAR Works\n",
    "- Under the Hood:<br>\n",
    "    DeepAR automatically creates feature time series<br>\n",
    "    DeepAR model is trained by randomly sampling several training examples from each of the time series in the training dataset.<br>\n",
    "    To capture seasonality patterns, DeepAR also automatically feeds lagged values from the target time series. <br>\n",
    "    For inference, the trained model takes as input target time series, which might or might not have been used during training, and forecasts a probability distribution for the next prediction_length values.\n",
    "    \n",
    "### 6.3.6 [DeepAR Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)\n",
    "- context_length, prediction_length\n",
    "- epochs (+ early_stopping_patience)\n",
    "- time_freq (every minutes, hourly, daily, weekly, monthly)\n",
    "- cardinality (for categorical)\n",
    "- dropout_rate, embedding_dimension, learning_rate\n",
    "- likelihood (gaussian, beta, negative-binomial, student-T, deterministic-L1)\n",
    "- mini_batch_size, num_cells, num_dynamic_feat, num_eval_samples, num_layers, test_quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.7 Tuning a DeepAR Model\n",
    "- **Metrics Computed by the DeepAR Algorithm**:\n",
    "<table id=\"w1649aac23c28c21b7b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:RMSE</code></td>\n",
    "     <td>\n",
    "        <p>Root mean square error between forecast and actual target computed on the test set.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:mean_wQuantileLoss</code></td>\n",
    "     <td>\n",
    "        <p>Average overall quantile losses computed on the test set. Setting the <code class=\"code\">test_quantiles</code> hyperparameter controls which quantiles are used. \n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">train:final_loss</code></td>\n",
    "     <td>\n",
    "        <p>Training negative log-likelihood loss averaged over the last training epoch for the model.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>Minimize</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "</table>    \n",
    "- **Tunable Hyperparameters**:\n",
    "<table id=\"w1649aac23c28c21b9b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 32, MaxValue: 1028</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">epochs</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 1000</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">context_length</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 200</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_cells</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 30, MaxValue: 200</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_layers</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 8</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">dropout_rate</code></td>\n",
    "     <td>\n",
    "        <p>ContinuousParameterRange</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 0.00, MaxValue: 0.2</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">embedding_dimension</code></td>\n",
    "     <td>\n",
    "        <p>IntegerParameterRanges</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1, MaxValue: 50</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">learning_rate</code></td>\n",
    "     <td>\n",
    "        <p>ContinuousParameterRange</p>\n",
    "     </td>\n",
    "     <td>\n",
    "        <p>MinValue: 1e-5, MaxValue: 1e-1</p>\n",
    "     </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.8 DeepAR Inference Formats\n",
    "[**JSON**](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Factorization Machines\n",
    "- A factorization machine is a **general-purpose supervised learning** algorithm that can be used for both **classification** and **regression** tasks. \n",
    "- It is an **extension of a linear model** that is designed to capture interactions between features within high dimensional sparse datasets economically.\n",
    "\n",
    "### 6.4.1 Input/Output Interface\n",
    "- RMSE, Log Loss, Accuracy, F1-score\n",
    "- application/json, x-recordio-protobuf\n",
    "\n",
    "### 6.4.2 EC2 Instance Recommendation\n",
    "- CPUs instances\n",
    "\n",
    "### 6.4.3 Factorization Machines Sample Notebooks\n",
    "- [ An Introduction to Factorization Machines with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/factorization_machines_mnist/factorization_machines_mnist.ipynb)\n",
    "\n",
    "### 6.4.4 How Factorization Machines Work\n",
    "- for prediction task, [FM](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) estimates a function $\\hat{y}$ from a feature set $x_i$ to a target domain:\n",
    "$$\\hat{y}=w_0+\\sum_i w_ix_i +\\sum_i\\sum_{j>i}<v_i,v_j>x_ix_j$$\n",
    "- for regression task, FM minimizes:\n",
    "$$L=\\frac{1}{N}\\sum_n(y_n-\\hat{y}_n)^2$$\n",
    "- for classification task, FM minimizes:\n",
    "$$L=\\frac{1}{N}\\sum_n\\left[y_n\\log\\hat{p}_n+(1-y_n)\\log(1-\\hat{p}_n)\\right]\\text{ where } \\hat{p}_n=\\frac{1}{1+e^{-\\hat{y}_n}}$$\n",
    "\n",
    "### 6.4.5 Factorization Machines Hyperparameters\n",
    "- feature_dim, num_factors,\n",
    "- predictor_type (binary_classifier, regressor)\n",
    "- bias_init_method, bias_init_scale, bias_init_sigma\n",
    "- bias_init_value, bias_lr, bias_wd, \n",
    "- clip_gradient, epochs, eps\n",
    "- factors_init_method, factors_init_scale, factors_init_sigma, factors_init_value, factors_lr, factors_wd, \n",
    "- linear_lr, linear_init_method, linear_init_scale, linear_init_sigma, linear_init_value, linear_wd, \n",
    "- mini_batch_size, rescale_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.6 Tuning a Factorization Machines Model\n",
    "- **Metrics Computed by the Factorization Machines Algorithm**\n",
    "\n",
    "**Regression**\n",
    "<table id=\"w1649aac23c31c21b7b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:rmse</code></td>\n",
    "     <td><p>Root Mean Square Error</p></td>\n",
    "     <td><p>Minimize</p></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "**Classification**\n",
    "<table id=\"w1649aac23c31c21b7b9\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:binary_classification_accuracy</code></td>\n",
    "     <td><p>Accuracy</p></td>\n",
    "     <td><p>Maximize</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:binary_classification_cross_entropy</code></td>\n",
    "     <td><p>Cross Entropy</p></td>\n",
    "     <td><p>Minimize</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:binary_f_beta</code></td>\n",
    "     <td><p>Beta</p></td>\n",
    "     <td><p>Maximize</p></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "- **Tunable Hyperparameters**\n",
    "<table id=\"w1649aac23c31c21b9b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "     <th>Dependency</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_init_scale</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==uniform</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_init_sigma</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==normal</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_init_value</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==constant</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_lr</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">bias_wd</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">epoch</code></td>\n",
    "     <td><p>IntegerParameterRange</p></td>\n",
    "     <td><p>MinValue: 1, MaxValue: 1000</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_init_scale</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==uniform</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_init_sigma</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==normal</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_init_value</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==constant</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_lr</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">factors_wd</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512]</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_init_scale</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==uniform</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_init_sigma</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==normal</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_init_value</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>bias_init_method==constant</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_lr</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">linear_wd</code></td>\n",
    "     <td><p>ContinuousParameterRange</p></td>\n",
    "     <td><p>MinValue: 1e-8, MaxValue: 512</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td><p>IntegerParameterRange</p></td>\n",
    "     <td><p>MinValue: 100, MaxValue: 10000</p></td>\n",
    "     <td><p>None</p></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.7 Factorization Machine Response Formats\n",
    "[JSON](https://docs.aws.amazon.com/sagemaker/latest/dg/fm-in-formats.html#fm-json), [JSONLINES](https://docs.aws.amazon.com/sagemaker/latest/dg/fm-in-formats.html#fm-jsonlines), [RECORDIO](https://docs.aws.amazon.com/sagemaker/latest/dg/fm-in-formats.html#fm-recordio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Image Classification Algorithm\n",
    "- A **supervised learning** algorithm that **takes an image as input** and **classifies** it into one of **multiple** output **categories**. \n",
    "- Uses a convolutional neural network (**ResNet**) that can be **trained from scratch**, or trained using **transfer learning** when a large number of training images are not available.\n",
    "- input format: **MXNet [RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html)** (differs from the *protobuf* data), .jpg, .png\n",
    "- References:\n",
    "    - [Deep residual learning for image recognition ](https://arxiv.org/abs/1512.03385)\n",
    "    - [ImageNet image database](http://www.image-net.org/)\n",
    "    - [Image classification in MXNet](https://github.com/apache/incubator-mxnet/tree/master/example/image-classification)\n",
    "    \n",
    "### 6.5.1 Input/Output Interface\n",
    "- application/x-recordio\n",
    "- application/x-image\n",
    "- training with both, inference only with image format\n",
    "\n",
    "### 6.5.2 EC2 Instance Recommendation\n",
    "- prefer GPU\n",
    "- can train with CPU\n",
    "\n",
    "### 6.5.3 Image Classification Sample Notebooks\n",
    "- [ End-to-End Multiclass Image Classification Example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-fulltraining.ipynb)\n",
    "\n",
    "### 6.5.4 How Image Classification Works\n",
    "- takes an image as input and classifies it into one of the output categories\n",
    "- full and transfer learnings\n",
    "\n",
    "### 6.5.5 Hyperparameters\n",
    "- [link](https://docs.aws.amazon.com/sagemaker/latest/dg/IC-Hyperparameter.html)\n",
    "- num_classes, num_training_samples, augmentation_type (crop, crop_color, crop_color_transform)\n",
    "- beta_1, beta_2, checkpoint_frequency, epochs, eps, gamma\n",
    "- image_shape, kv_store, \n",
    "- learning_rate, lr_scheduler_factor, mlr_scheduler_step, mini_batch_size\n",
    "- momentum, multi_label, num_layers, optimizer\n",
    "- precision_dtype, resize, top_k\n",
    "- use_pretrained_model, use_weighted_loss, weight_decay\n",
    "\n",
    "### 6.5.6 Tuning an Image Classification Model\n",
    "- **Metrics Computed by the Image Classification Algorithm**\n",
    "<table id=\"w1649aac23c34c25b7b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "      <th>Description</th>\n",
    "      <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:accuracy</code></td>\n",
    "     <td>        <p>The ratio of the number of correct predictions to the total           number of predictions made.        </p>     </td>\n",
    "     <td>        <p>Maximize</p>     </td>\n",
    "  </tr>\n",
    "</table>\n",
    "- **Tunable Hyperparameters**\n",
    "<table id=\"w1649aac23c34c25b9b7\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">beta_1</code></td>\n",
    "     <td>       <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-6, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">beta_2</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-6, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">eps</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-8, MaxValue: 1.0</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code class=\"code\">gamma</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "      <td>        <p>MinValue: 1e-8, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code class=\"code\">learning_rate</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1e-6, MaxValue: 0.5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 8, MaxValue: 512</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">momentum</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">optimizer</code></td>\n",
    "     <td>        <p>CategoricalParameterRanges</p>     </td>\n",
    "     <td>        <p>['sgd', ‘adam’, ‘rmsprop’, 'nag']</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">weight_decay</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.999</p>     </td>\n",
    "  </tr>\n",
    "    \n",
    "</table>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 K-Means Algorithm\n",
    "- **find discrete groupings within data**, where members of a group are **as similar as possible** to one another and as **different** as possible from members of other groups.\n",
    "- the version used by Amazon SageMaker is more accurate than [web-scale](https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf)\n",
    "- [k-means algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/algo-kmeans-tech-notes.html) expects tabular data, \n",
    "- Euclidean distance represents the similarity\n",
    "\n",
    "### 6.6.1 Input/Output Interface\n",
    "[k-means Response Formats](https://docs.aws.amazon.com/sagemaker/latest/dg/km-in-formats.html)\n",
    "\n",
    "### 6.6.2 EC2 Instance Recommendation\n",
    "- recommeded CPU instances\n",
    "- can train with GPUs\n",
    "\n",
    "### 6.6.3 K-Means Sample Notebooks\n",
    "- [Analyze US census data for population segmentation using Amazon SageMaker](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/US-census_population_segmentation_PCA_Kmeans/sagemaker-countycensusclustering.ipynb)\n",
    "\n",
    "\n",
    "### 6.6.4 How K-Means Clustering Works\n",
    "- Step 1: Determine the Initial Cluster Centers\n",
    "- Step 2: Iterate over the Training Dataset and Calculate Cluster Centers \n",
    "- Step 3: Reduce the Clusters from K to k\n",
    "\n",
    "### 6.6.5 K-Means Hyperparameters\n",
    "- feature_dim, mini_batch_size\n",
    "- k, init_method, \n",
    "- epochs, eval_metrics, extra_center_factor\n",
    "- half_life_time_size\n",
    "- local_lloyd_max_iter, local_lloyd_init_method, local_lloyd_num_trials, local_lloyd_tol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.6 Tuning a K-Means Model\n",
    "- **Metrics Computed by the K-Means Algorithm**\n",
    "<table id=\"w1649aac23c37c21b9b5\">\n",
    "  <tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization Direction</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:msd</code></td>\n",
    "     <td>\n",
    "        <p>Mean squared distances between each record in the test set and the closest center of the model.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>        <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">test:ssd</code></td>\n",
    "     <td>\n",
    "        <p>Sum of the squared distances between each record in the\n",
    "           test set and the closest center of the model.\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>        <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "- **Tunable Hyperparameters**:\n",
    "<table id=\"w1649aac23c37c21c11b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">epochs</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 1, MaxValue:10</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">extra_center_factor</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 4, MaxValue:10</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">init_method</code></td>\n",
    "     <td>        <p>CategoricalParameterRanges</p>     </td>\n",
    "     <td>        <p>['kmeans++', 'random']</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">mini_batch_size</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 3000, MaxValue:15000</p>     </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6.7 k-means Response Formats\n",
    "- JSON\n",
    "- JSONLINES\n",
    "- RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 K-Nearest Neighbors\n",
    "- an index-based algorithm\n",
    "- uses a non-parametric method for **classification** or **regression**\n",
    "- For **classification** problems, the algorithm queries the **k points** that are **closest** to the sample point and **returns the most frequently used label** of their class **as the predicted** label. \n",
    "- For **regression problems**, the algorithm queries the **k closest points** to the sample point and **returns the average** of their feature values **as the predicted** value.\n",
    "- **three steps**: sampling, dimension reduction, and index building\n",
    "\n",
    "### 6.7.1 Input/Output Interface\n",
    "- train in: text/csv, application/x-recordio-protobuf; \n",
    "- train out: text/csv\n",
    "- inference in: application/json, application/x-recordio-protobuf, text/csv, \n",
    "- inference out: application/json, application/x-recordio-protobuf\n",
    "- batch transform: application/jsonlines\n",
    "\n",
    "### 6.7.2 kNN Sample Notebooks\n",
    "[K-Nearest Neighbor Covertype](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/k_nearest_neighbors_covtype/k_nearest_neighbors_covtype.ipynb)\n",
    "\n",
    "### 6.7.4 EC2 Instance Recommendation\n",
    "- Training: CPU\n",
    "- Inference: CPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.3 How It Works\n",
    "- Step 1: Sampling\n",
    "- Step 2: Dimension Reduction: ***sign*** for specifies a random projection and ***fjlt*** for fast Johnson-Lindenstrauss transform\n",
    "- Step 3: Building an Index\n",
    "- Model Serialization: preparation for inference\n",
    "\n",
    "### 6.7.5 K-Nearest Neighbors Hyperparameters\n",
    "- [feature_dim](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN_hyperparameters.html), k, predictor_type (classifier, regressor)\n",
    "- sample_size, dimension_reduction_target, dimension_reduction_type (sign, fjlt) \n",
    "- faiss_index_ivf_nlists, faiss_index_pq_m, \n",
    "- index_metric, index_type, mini_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7.6 Tuning a K-Nearest Neighbors Model\n",
    "- **Metrics Computed by the K-Nearest Neighbors Algorithm**\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Metric Name</th>\n",
    "        <th>Optimization Direction</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>test:accuracy</td>\n",
    "        <td>Maximize</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>test:mse</td>\n",
    "        <td>Minimize</td>\n",
    "    </tr>\n",
    "</table>\n",
    "- **Tunable Hyperparameters**\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Parameter Name</th>\n",
    "        <th>Parameter Type</th>\n",
    "        <th>Recommended Ranges</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>k</td>\n",
    "        <td>IntegerParameterRanges</td>\n",
    "        <td>MinValue: 1, MaxValue: 1024</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>sample_size</td>\n",
    "        <td>IntegerParameterRanges</td>\n",
    "        <td>MinValue: 256, MaxValue: 20000000</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### 6.7.7 Data Formats for K-Nearest Neighbors Training Input\n",
    "- [CSV] and [RECORDIO](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-in-formats.html)\n",
    "\n",
    "### 6.7.8 K-NN Request and Response Formats\n",
    "- [INPUT](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-inference-formats.html): CSV, JSON, JSONLINES, RECORDIO\n",
    "- [OUPUT](https://docs.aws.amazon.com/sagemaker/latest/dg/kNN-inference-formats.html#kNN-output-json): JSON, JSONLINES, VERBOSE JSON, RECORDIO-PROTOBUF, VERBOSE RECORDIO-PROTOBUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 Latent Dirichlet Allocation (LDA)\n",
    "- attempts to describe a set of observations as a mixture of distinct categories\n",
    "- used to discover a user-specified number of topics shared by documents within a text corpus\n",
    "\n",
    "### 6.8.1 Input/Output Interface\n",
    "Supports recordIO-wrapped-protobuf (dense and sparse) and CSV file formats\n",
    "For inference, text/csv, application/json, and application/x-recordio-protobuf content types are supported.\n",
    "\n",
    "### 6.8.2 EC2 Instance Recommendation\n",
    "- currently only supports single-instance CPU training\n",
    "\n",
    "### 6.8.3 LDA Sample Notebooks\n",
    "[An Introduction to SageMaker LDA](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/lda_topic_modeling/LDA-Introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8.4 How LDA Works\n",
    "- an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of different categories\n",
    "- LDA is a generative probability model\n",
    "- LDA can be used for a variety of tasks, from **clustering customers** based on product purchases to **automatic harmonic** analysis in music. \n",
    "- is most commonly associated with **topic modeling** in text corpuses.\n",
    "- LDA is a \"bag-of-words\" model\n",
    "- For each word: \n",
    "    - Choose a **topic** z ∼ Multinomial(θ) \n",
    "    - Choose the corresponding **topic-word distribution** β_z\n",
    "    - Draw a **word** w ∼ Multinomial(β_z)\n",
    "- The goal is to find parameters α and β:\n",
    "    - α — A prior estimate on topic probability\n",
    "    - β — \"topic-word distribution.\"\n",
    "- use Gibbs sampling or Expectation Maximization (EM) techniques to estimate\n",
    "- **Tensor decomposition algorithm**:\n",
    "    - The goal is to calculate the spectral decomposition of a V x V x V tensor\n",
    "    - uses a V x V moment matrix to find a whitening matrix of dimension V x k\n",
    "    - This same whitening matrix can then be used to find a smaller k x k x k tensor\n",
    "    - Alternating Least Squares is used to decompose the smaller k x k x k tensor. \n",
    "- [More info](https://docs.aws.amazon.com/sagemaker/latest/dg/lda-how-it-works.html)\n",
    "        \n",
    "### 6.8.5 LDA Hyperparameters\n",
    "- num_topics, feature_dim, mini_batch_size\n",
    "- alpha0, max_restarts, max_iterations, tol\n",
    "\n",
    "### 6.8.6 Tuning an LDA Model\n",
    "- **Metrics Computed by the LDA Algorithm**\n",
    "    - Maximize per-word log-likelihood\n",
    "- **Tunable Hyperparameters**\n",
    "    - alpha0: 0.1-10\n",
    "    - number of topic: 1-150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.9 Linear Learner\n",
    "- algorithms used for solving either **classification** or **regression** problems\n",
    "- learns a linear function (**Linear regression**), or linear threshold function (**Logistic regression**) for classification, mapping a vector x to an approximation of the label y\n",
    "\n",
    "### 6.9.1 Input/Output Interface\n",
    "- recordIO wrapped protobuf and CSV\n",
    "- application/x-recordio-protobuf, text/csv,\n",
    "- For inference: application/json, application/x-recordio-protobuf, and text/csv\n",
    "\n",
    "### 6.9.2 EC2 Instance Recommendation\n",
    "- single- or multi-machine CPU and GPU instances\n",
    "\n",
    "### 6.9.3 Linear Learner Sample Notebooks\n",
    "- [ An Introduction to Linear Learner with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/linear_learner_mnist/linear_learner_mnist.ipynb)\n",
    "\n",
    "### 6.9.4 How It Works\n",
    "- Step 1: Preprocessing: normalization and standalization\n",
    "- Step 2: Training \n",
    "- Step 3: Validation and Setting the Threshold \n",
    "\n",
    "### 6.9.5 Linear Learner Hyperparameters\n",
    "- feature_dim, num_classes, predictor_type, \n",
    "- accuracy_top_k, balance_multiclass_weights\n",
    "- beta_1, beta_2, bias_lr_mult, bias_wd_mult\n",
    "- binary_classifier_model_selection_criteria\n",
    "- early_stopping_tolerance, early_stopping_patience, \n",
    "- epochs [...](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html)\n",
    "\n",
    "\n",
    "### 6.9.6 Tuning a Linear Learner Model\n",
    "- **Metrics Computed by the Linear Learner Algorithm**, [source](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html#linear-learner-metrics)\n",
    "- **Tuning Hyperparameters** [source](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html#linear-learner-tunable-hyperparameters)\n",
    "\n",
    "### 6.9.7 Linear Learner Response Formats\n",
    "- JSON, JSONLINES, RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.10 Neural Topic Model (NTM)\n",
    "- an unsupervised learning algorithm that is used to **organize a corpus of documents into topics** that contain word groupings based on their statistical distribution\n",
    "- [NTM](https://arxiv.org/pdf/1511.06038.pdf) and LDA are distinct algorithms for topic modeling\n",
    "\n",
    "### 6.10.1 Input/Output Interface\n",
    "- for train, validation, test, and [auxiliary](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/scientific_details_of_algorithms/ntm_topic_modeling/ntm_wikitext.ipynb): recordIO-wrapped-protobuf, csv\n",
    "- for inference: text/csv, application/json, application/jsonlines, application/x-recordio-protobuf\n",
    "- [WETC score](https://arxiv.org/pdf/1809.02687.pdf)\n",
    "\n",
    "\n",
    "### 6.10.2 EC2 Instance Recommendation\n",
    "- GPU and CPU instance types\n",
    "\n",
    "### 6.10.3 NTM Sample Notebooks\n",
    "- [Introduction to Basic Functionality of NTM](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/ntm_synthetic/ntm_synthetic.ipynb)\n",
    "\n",
    "### 6.10.4 NTM Hyperparameters\n",
    "[link](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm_hyperparameters.html)\n",
    "\n",
    "### 6.10.5 Tuning an NTM Model\n",
    "- Minimize ***total_loss***\n",
    "- Tunable Hyperparameters: encoder_layers_activation, learning_rate, mini_batch_size, optimizer, rescale_gradient, weight_decay\n",
    "\n",
    "### 6.10.6 NTM Response Formats\n",
    "- JSON, JSONLINES, RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.11 Object Detection Algorithm\n",
    "- **detects and classifies** objects in images using a single deep neural network\n",
    "- It uses the **Single Shot multibox Detector** (SSD) framework and supports two base networks: **VGG** and **ResNet**. The network can be **trained from scratch**, or trained with models that have been **pre-trained** on the **ImageNet** dataset.\n",
    "\n",
    "### 6.11.1 Input/Output Interface\n",
    "- Training with RecordIO Format \n",
    "- Training with Image Format\n",
    "\n",
    "### 6.11.2 EC2 Instance Recommendation\n",
    "- GPU instances \n",
    "\n",
    "### 6.11.3 Object Detection Sample Notebooks\n",
    "- [Object Detection using the Image and JSON format](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_image_json_format.ipynb)\n",
    "\n",
    "### 6.11.4 How Object Detection Works\n",
    "- takes an image as input and outputs the category that the object belongs to, along with a confidence score\n",
    "- Object detection in Amazon SageMaker supports both **VGG-16** and **ResNet-50** as a base network for SSD.\n",
    "- full/transfer learning\n",
    "\n",
    "### 6.11.5 Object Detection Hyperparameters\n",
    "- num_classes, num_training_samples, base_network\n",
    "- image_shape, epochs, freeze_layer_pattern\n",
    "- kv_store, label_width\n",
    "- learning_rate, lr_scheduler_factor, lr_scheduler_step, mini_batch_size, momentum, nms_threshold, \n",
    "- optimizer, overlap_threshold, \n",
    "- use_pretrained_model, weight_decay\n",
    "\n",
    "### 6.11.6 Tuning an Object Detection Model\n",
    "- Maximize **mAP**\n",
    "- Tunable Hyperparameters: learning_rate, mini_batch_size, momentum, optimizer, weight_decay\n",
    "\n",
    "### 6.11.7 Object Detection Request and Response Formats\n",
    "- in: image/jpeg and image/png\n",
    "- out: JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.12 Principal Component Analysis (PCA)\n",
    "-  attempts to **reduce the dimensionality**, retaining as **much infor**mation as possible\n",
    "- uses tabular data\n",
    "\n",
    "### 6.12.1 Input/Output Interface\n",
    "- training:  recordIO-wrapped-protobuf, csv\n",
    "- inference: text/csv, application/json, application/x-recordio-protobuf\n",
    "\n",
    "### 6.12.2 EC2 Instance Recommendation\n",
    "- GPU and CPU computation\n",
    "\n",
    "### 6.12.3 Principal Component Analysis Sample Notebooks\n",
    "- [Principal Component Analysis Sample Notebooks](https://docs.aws.amazon.com/sagemaker/latest/dg/pca.html#PCA-sample-notebooks)\n",
    "- [An Introduction to PCA with MNIST](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/pca_mnist/pca_mnist.ipynb)\n",
    "\n",
    "### 6.12.4 How PCA [Works](https://docs.aws.amazon.com/sagemaker/latest/dg/how-pca-works.html)\n",
    "- finding a new set of features called **components**, which are composites of the original features, but are **uncorrelated with one another**. \n",
    "- The **first** component accounts for the **largest possible variability** in the data, the **second** component the **second most variability**, and so on.\n",
    "- Mode 1: **Regular**: for datasets with sparse data and a moderate number of observations and features\n",
    "- Mode 2: **[Randomized](https://docs.aws.amazon.com/sagemaker/latest/dg/how-pca-works.html#mode-2)**: for datasets with both a large number of observations and features, [FJLT transform](https://www.cs.princeton.edu/~chazelle/pubs/FJLT-sicomp09.pdf)\n",
    "\n",
    "### 6.12.5 PCA Hyperparameters\n",
    "- feature_dim, mini_batch_size, num_components, algorithm_mode\n",
    "- extra_components, subtract_mean\n",
    "\n",
    "### 6.12.6 PCA Response Formats\n",
    "- JSON, JSONLINES, RECORDIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.13 Random Cut Forest\n",
    "- algorithm for **detecting anomalous** data points within a data set\n",
    "\n",
    "### 6.13.1 Input/Output Interface\n",
    "- input: <code>application/x-recordio-protobuf, text/csv</code> and <code>application/json</code>\n",
    "- output: <code>appplication/x-recordio-protobuf</code> or <code>application/json</code>\n",
    "\n",
    "### 6.13.2 Instance Recommendations\n",
    "- training: <code>ml.m4, ml.c4</code>, and <code>ml.c5</code> instance families\n",
    "- inference:  <code>ml.c5.xl</code> instance type\n",
    "\n",
    "### 6.13.3 Randon Cut Forest Sample Notebooks\n",
    "[An Introduction to SageMaker Random Cut Forests](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb)\n",
    "\n",
    "### 6.13.4 How RCF Works\n",
    "Create a **forest of trees** where each tree is obtained using a **partition of a sample** of the training data<br>\n",
    "- Randomly Sampling Data<br>\n",
    "    [Reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling): draw sample data from big training data\n",
    "- Training and Inference \n",
    "    - the sample is partitioned into a **number** of equal-sized **partitions** equal to the **number of trees** in the forest. \n",
    "    - each partition is sent to an individual tree. \n",
    "    - The **tree recursively organizes its partition** into a **binary** tree by partitioning the data domain into bounding boxes\n",
    "- Choosing Hyperparameters: <code>num_trees</code> and <code>num_samples_per_tree</code>\n",
    "\n",
    "### 6.13.5 RCF Hyperparameters\n",
    "- <code>feature_dim</code>\n",
    "- <code>eval_metrics</code>\n",
    "- <code>num_samples_per_tree</code>\n",
    "- <code>num_trees</code>\n",
    "\n",
    "### 6.13.6 Tuning a RCF Model\n",
    "- Metrics Computed by the RCF Algorithm:<br>\n",
    "    Maximize <code>test:f1</code> - F1 score on the test dataset, based on the difference between calculated labels and actual labels.\n",
    "- Tunable Hyperparameters:\n",
    "<table id=\"w1685aac23c60c21c11b5\">\n",
    "  <tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter Type</th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_samples_per_tree</code></td>\n",
    "     <td>  <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>       <p>MinValue: 1, MaxValue:2048</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_trees</code></td>\n",
    "     <td>       <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>       <p>MinValue: 50, MaxValue:1000</p>    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### 6.13.7 RCF Response Formats\n",
    "- JSON/JSONLINES/RECODIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.14 Sequence to Sequence (seq2seq)\n",
    "- a supervised learning algorithm where the **input** is a **sequence of tokens** and the **output** generated is **another sequence** of tokens\n",
    "- example: machine translation, text summarization, speech-to-text, \n",
    "- uses **RNNs** and **CNNs** with **attention** as **encoder-decoder** architectures\n",
    "\n",
    "### 6.14.1 Input/Output Interface\n",
    "- Training\n",
    "    - data in **RecordIO-Protobuf** format\n",
    "    - tokens are expected as **integers**\n",
    "    - The algorithm expects three channels: *train*, *validation* and *vocab*\n",
    "- Inference\n",
    "    - <code>application/json</code> and <code>application/x-recordio-protobuf</code>\n",
    "\n",
    "### 6.14.2 EC2 Instance Recommendation\n",
    "- only supported on GPU instance types on single machine\n",
    "\n",
    "### 6.14.3 Sample Notebooks\n",
    "[ Machine Translation English-German Example Using SageMaker Seq2Seq](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/seq2seq_translation_en-de/SageMaker-Seq2Seq-Translation-English-German.ipynb)\n",
    "\n",
    "### 6.14.4 How Sequence to Sequence Works\n",
    "- neural network for seq2seq includes:\n",
    "    - An **embedding layer** transform one-hot encoding to dense feature vector\n",
    "    - An **encoder layer** compresses input to a fixed-length feature vector\n",
    "    - A **decoder layer** takes encoded feature vector and produces the output sequence\n",
    "- The whole model is trained jointly to **maximize** the **probability** of the **target** sequence **given** the **source** sequence. [Link](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
    "- **Attention mechanism**: In an [attention mechanism](https://arxiv.org/pdf/1409.0473.pdf), the decoder tries to **find the location** in the encoder sequence where the **most important information** could be located and uses that information and **previously decoded words** to **predict the next** token in the sequence.\n",
    "- Whitepaper: [1](https://arxiv.org/abs/1508.04025), [2](https://arxiv.org/abs/1609.08144)\n",
    "\n",
    "### 6.14.5 [Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/seq-2-seq-hyperparameters.html)\n",
    "- batch_size, beam_size, bleu_sample_size, bucket_width\n",
    "- bucketing_enabled, checkpoint_frequency_num_batches, checkpoint_threshold, clip_gradient\n",
    "- cnn_activation_type, cnn_hidden_dropout, cnn_kernel_width_decoder, cnn_kernel_width_encoder, cnn_num_hidden\n",
    "- decoder_type, embed_dropout_source, embed_dropout_target, encoder_type\n",
    "- fixed_rate_lr_half_life, learning_rate, loss_type, lr_scheduler_type\n",
    "- max_num_batches, max_num_epochs, max_seq_len_source, max_seq_len_target, min_num_epochs, momentum\n",
    "- num_embed_source, num_embed_target, num_layers_decoder, num_layers_encoder\n",
    "- optimized_metric, optimizer_type, plateau_reduce_lr_factor, plateau_reduce_lr_threshold\n",
    "- rnn_attention_in_upper_layers, rnn_attention_num_hidden, rnn_attention_type, rnn_cell_type, rnn_decoder_state_init, rnn_first_residual_layer, rnn_num_hidden, rnn_residual_connections, rnn_decoder_hidden_dropout\n",
    "- training_metric, weight_decay, weight_init_scale, weight_init_type, xavier_factor_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.14.6 Tuning Model\n",
    "- **Metrics Computed by the Sequence to Sequence Algorithm**\n",
    "<table>\n",
    "    <tr>\n",
    " <th>Metric Name </th>\n",
    " <th>Description</th>\n",
    " <th>Optimization Direction</th>\n",
    "</tr>\n",
    "<tr>\n",
    " <td><code class=\"code\">validation:accuracy</code></td>\n",
    " <td>    <p>Accuracy       computed on the validation dataset.    </p> </td>\n",
    " <td>    <p>Maximize</p> </td>\n",
    "</tr>\n",
    "<tr>\n",
    " <td><code class=\"code\">validation:bleu</code></td>\n",
    " <td>\n",
    "    <p><a href=\"https://en.wikipedia.org/wiki/BLEU\" target=\"_blank\">Bleu﻿</a>\n",
    "       score computed on the validation dataset. Because BLEU\n",
    "       computation is expensive, you can choose to compute BLEU on a\n",
    "       random subsample of the validation dataset to speed up the\n",
    "       overall training process.\n",
    "       Use\n",
    "       the <code class=\"code\">bleu_sample_size</code> parameter to specify the\n",
    "       subsample.\n",
    "    </p>\n",
    " </td>\n",
    " <td>    <p>Maximize</p> </td>\n",
    "</tr>\n",
    "<tr>\n",
    " <td><code class=\"code\">validation:perplexity</code></td>\n",
    " <td>\n",
    "    <p><a href=\"https://en.wikipedia.org/wiki/Perplexity\" target=\"_blank\">Perplexity</a>,\n",
    "       is\n",
    "       a loss function computed on the validation\n",
    "       dataset.\n",
    "       Perplexity measures the cross-entropy between an empirical\n",
    "       sample and the distribution predicted by a model and so provides\n",
    "       a measure of how well a model predicts the sample values, Models\n",
    "       that are good at predicting a sample have a low\n",
    "       perplexity.\n",
    "    </p>\n",
    " </td>\n",
    " <td>    <p>Minimize</p> </td>\n",
    "</tr>\n",
    "</table>\n",
    "- **Tunable Hyperparameters**\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Parameter       Name     </th>\n",
    "     <th>Parameter        Type     </th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_layers_encoder</code></td>\n",
    "     <td>        <p>IntegerParameterRange</p>     </td>\n",
    "     <td>        <p>[1-10]</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code class=\"code\">num_layers_decoder</code></td>\n",
    "     <td>        <p>IntegerParameterRange</p>     </td>\n",
    "    <td>        <p>[1-10]</p>     </td>\n",
    "  </tr>\n",
    " <tr>\n",
    "     <td><code class=\"code\">batch_size</code></td>\n",
    "    <td>        <p>CategoricalParameterRange</p>     </td>\n",
    "    <td>        <p>[16,32,64,128,256,512,1024,2048]</p>     </td>\n",
    " </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">optimizer_type</code></td>\n",
    "     <td>        <p>CategoricalParameterRange</p>     </td>\n",
    "     <td>        <p>['adam', 'sgd', 'rmsprop']</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">weight_init_type</code></td>\n",
    "     <td>        <p>CategoricalParameterRange</p>     </td>\n",
    "     <td>        <p>['xavier',          'uniform']        </p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">weight_init_scale</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>\n",
    "        <p>For the xavier type: MinValue: 2.0, MaxValue: 3.0 For the\n",
    "           uniform type: MinValue: -1.0, MaxValue: 1.0\n",
    "        </p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">learning_rate</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.00005, MaxValue: 0.2</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">weight_decay</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.1</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">momentum</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.5, MaxValue: 0.9</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">clip_gradient</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 1.0, MaxValue: 5.0</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">rnn_num_hidden</code></td>\n",
    "     <td>        <p>CategoricalParameterRange</p>     </td>\n",
    "     <td>\n",
    "        <p>Applicable\n",
    "           only to recurrent neural networks (RNNs).\n",
    "           [128,256,512,1024,2048] \n",
    "        </p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">cnn_num_hidden</code></td>\n",
    "     <td>        <p>CategoricalParameterRange</p>     </td>\n",
    "     <td>\n",
    "        <p>Applicable\n",
    "           only to convolutional neural networks (CNNs).\n",
    "           [128,256,512,1024,2048] \n",
    "        </p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_embed_source</code></td>\n",
    "     <td>        <p>IntegerParameterRange</p>     </td>\n",
    "     <td>        <p>[256-512]</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_embed_target</code></td>\n",
    "     <td>        <p>IntegerParameterRange</p>     </td>\n",
    "     <td>        <p>[256-512]</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">embed_dropout_source</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><code class=\"code\">embed_dropout_target</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">rnn_decoder_hidden_dropout</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.5</p>    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">cnn_hidden_dropout</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.0, MaxValue: 0.5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">lr_scheduler_type</code></td>\n",
    "     <td>        <p>CategoricalParameterRange</p>     </td>\n",
    "     <td><p>['plateau_reduce', 'fixed_rate_inv_t', 'fixed_rate_inv_sqrt_t']</p> </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">plateau_reduce_lr_factor</code></td>\n",
    "     <td>        <p>ContinuousParameterRange</p>     </td>\n",
    "     <td>        <p>MinValue: 0.1, MaxValue: 0.5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">plateau_reduce_lr_threshold</code></td>\n",
    "     <td>        <p>IntegerParameterRange</p>     </td>\n",
    "     <td>        <p>[1-5]</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">fixed_rate_lr_half_life</code></td>\n",
    "     <td>        <p>IntegerParameterRange</p>     </td>\n",
    "     <td>        <p>[10-30]</p> </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.15 XGBoost Algorithm\n",
    "- [XGBoost](https://github.com/dmlc/xgboost)(eXtreme Gradient Boosting) is a supervised learning algorithm that **attempts to accurately predict** a target variable by **combining** the estimates of a set of **simpler, weaker models**.\n",
    "\n",
    "### 6.15.1 Input/Output Interface\n",
    "- CSV and libsvm for training/inference\n",
    "\n",
    "### 6.15.2 EC2 Instance Recommendation\n",
    "- only CPUs (M4 & C4)\n",
    "\n",
    "### 6.15.3 XGBoost Sample Notebooks\n",
    "[ Regression with Amazon SageMaker XGBoost algorithm](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/xgboost_abalone/xgboost_abalone.ipynb)\n",
    "\n",
    "### 6.15.4 How XGBoost Works\n",
    "- When using gradient boosting for regression, the **weak learners are regression trees**, and **each** regression tree **maps an input** data point **to one** of its **leafs** that contains a continuous score. \n",
    "- **XGBoost minimizes a regularized** (L1 and L2) objective function that combines a **convex loss function** and a **penalty term** for model complexity. \n",
    "- The **training** proceeds **iteratively**, **adding new trees** that **predict the residuals** or errors **of prior** trees that are then **combined with previous** trees to **make the final** prediction. \n",
    "- It's called **gradient boosting** because it uses a **gradient descent** algorithm **to minimize the loss** when **adding new models**. \n",
    "- Link: [1](https://arxiv.org/pdf/1603.02754.pdf), [2](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)\n",
    "\n",
    "### 6.15.5 [XGBoost Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)\n",
    "- num_class, num_round\n",
    "- alpha, base_score\n",
    "- booster, colsample_bylevel, colsample_bytree, csv_weights, early_stopping_rounds\n",
    "- eta, eval_metric, gamma, grow_policy, lambda, lambda_bias\n",
    "- max_bin, max_delta_step, max_depth, max_leaves, min_child_weight\n",
    "- normalize_type, nthread, objective\n",
    "- one_drop, process_type, rate_drop\n",
    "- refresh_leaf, sample_type, scale_pos_weight, seed\n",
    "- silent, sketch_eps, skip_drop, subsample, tree_method, tweedie_variance_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### 6.15.6 Tuning a XGBoost Model\n",
    "- **Metrics Computed by the XGBoost Algorithm**:\n",
    "<table><tr>\n",
    "     <th>Metric Name</th>\n",
    "     <th>Description</th>\n",
    "     <th>Optimization       Direction     </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:auc</code></td>\n",
    "     <td>       <p>Area under the curve.</p>     </td>\n",
    "     <td>        <p>Maximize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:error</code></td>\n",
    "     <td>\n",
    "        <p>Binary classification error rate, calculated as #(wrong\n",
    "           cases)/#(all cases).\n",
    "        </p>\n",
    "     </td>\n",
    "     <td>       <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:logloss</code></td>\n",
    "     <td>        <p>Negative log-likelihood.</p>     </td>\n",
    "     <td>        <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:mae</code></td>\n",
    "     <td>        <p>Mean absolute           error.        </p>     </td>\n",
    "     <td>\n",
    "        <p>You must choose one of them as an objective to optimize when\n",
    "           tuning the algorithm with hyperparameter values.&gt;Minimize\n",
    "        </p>\n",
    "     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:map</code></td>\n",
    "     <td>        <p>Mean average           precision.        </p>     </td>\n",
    "     <td>        <p>Maximize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:merror</code></td>     <td>\n",
    "        <p>Multiclass classification error rate, calculated as #(wrong\n",
    "           cases)/#(all cases).        </p>\n",
    "     </td>\n",
    "     <td>       <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:mlogloss</code></td>\n",
    "     <td>        <p>Negative log-likelihood for multiclass classification.</p>     </td>\n",
    "     <td>        <p>Minimize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:ndcg</code></td>\n",
    "     <td>        <p>Normalized Discounted Cumulative           Gain.        </p>     </td>\n",
    "     <td>        <p>Maximize</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">validation:rmse</code></td>\n",
    "     <td>        <p>Root mean square           error.        </p>     </td>\n",
    "     <td>        <p>Minimize</p>     </td>\n",
    "  </tr></table>\n",
    "  \n",
    "- **Tunable Hyperparameters**:\n",
    "<table>\n",
    "<tr>\n",
    "     <th>Parameter Name</th>\n",
    "     <th>Parameter        Type     </th>\n",
    "     <th>Recommended Ranges</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">alpha</code></td>\n",
    "     <td>         <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0, MaxValue: 1000</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">colsample_bylevel</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.1, MaxValue: 1</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">colsample_bytree</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.5, MaxValue: 1</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">eta</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.1, MaxValue: 0.5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">gamma</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0, MaxValue: 5</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">lambda</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0, MaxValue: 1000</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">max_delta_step</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>[0, 10]</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">max_depth</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>[0, 10]</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">min_child_weight</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0, MaxValue: 120</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">num_round</code></td>\n",
    "     <td>        <p>IntegerParameterRanges</p>     </td>\n",
    "     <td>        <p>[1, 4000]</p>     </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "     <td><code class=\"code\">subsample</code></td>\n",
    "     <td>        <p>ContinuousParameterRanges</p>     </td>\n",
    "     <td>        <p>MinValue: 0.5, MaxValue: 1</p>     </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Using Custom Algorithms\n",
    "Custom algorithms can be packaged to use with Amazon SageMaker, regardless of programming language or framework. Amazon SageMaker allows:\n",
    "- training by built-in algorithms and inference with custom code\n",
    "- training and inference by built-in algorithms\n",
    "- training and inference by custom algorithms\n",
    "- training by deep learning containers and inference with custom code.\n",
    "\n",
    "Amazon SageMaker algorithms are packaged as Docker images.<br>\n",
    "Separate Docker images can be provided for the training algorithm and inference code\n",
    "\n",
    "## 7.1. Using Custom Training Algorithms\n",
    "This section explains how Amazon SageMaker interacts with a Docker container that runs custom training algorithm.\n",
    "\n",
    "### 7.1.1 How Amazon SageMaker Runs Custom Training Image\n",
    "- To configure a Docker container to run as an executable, use an <code>ENTRYPOINT</code> instruction in a Dockerfile\n",
    "- [Note](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html)\n",
    "\n",
    "### 7.1.2 How Amazon SageMaker Provides Training Information\n",
    "- [CreateTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html) to SageMaker requests to specify: *Amazon Elastic Container Registry path of the Docker image that contain the training algorithm*, *S3 location* for training data and parameters \n",
    "- **Hyperparameters** are stored in <code> /opt/ml/input/config/hyperparameters.json</code>\n",
    "- **Environment Variables**: <code>TRAINING_JOB_NAME</code> ...\n",
    "- **Input Data Configuration**: in <code> /opt/ml/input/config/inputdataconfig.json </code>\n",
    "- **Training Data**: in **FILE** mode or **PIPE** mode\n",
    "- **Distributed Training Configuration**: <code> /opt/ml/input/config/resourceconfig.json</code>\n",
    "\n",
    "### 7.1.3 Signalling Algorithm Success and Failure\n",
    "- A training algorithm indicates whether it succeeded or failed using the exit code of its process.\n",
    "\n",
    "### 7.1.4 How Amazon SageMaker Processes Training Output\n",
    "- Amazon SageMaker returns the first 1024 characters from <code>/opt/ml/output/failure</code> as FailureReason\n",
    "- all final model artifacts are written to <code>/opt/ml/model</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Using Custom Inference Code\n",
    "### 7.2.1 Hosting Services\n",
    "- **How Amazon SageMaker Runs Custom Inference Image**<br>\n",
    "    use an <code>ENTRYPOINT</code> instruction in a Dockerfile\n",
    "- **How Amazon SageMaker Loads Custom Model Artifacts**<br>\n",
    "    Amazon SageMaker copies model artifects from <code>ModelDataUrl</code> to <code>/opt/ml/model</code>\n",
    "- **How Containers Serve Requests**:<br>\n",
    "    Containers need to implement a **web server** that responds to <code>/invocations</code> and <code>/ping</code> on **port 8080**.\n",
    "- **How Custom Container Should Respond to Inference Requests**:<br>\n",
    "    To **obtain inferences**, the **client application** sends a **POST request** to the Amazon SageMaker **endpoint**. [ InvokeEndpoint API](https://docs.aws.amazon.com/sagemaker/latest/dg/API_runtime_InvokeEndpoint.html)\n",
    "- **How Custom Container Should Respond to Health Check (Ping) Requests**:<br>\n",
    "    The simplest requirement on the container is to respond with an **HTTP 200 status** code and an **empty body**\n",
    "    \n",
    "### 7.2.2. Batch Transform\n",
    "- **How Amazon SageMaker Runs Custom Inference Image in Batch Transform**:<br>\n",
    "    use an <code>ENTRYPOINT</code> instruction in a Dockerfile\n",
    "- **How Amazon SageMaker Loads Custom Model Artifacts**:<br>\n",
    "    Amazon SageMaker copies model artifects from <code>ModelDataUrl</code> to <code>/opt/ml/model</code>\n",
    "- **How Containers Serve Requests**:<br>\n",
    "    Containers need to implement a **web server** that responds to <code>/invocations</code> and <code>/ping</code> on **port 8080**.\n",
    "- **How Custom Container Should Respond to Health Check (Ping) Requests**:<br>\n",
    "    The simplest requirement on the container is to respond with an **HTTP 200 status** code and an **empty body**\n",
    " \n",
    "## 7.3 [Example](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Automatically Scaling Amazon SageMaker Models\n",
    "- Automatic scaling dynamically adjusts the number of instances provisioned for a production variant in response to changes in our workload. \n",
    "- When the **workload increases**, automatic scaling **brings more instances** online. When the **workload decreases**, automatic scaling **removes** unnecessary **instances** so that we don't pay for provisioned variant instances that you aren't using.\n",
    "- [Topics](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html):\n",
    "    - [Configure Automatic Scaling for a Variant](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-add-policy.html)\n",
    "    - [Editing a Scaling Policy](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-edit.html)\n",
    "    - [Deleting a Scaling Policy](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-delete.html)\n",
    "    - [Load Testing for Variant Automatic Scaling](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html)\n",
    "    - [Additional Considerations for Configuring Automatic Scaling](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-considerations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. Using TensorFlow\n",
    "- [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html)\n",
    "- We can use Amazon SageMaker to train a model using custom TensorFlow code.\n",
    "- [PIPE mode](https://github.com/aws/sagemaker-tensorflow-extensions/blob/master/README.rst)\n",
    "- [Version support](https://github.com/aws/sagemaker-python-sdk#tensorflow-sagemaker-estimators)\n",
    "- [GitHub repository](https://github.com/aws/sagemaker-tensorflow-containers)\n",
    "\n",
    "## 9.1. Writing Custom TensorFlow Model Training and Inference Code\n",
    "TensorFlow training script must be a Python 2.7 source file, contaning:\n",
    "- <code>model_fn</code>: Defines the model that will be trained.\n",
    "- <code>train_input_fn</code></code>: Preprocess and load training data.\n",
    "- <code>eval_input_fn</code>: Preprocess and load evaluation data.\n",
    "- <code>serving_input_fn</code>: Defines the features to be passed to the model during prediction.\n",
    "\n",
    "## 9.2. Examples\n",
    "- [TensorFlow Example 1: Using the tf.estimator ](https://docs.aws.amazon.com/sagemaker/latest/dg/tf-example1.html)\n",
    "- tf.estimator (iris_dnn_classifier)\n",
    "- tf.layers (abalone_using_layers)\n",
    "- tf.contrib.keras (abalone_using_keras)\n",
    "- distributed TensorFlow (distributed_mnist)\n",
    "- ResNet CIFAR-10 with Tensorboard (resnet_cifar10_with_tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Using Apache MXNet\n",
    "- [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet.html)\n",
    "- We can use Amazon SageMaker to train a model using your own custom Apache MXNet training code. \n",
    "- [supported versions](https://github.com/aws/sagemaker-python-sdk#mxnet-sagemaker-estimators)\n",
    "- [GitHub repository](https://github.com/aws/sagemaker-mxnet-containers)\n",
    "\n",
    "## 10.1. Writing Custom Apache MXNet Model Training and Inference Code \n",
    "**Must implement** [following intefaces](https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet-training-inference-code-template.html)\n",
    "- <code>def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir, model_dir, num_gpus, num_cpus, hosts, current_host, ** kwargs)</code>\n",
    "- <code>def save(model, model_dir)</code>\n",
    "\n",
    "**Optinal**:<br>\n",
    "- <code>def model_fn(model_dir)</code>\n",
    "- <code>def transform_fn(model, input_data, content_type, accept)</code>\n",
    "- <code>def input_fn(input_data, content_type)</code>\n",
    "- <code>def predict_fn(block, array)</code>\n",
    "- <code>def output_fn(ndarray, accept)</code>\n",
    "- <code>def input_fn(model, input_data, content_type)</code>\n",
    "- <code>def predict_fn(module, data)</code>\n",
    "- <code>def output_fn(data, accept)</code>\n",
    "\n",
    "## 10.2 Examples:\n",
    "- The Apache MXNet Module API\n",
    "- The Apache MXNet Gluon API\n",
    "- [Apache MXNet Example 1: Using the Module API](https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet-example1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XI. Using Chainer\n",
    "We can use Amazon SageMaker to train and deploy a model using custom Chainer code\n",
    "- [GitHub repository](https://github.com/aws/sagemaker-chainer-container)\n",
    "- [versions supported](https://github.com/aws/sagemaker-python-sdk#chainer-sagemaker-estimators)\n",
    "\n",
    "# XII. Using PyTorch\n",
    "We can use Amazon SageMaker to train and deploy a model using custom PyTorch code.\n",
    "- [GitHub repository](https://github.com/aws/sagemaker-pytorch-container)\n",
    "- [versions supported](https://github.com/aws/sagemaker-python-sdk#pytorch-sagemaker-estimators)\n",
    "- [writing PyTorch training scripts](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/pytorch/README.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XIII. Using Apache Spark\n",
    "- [supported versions](https://github.com/aws/sagemaker-spark#getting-sagemaker-spark)\n",
    "- Amazon SageMaker Spark library <code>com.amazonaws.services.sagemaker.sparksdk</code> contains:\n",
    "    - <code>SageMakerEstimator</code>\n",
    "    - <code>KMeansSageMakerEstimator, PCASageMakerEstimator, XGBoostSageMakerEstimator</code>\n",
    "    - <code>SageMakerModel</code>\n",
    " - **Downloading the Amazon SageMaker Spark Library**:\n",
    "     - [GitHub link](https://github.com/aws/sagemaker-spark)\n",
    "     - <code> pip install sagemaker_pyspark </code>\n",
    "     - create new notebook with <code>Sparkmagic (PySpark)</code> or the <code>Sparkmagic (PySpark3)</code>, [more](http://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/)\n",
    " - **Integrating Custom Apache Spark Application with Amazon SageMaker**:\n",
    "     - Continue data preprocessing using the Apache Spark library\n",
    "     - Use the estimator in the Amazon SageMaker Spark library to train the model\n",
    "     - Get inferences from the model hosted in Amazon SageMaker\n",
    "\n",
    "## 13.1. Example 1\n",
    "- [Link](https://docs.aws.amazon.com/sagemaker/latest/dg/apache-spark-example1.html)\n",
    "- [Using Custom Algorithms for Model Training and Hosting on Amazon SageMaker with Apache Spark](https://docs.aws.amazon.com/sagemaker/latest/dg/apache-spark-example1-cust-algo.html)\n",
    "- [Using the SageMakerEstimator in a Spark Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/apache-spark-example1-extend-pipeline.html)\n",
    "\n",
    "## 13.2. Additional Examples\n",
    "- [GitHub repository](https://github.com/aws/sagemaker-spark/tree/master/examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XIV. Amazon SageMaker Libraries\n",
    "- [Amazon SageMaker Apache Spark Library](https://github.com/aws/sagemaker-spark)\n",
    "- [Amazon SageMaker high-level Python library](https://github.com/aws/sagemaker-python-sdk)\n",
    "\n",
    "# XV. Authentication and Access Control\n",
    "[Access](https://docs.aws.amazon.com/sagemaker/latest/dg/authentication-and-access-control.html) to Amazon SageMaker requires credentials. Those credentials must have permissions to access AWS resources, such as an Amazon SageMaker notebook instance or an Amazon EC2 instance.\n",
    "\n",
    "- **Authentication**:\n",
    "    - AWS account root user: only user to [create IAM user](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#create-iam-users)\n",
    "    - IAM user: an identity within AWS account that has specific custom permissions\n",
    "    - IAM role: an IAM identity that we can create in our account that has specific permissions: Federated user access, AWS service access, Applications running on Amazon EC2\n",
    "\n",
    "- **Access Control**:\n",
    "- **Topic**\n",
    "    - [Overview of Managing Access Permissions to Your Amazon SageMaker Resources](https://docs.aws.amazon.com/sagemaker/latest/dg/access-control-overview.html)\n",
    "    - [Using Identity-based Policies (IAM Policies) for Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/using-identity-based-policies.html)\n",
    "    - [Amazon SageMaker API Permissions: Actions, Permissions, and Resources Reference](https://docs.aws.amazon.com/sagemaker/latest/dg/api-permissions-reference.html)\n",
    "\n",
    "# XVI. Monitoring\n",
    "Monitoring is an important part of **maintaining the reliability, availability, and performance** of Amazon SageMaker and other AWS solutions.\n",
    "\n",
    "- [Amazon CloudWatch](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/) monitors our AWS resources and the applications that we run on AWS in real time.\n",
    "- [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/) enables us to monitor, store, and access our log files from EC2 instances, AWS CloudTrail, and other sources.\n",
    "- [AWS CloudTrail](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/) captures API calls and related events made by or on behalf of our AWS account and delivers the log files to an Amazon S3 bucket that we specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XVII. Other TOPICS\n",
    "- [Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/best-pratices.html)\n",
    "- [Security](https://docs.aws.amazon.com/sagemaker/latest/dg/security.html)\n",
    "- [Limits and Supported Regions](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix.html)\n",
    "- [API Reference](https://docs.aws.amazon.com/sagemaker/latest/dg/API_Reference.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
