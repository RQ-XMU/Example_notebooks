{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRN: A Deep Reinforcement Learning Framework for News Recommendation\n",
    "[Link](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "### &#160;&#160;&#160;&#160; Issues:\n",
    "- only try to model current reward (e.g., Click Through Rate (CTR))\n",
    "- very few studies consider to use user feedback other than click / no click labels (e.g., how frequent user returns) to help improve recommendation\n",
    "- existing methods tend to keep recommending similar news to users, which may cause users to get bored.\n",
    "### Solutions\n",
    "- Deep Q-Learning based recommendation framework can model future reward explcitly\n",
    "- consider user return pattern as a supplement to click / no click label in order to capture more user feedback information\n",
    "- an effective exploration strategy is incorporated to find new attractive news for users.\n",
    "### Applications:\n",
    "- Offline datasets\n",
    "- online production environment of a commercial news\n",
    "### Pros\n",
    "- the superior performance of new methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCTION\n",
    "### &#160;&#160;&#160;&#160; Problem\n",
    "- explosive growth of online content and services has provided tons of choices for users\n",
    "- personalized online content recommendation are necessary to improve user experience.\n",
    "### Existing methods:\n",
    "- content-based\n",
    "- collaborative filtering\n",
    "- hybrid methods\n",
    "- deep learning models (state-of-the-art)\n",
    "### Chalenges:\n",
    "- dynamic changes in news recommendations are difficult to handle: \n",
    "    - news become outdated very fast $\\Rightarrow$ news features and news candidate set are changing rapidly.\n",
    "    - users’ interest on different news might evolve during time $\\Rightarrow$ update the model periodically\n",
    "    - Existing method can only try to optimize the current reward, and hence ignore what effect the current recommendation might bring to the future.\n",
    "- current recommendation methods usually only consider the click / no click labels or ratings as users’ feedback\n",
    "- tendency to keep recommending similar items to users, which might decrease users’ interest in similar topics\n",
    "    - $\\epsilon$-greedy strategy may recommend the customer with totally unrelated items\n",
    "    - Upper Confidence Bound can not get a relatively accurate reward estimation for an item until this item has been tried several times.\n",
    "\n",
    "### &#160;&#160;&#160;&#160; Proposal:\n",
    "- use Deep Q-Learning to better model the dynamic nature of news characteristics and user preference. DQN structure can easily scale up\n",
    "-  consider user return as another form of user feedback information, by maintaining an activeness score for each user: **multiple historical return interval information**, so model can estimate user activeness at any time.\n",
    "- apply a Dueling Bandit Gradient Descent (DBGD) method for exploration, by choosing random item candidates in the neighborhood of the current recommender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#160;&#160;&#160;&#160;  The system\n",
    "![](https://imgur.com/QCprUl0.png)\n",
    "- user pool and news pool make up the **environment**, \n",
    "- recommendation algorithms play the role of **agent** \n",
    "- **state** is defined as *feature* representation for *users* \n",
    "- **action** is defined as *feature* representation for *news*.\n",
    "- **reward** is composed of *click labels* and estimation of *user activeness*.\n",
    "\n",
    "#### **Work flow**:\n",
    "1. User requests for news, \n",
    "2. A state representation (*users*) and a set of action representations (*items*) are passed to the agent,\n",
    "3. Agent selects the best action (i.e., recommending a list of news to user), \n",
    "4. Agent fetches user feedback as reward, \n",
    "5. Agent stores recommendation and feedback log in the memory,\n",
    "6. The agent use the log in the memory to update its recommendation algorithm **every 1 hour**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#160;&#160;&#160;&#160; Contribution:\n",
    "- DQN based frame work can take care of both immediate and future reward and can be generalized to many other recommendation problems\n",
    "- consider user activeness to help improve recommendation accuracy,\n",
    "- apply a more effective exploration method Dueling Bandit Gradient Descent\n",
    "- deploy model online in a commercial news recommendation application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RELATED WORKS\n",
    "### 2.1 News recommendation algorithms\n",
    "- **Content-based** methods maintain news term frequency features and user profilesand select news that is more similar to user profile.\n",
    "- **Collaborative filtering** methods makes rating prediction, utilizing the past ratings of current user or similar users or the combination of these two.\n",
    "- **Hybrid** methods are proposed to improve the user profile modeling\n",
    "- **Deep learning** models have shown much superior performance due to capability of modeling complex user-item relationship\n",
    "\n",
    "*DQN focuses on dealing with the dynamics nature of online news recommendation, and modeling of future reward that can be* **esily intergrated**.\n",
    "\n",
    "### 2.2 Reinforcement learning in recommendation\n",
    "#### 2.2.1 Contextual Multi-Armed Bandit models (MAB)\n",
    "- The context contains user and item features\n",
    "- Expected reward is a linear function of the context\n",
    "- Some try to combine **bandit** with **clustering based collaborative filtering** and **matrix factorization** to *model* more **complex user and item relationship** and **utilize** the **social network relationship** in determining the reward function.\n",
    "\n",
    "*DQN-based method* **applies Markov Decision Process**, *and is able to explicitly model future rewards*\n",
    "\n",
    "#### 2.2.2 Markov Decision Process models\n",
    "- **MDP-based** methods can not only **capture the reward of current iteration**, but also the **potential reward in the future iterations**.\n",
    "- Try to model item/n-gam of item as *state* and transition as *action* $\\Rightarrow$:\n",
    "    - giant state space size $\\Rightarrow$ can't scale up\n",
    "    - sparcity of action space $\\Rightarrow$ hard to train\n",
    "\n",
    "*DQN uses continuous state and action representation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PROBLEM DEFINITION\n",
    "\n",
    "**When a user $u$ sends a news request to the recommendation agent $G$ at time $t$, given a candidate set $I$ of news, our algorithm is going to select a list $L$ of top-k appropriate news for this user.|**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. METHOD\n",
    "### 4.1 Model framework\n",
    "![](https://imgur.com/y4wfV6q.png)\n",
    "#### Offline stage:\n",
    "- Extract features from news and users\n",
    "- use DQN to predict reward\n",
    "- train DQN, using **offline user-news click logs**\n",
    "\n",
    "#### Online stage:\n",
    "Agent interacts with users and update the network:\n",
    "\n",
    "1. **PUSH**: users sends request, G takes feature representstions , G gernerate top-k list, combining explotation current model and ecploration novel items.\n",
    "2. **FEEDBACK**: User give feedback by clicking\n",
    "3. **MINOR UPDATE**: Agent updates model after each timestamp, using *news list, feeadback* comparing *exploitation network* with *exploration network*.\n",
    "4. **MAJOR UPDATE**: after one hour, update network by replaying memory.\n",
    "5. **REPEAT** (1-4) until termination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature construction\n",
    "- **News features**: 417 dimension one hot features that describe whether certain property appears in this piece of news, including \n",
    "    - headline, \n",
    "    - provider, \n",
    "    - ranking, \n",
    "    - entity name,\n",
    "    - category, \n",
    "    - topic category, \n",
    "    - click counts in last 1 hour, 6 hours, 24 hours, 1 week, and 1 year.\n",
    "- **User features**: 413 × 5 = 2065 dimensions, describes the features\n",
    "    - headline\n",
    "    - provider\n",
    "    - ranking, \n",
    "    - entity name, \n",
    "    - category, \n",
    "    - topic category\n",
    "    - click in 1 hour, 6 hours, 24 hours, 1 week, and 1 year\n",
    "    - total click count for each time granularity\n",
    "    \n",
    "- **User news features**: 25-dimensional, describes the interaction between user and one certain piece of news appearing in the history of the user’s readings:\n",
    "    - the frequency for the entity\n",
    "    - category, \n",
    "    - topic category\n",
    "    - and provider\n",
    "- **Context features**: 32-dimensional, describe the context when a news request happens, including time, weekday, and the freshness of the news\n",
    "- **Textual features**: Not iplementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Deep Reinforcement Recommendation\n",
    "\n",
    "- Total reward:\n",
    "$$y_{a,s}=Q(a,a)=r_{\\text{immediate}}+\\gamma r_{\\text{future}}\\tag{1}$$\n",
    "- state $s$ = {context & user features}\n",
    "- action $a$ = {news & user-news interaction features}\n",
    "- $r_{\\text{immediate}}$ = rewards for current situation\n",
    "- $r_{\\text{future}}$ = projection of future rewards\n",
    "- DDQN predict total reward:\n",
    "$$\n",
    "y_{s,a,t}=r_{a,t+1}+\\gamma Q\\left(s_{a, t+1},\\text{arg}\\max_{a'}{Q\\left(s_{a, t+1},a';W_t\\right);W'_t}\\right)\n",
    "\\tag{2}\n",
    "$$\n",
    "-  Every a few iterations, $W_t$ and $W′_t$ will be switched\n",
    "\n",
    "#### Q-network\n",
    "![](https://imgur.com/7yqDKjQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 User Activeness\n",
    "Use **survival models** to model user return and user activeness. Suppose $T$ is the time until next event happens then the **hazard function** (i.e., instantaneous rate for the event to happen) can be defined as:\n",
    "$$\n",
    "\\lambda(t)=\\lim_{dt\\rightarrow 0}\\frac{\\text{Pr}\\left[t\\leq T<t+dt\\mid T\\geq t\\right]}{dt}\n",
    "\\tag{3}\n",
    "$$\n",
    "the probability for the event to happen after $t$ can be defined as:\n",
    "$$ S(t)=\\exp \\left(\\int_0^t \\lambda(x)dx\\right) \\tag{4}$$\n",
    "and the expected life span $T_0$ can be calculated as:\n",
    "$$T_0=\\int_0^\\infty S(t)dt\n",
    "\\tag{5}$$\n",
    "In our problem, we simply set $λ(t)=λ_0$, which means each user has a constant probability to return. Every time we detect a return of user, we will set $S(t)=S(t)+S_a$ for this particular user. The user activeness score will not exceed 1.\n",
    "\n",
    "Parameters $S_0,S_a,λ_0,T_0$ are determined according to the real user pattern in dataset.\n",
    "\n",
    "The click / no click label $r_{\\text{click}}$ and the user activeness $r_{\\text{active}} \n",
    "$ are combined as:\n",
    "\n",
    "$$\n",
    "r_\\text{total} = r_\\text{click} +\\beta\\cdot r_\\text{active}\n",
    "\\tag{6}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Explore\n",
    "- $\\epsilon$-greedy randomly recommends new intems with a probability of $\\epsilon$\n",
    "- UCB will pick items that have not been explored for many times\n",
    "- Dueling Bandit Gradient Descent algorithm is used in this paper\n",
    "\n",
    "![](https://imgur.com/kCJjG44.png)\n",
    "\n",
    "The agent $G$ is going to generate a recommendation list $L$ using the current network $Q$ and another list $\\tilde{L}$ using and explore network $\\tilde{Q}$. The parameters $\\tilde{W}$ of network $\\tilde{Q}$ can be obtained by adding a small disturb $\\Delta W$ to the parameters $W$ of the current network $Q$:\n",
    "$$\\Delta W =\\alpha\\cdot\\text{rand}(-1,1)\\cdot W \\tag{7}$$\n",
    "Then the agent $G$ will do a **probabilistc interleave** to generate the merged recommendation list $\\tilde{L}$ using $L$ and $\\tilde{L}$. To determine the item for each position in the recommendation list $\\tilde{L}$, the probabilistics interleave approach basically will first randomly select between list $L$  nad $\\tilde{L}$.\n",
    "\n",
    "Suppose $L$ is selected, then an item $i \\in L$ will be put into $\\hat{L}$ with a probability determnied by its ranking in $L$. Then list $\\hat{L}$ will be recommended to user $u$ and agent $G$ will obtain the feedback $B$. It the items recommended by the explore network $\\tilde{Q}$ receive a better feedback, the agent $G$ will update the network $Q$ and $\\tilde{Q}$, with the paraemeters of the network being updated as:\n",
    "$$\n",
    "W' = W +\\eta \\tilde{W}\\tag{8}$$\n",
    "Othersiwe the agent $G$ will keep network $Q$ unchanged. Through this kind of exploration, the agetn can do more effective exploration without losing too much recommendation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 EXPERIMENT\n",
    "### 5.1. Dataset: \n",
    "- crawled: \n",
    "\n",
    "| Stage         | Duration | # of users | # of news |\n",
    "|---------------|:--------:|------------|-----------|\n",
    "| Offline stage | 6 months |    541.337 | 1,355,344 |\n",
    "| Online stage  | 1 months |     64,610 |   157,088 |\n",
    "\n",
    "### 5.2 Evaluation measures\n",
    "- Click through rate (CTR)\n",
    "$$\n",
    "\\text{CTR}=\\frac{\\text{# of clicked items}}{\\text{# of total items}}\n",
    "\\tag{9}$$\n",
    "\n",
    "- Precision@k\n",
    "$$\n",
    "\\text{Precision@k}=\\frac{\\text{# of clicks in top-k recommended items}}{\\text{k}}\n",
    "\\tag{10}$$\n",
    "\n",
    "- Normalized Discounted Cumulative Gain (nDGC)\n",
    "$$\n",
    "DCG(f)=\\sum_{r=1}^{n}{y^f_r D(r)}\n",
    "\\tag{11}$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "D(r)=\\frac{1}{\\log(1+r)}\n",
    "\\tag{12}$$\n",
    "\n",
    "### 5.3 Experiment setting\n",
    "| Parameter|                  Setting                 |\n",
    "|----------|:----------------------------------------:|\n",
    "| Future reward discount $γ$ (Equation 1)<br> User activeness coefficient $β$ (Equation 6)<br> Explore coefficient $α$ (Equation 7) <br>Exploit coefficient $η$ (Equation 8) <br>Major update period $T_R$ (for DQN experience replay) <br>Minor update period $T_D$ (for DBGD) |  0.4 <br>0.05 <br>0.1 <br>0.05 <br>60 minites <br>30 minutes |\n",
    "\n",
    "### 5.4 Compared methods\n",
    "\n",
    "- *LR*: Logistic Regression\n",
    "- *FM*: Factorization Machines\n",
    "- *W&D*: Wide & Deep - Deep learning model\n",
    "- *LinUCB*: Linear Upper Confidence Bound\n",
    "- *HLinUCB*: Hidden Linear Upper Confidence Bound\n",
    "\n",
    "### 5.5 Offline evaluation\n",
    "#### 5.5.1 Accuracy\n",
    "![](https://imgur.com/3zfa9wb.png)\n",
    "#### 5.5.2 Model converge process.\n",
    "algorithm in this paper (*DDQN + U + DBGD*) converges to a better CTR faster than other methods.\n",
    "![](https://imgur.com/WpKlatp.png)\n",
    "\n",
    "### 5.6 Online evaluation\n",
    "#### 5.6.1 Accuracy.\n",
    "![](https://imgur.com/yHtcHzC)\n",
    "#### 5.6.2 Recommendation diversity\n",
    "\n",
    "$$\n",
    "ILS(L)=\\frac{\\sum_{b_i \\in L}\\sum_{b_j \\in L,b_j\\neq b_i}S(b_i,b_j)}{\\sum_{b_i \\in L}\\sum_{b_j \\in L,b_j\\neq b_i}\\mathbb{1}}\n",
    "\\tag{13}$$\n",
    "\n",
    "$S(b_i,b_j)$: represents the cosine similarity between item $b_i$ and item $b_j$\n",
    "\n",
    "![](https://imgur.com/byfmw42.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 CONCLUSION\n",
    "\n",
    "In this paper, we propose a DQN-based reinforcement learning framework to do online personalized news recommendation. Different from previous methods, our method can effectively model the dynamic news features and user preferences, and plan for future explicitly, in order to achieve higher reward (e.g., CTR) in the long run. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, we apply an effective exploration strategy into our framework to improve the recommendation diversity and look for potential more rewarding recommendations. Experiments have shown that our method can improve the recommendation accuracy and recommendation diversity significantly. Our method can be generalized to many other recommendation problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
